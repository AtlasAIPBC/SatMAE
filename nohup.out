/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
job dir: /home/ada/satmae/SatMAE
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=1,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAEjob dir: /home/ada/satmae/SatMAE

Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=7,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAENamespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=4,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=6,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)

Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=5,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=2,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=3,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
<util.datasets.CustomDatasetFromImages object at 0x7fac4fd5be10>
<util.datasets.CustomDatasetFromImages object at 0x7f0fd24b2bd0>
<util.datasets.CustomDatasetFromImages object at 0x7f72aa480c50>
<util.datasets.CustomDatasetFromImages object at 0x7f8a9aba1d10>
<util.datasets.CustomDatasetFromImages object at 0x7f1e4db08d50>
<util.datasets.CustomDatasetFromImages object at 0x7fa71bbd9850>
<util.datasets.CustomDatasetFromImages object at 0x7fcdcb02ef50>
<util.datasets.CustomDatasetFromImages object at 0x7fc2ecced550>
<util.datasets.CustomDatasetFromImages object at 0x7f0fd24acfd0>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f0fd24acd10>
<util.datasets.CustomDatasetFromImages object at 0x7fac4fd5b190>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fac58c0e390>
<util.datasets.CustomDatasetFromImages object at 0x7f8a9ab9bc90>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f8a9ab9bdd0>
<util.datasets.CustomDatasetFromImages object at 0x7f72aa47ac10>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f72aa47ad10>
<util.datasets.CustomDatasetFromImages object at 0x7f1e4db08250>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f1e4db08ed0>
<util.datasets.CustomDatasetFromImages object at 0x7fcdc3289c10>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fcdc3289850>
<util.datasets.CustomDatasetFromImages object at 0x7fa712e7bd90>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fa712e7bdd0>
<util.datasets.CustomDatasetFromImages object at 0x7fc2798ffcd0>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fc2798ffd90>
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
Test:  [   0/3316]  eta: 1 day, 5:19:47  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 31.8417  data: 1.3043
Test:  [   0/3316]  eta: 1 day, 5:07:53  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 31.6264  data: 1.7369
Test:  [   0/3316]  eta: 1 day, 5:39:43  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.2025  data: 1.5528
Test:  [   0/3316]  eta: 1 day, 5:49:53  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.3865  data: 1.1799
Test:  [   0/3316]  eta: 1 day, 5:51:19  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.4123  data: 1.6570
Test:  [   0/3316]  eta: 1 day, 5:58:53  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.5495  data: 1.5203
Test:  [   0/3316]  eta: 1 day, 6:01:56  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.6044  data: 1.1988
Test:  [   0/3316]  eta: 1 day, 6:04:52  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.6577  data: 1.6810
Test:  [  10/3316]  eta: 1 day, 3:03:27  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 29.4639  data: 0.1206
Test:  [  10/3316]  eta: 1 day, 3:05:00  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 29.4921  data: 0.1603
Test:  [  10/3316]  eta: 1 day, 3:27:39  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 29.9031  data: 0.1545
Test:  [  10/3316]  eta: 1 day, 3:35:49  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 30.0513  data: 0.1531
Test:  [  10/3316]  eta: 1 day, 3:37:05  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 30.0742  data: 0.1437
Test:  [  10/3316]  eta: 1 day, 3:40:59  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 30.1449  data: 0.1102
Test:  [  10/3316]  eta: 1 day, 3:44:08  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 30.2023  data: 0.1411
Test:  [  10/3316]  eta: 1 day, 3:48:04  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 30.2737  data: 0.1123
Test:  [  20/3316]  eta: 1 day, 2:42:36  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.0511  data: 0.0021
Test:  [  20/3316]  eta: 1 day, 2:48:27  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.1522  data: 0.0022
Test:  [  20/3316]  eta: 1 day, 3:13:23  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.5879  data: 0.0019
Test:  [  20/3316]  eta: 1 day, 3:17:11  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.6727  data: 0.0024
Test:  [  20/3316]  eta: 1 day, 3:19:50  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.7338  data: 0.0023
Test:  [  20/3316]  eta: 1 day, 3:23:00  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.7773  data: 0.0023
Test:  [  20/3316]  eta: 1 day, 3:23:47  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.8003  data: 0.0026
Test:  [  20/3316]  eta: 1 day, 3:27:08  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.8534  data: 0.0028
Test:  [  30/3316]  eta: 1 day, 2:39:23  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.0452  data: 0.0023
Test:  [  30/3316]  eta: 1 day, 2:47:22  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.2864  data: 0.0027
Test:  [  30/3316]  eta: 1 day, 3:14:55  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.8245  data: 0.0025
Test:  [  30/3316]  eta: 1 day, 3:17:13  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.8084  data: 0.0025
Test:  [  30/3316]  eta: 1 day, 3:19:13  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.8522  data: 0.0023
Test:  [  30/3316]  eta: 1 day, 3:22:37  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.8779  data: 0.0021
Test:  [  30/3316]  eta: 1 day, 3:22:46  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.9138  data: 0.0024
Test:  [  30/3316]  eta: 1 day, 3:28:01  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.9918  data: 0.0025
Test:  [  40/3316]  eta: 1 day, 2:34:43  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 29.2429  data: 0.0028
Test:  [  40/3316]  eta: 1 day, 2:41:48  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 29.3971  data: 0.0030
Test:  [  40/3316]  eta: 1 day, 3:09:51  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 29.9733  data: 0.0029
Test:  [  40/3316]  eta: 1 day, 3:11:04  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 29.9466  data: 0.0028
Test:  [  40/3316]  eta: 1 day, 3:13:14  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 29.9776  data: 0.0027
Test:  [  40/3316]  eta: 1 day, 3:17:21  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 30.0713  data: 0.0027
Test:  [  40/3316]  eta: 1 day, 3:18:10  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 30.0871  data: 0.0027
Test:  [  40/3316]  eta: 1 day, 3:21:11  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 30.1359  data: 0.0030
Test:  [  50/3316]  eta: 1 day, 2:42:37  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 29.8112  data: 0.0032
Test:  [  50/3316]  eta: 1 day, 2:49:16  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 29.8968  data: 0.0031
Test:  [  50/3316]  eta: 1 day, 3:19:21  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.5263  data: 0.0030
Test:  [  50/3316]  eta: 1 day, 3:20:40  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.5229  data: 0.0027
Test:  [  50/3316]  eta: 1 day, 3:22:27  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.5502  data: 0.0028
Test:  [  50/3316]  eta: 1 day, 3:25:52  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.6142  data: 0.0031
Test:  [  50/3316]  eta: 1 day, 3:26:55  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.5099  data: 0.0034
Test:  [  50/3316]  eta: 1 day, 3:28:01  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.7104  data: 0.0031
Test:  [  60/3316]  eta: 1 day, 2:33:02  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 29.6598  data: 0.0033
Test:  [  60/3316]  eta: 1 day, 2:38:17  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 29.6891  data: 0.0034
Test:  [  60/3316]  eta: 1 day, 3:10:56  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.4715  data: 0.0032
Test:  [  60/3316]  eta: 1 day, 3:11:58  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.4830  data: 0.0030
Test:  [  60/3316]  eta: 1 day, 3:12:34  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.4353  data: 0.0030
Test:  [  60/3316]  eta: 1 day, 3:17:12  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.3979  data: 0.0034
Test:  [  60/3316]  eta: 1 day, 3:17:28  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.5566  data: 0.0033
Test:  [  60/3316]  eta: 1 day, 3:19:39  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.6490  data: 0.0033
Test:  [  70/3316]  eta: 1 day, 2:27:04  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.0653  data: 0.0029
Test:  [  70/3316]  eta: 1 day, 2:31:11  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.0243  data: 0.0033
Test:  [  70/3316]  eta: 1 day, 3:05:29  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.8662  data: 0.0033
Test:  [  70/3316]  eta: 1 day, 3:07:26  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.9325  data: 0.0032
Test:  [  70/3316]  eta: 1 day, 3:07:34  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.8570  data: 0.0030
Test:  [  70/3316]  eta: 1 day, 3:11:42  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.9191  data: 0.0030
Test:  [  70/3316]  eta: 1 day, 3:12:59  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 30.0527  data: 0.0033
Test:  [  70/3316]  eta: 1 day, 3:15:05  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 30.0895  data: 0.0031
Test:  [  80/3316]  eta: 1 day, 2:20:30  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 29.1491  data: 0.0029
Test:  [  80/3316]  eta: 1 day, 2:24:45  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 29.1733  data: 0.0029
Test:  [  80/3316]  eta: 1 day, 2:58:00  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 29.8352  data: 0.0033
Test:  [  80/3316]  eta: 1 day, 3:00:25  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 29.9251  data: 0.0030
Test:  [  80/3316]  eta: 1 day, 3:00:29  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 29.9640  data: 0.0030
Test:  [  80/3316]  eta: 1 day, 3:05:38  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 30.0562  data: 0.0028
Test:  [  80/3316]  eta: 1 day, 3:06:36  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 30.1137  data: 0.0032
Test:  [  80/3316]  eta: 1 day, 3:07:00  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 30.0211  data: 0.0031
Test:  [  90/3316]  eta: 1 day, 2:14:04  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 29.0639  data: 0.0029
Test:  [  90/3316]  eta: 1 day, 2:18:47  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 29.1926  data: 0.0026
Test:  [  90/3316]  eta: 1 day, 2:59:14  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.3646  data: 0.0030
Test:  [  90/3316]  eta: 1 day, 3:04:39  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.6952  data: 0.0030
Test:  [  90/3316]  eta: 1 day, 3:04:50  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.7016  data: 0.0031
Test:  [  90/3316]  eta: 1 day, 3:09:30  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.8254  data: 0.0028
Test:  [  90/3316]  eta: 1 day, 3:10:37  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.8347  data: 0.0030
Test:  [  90/3316]  eta: 1 day, 3:11:44  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.7927  data: 0.0032
Test:  [ 100/3316]  eta: 1 day, 2:16:54  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 29.8858  data: 0.0029
Test:  [ 100/3316]  eta: 1 day, 2:21:29  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 29.9988  data: 0.0030
Test:  [ 100/3316]  eta: 1 day, 2:51:38  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 30.3416  data: 0.0030
Test:  [ 100/3316]  eta: 1 day, 2:57:59  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 30.7539  data: 0.0032
Test:  [ 100/3316]  eta: 1 day, 2:58:51  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 30.8408  data: 0.0031
Test:  [ 100/3316]  eta: 1 day, 3:02:08  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 30.7593  data: 0.0030
Test:  [ 100/3316]  eta: 1 day, 3:04:20  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 30.8940  data: 0.0030
Test:  [ 100/3316]  eta: 1 day, 3:06:08  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 31.0320  data: 0.0033
Test:  [ 110/3316]  eta: 1 day, 2:09:24  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.8043  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 2:13:34  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.8382  data: 0.0030
Test:  [ 110/3316]  eta: 1 day, 2:43:16  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.4996  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 2:51:22  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.8831  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 2:52:23  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.9744  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 2:54:17  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.7768  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 2:57:14  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.9881  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 3:00:20  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 30.2156  data: 0.0028
Test:  [ 120/3316]  eta: 1 day, 2:03:51  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 29.0514  data: 0.0029
Test:  [ 120/3316]  eta: 1 day, 2:07:34  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 29.0420  data: 0.0029
Test:  [ 120/3316]  eta: 1 day, 2:36:55  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 29.5357  data: 0.0028
Test:  [ 120/3316]  eta: 1 day, 2:45:58  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 29.9651  data: 0.0028
Test:  [ 120/3316]  eta: 1 day, 2:47:29  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 30.0553  data: 0.0028
Test:  [ 120/3316]  eta: 1 day, 2:48:08  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 29.8180  data: 0.0030
Test:  [ 120/3316]  eta: 1 day, 2:51:56  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 30.0440  data: 0.0030
Test:  [ 120/3316]  eta: 1 day, 2:55:28  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 30.2760  data: 0.0028
Test:  [ 130/3316]  eta: 1 day, 1:58:29  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 29.2333  data: 0.0031
Test:  [ 130/3316]  eta: 1 day, 2:02:08  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 29.2503  data: 0.0032
Test:  [ 130/3316]  eta: 1 day, 2:30:44  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 29.6929  data: 0.0029
Test:  [ 130/3316]  eta: 1 day, 2:41:06  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 30.1290  data: 0.0030
Test:  [ 130/3316]  eta: 1 day, 2:42:02  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 30.1397  data: 0.0030
Test:  [ 130/3316]  eta: 1 day, 2:42:18  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 29.9749  data: 0.0034
Test:  [ 130/3316]  eta: 1 day, 2:46:45  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 30.2174  data: 0.0032
Test:  [ 130/3316]  eta: 1 day, 2:50:10  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 30.3168  data: 0.0030
Test:  [ 140/3316]  eta: 1 day, 1:53:21  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 29.2639  data: 0.0029
Test:  [ 140/3316]  eta: 1 day, 1:56:32  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 29.2660  data: 0.0032
Test:  [ 140/3316]  eta: 1 day, 2:24:48  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 29.6967  data: 0.0029
Test:  [ 140/3316]  eta: 1 day, 2:35:14  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 30.0592  data: 0.0030
Test:  [ 140/3316]  eta: 1 day, 2:36:12  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 30.0152  data: 0.0032
Test:  [ 140/3316]  eta: 1 day, 2:36:34  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 29.9916  data: 0.0032
Test:  [ 140/3316]  eta: 1 day, 2:41:11  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 30.1717  data: 0.0031
Test:  [ 140/3316]  eta: 1 day, 2:44:19  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 30.1885  data: 0.0031
Test:  [ 150/3316]  eta: 1 day, 1:51:11  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 29.7049  data: 0.0028
Test:  [ 150/3316]  eta: 1 day, 1:55:53  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 29.9270  data: 0.0032
Test:  [ 150/3316]  eta: 1 day, 2:26:25  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 30.7686  data: 0.0029
Test:  [ 150/3316]  eta: 1 day, 2:38:01  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 31.1495  data: 0.0030
Test:  [ 150/3316]  eta: 1 day, 2:38:32  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 31.0744  data: 0.0029
Test:  [ 150/3316]  eta: 1 day, 2:38:37  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 31.1204  data: 0.0033
Test:  [ 150/3316]  eta: 1 day, 2:43:56  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 31.3007  data: 0.0032
Test:  [ 150/3316]  eta: 1 day, 2:46:27  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 31.2372  data: 0.0032
Test:  [ 160/3316]  eta: 1 day, 1:49:36  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 30.2696  data: 0.0028
Test:  [ 160/3316]  eta: 1 day, 1:52:43  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 30.3217  data: 0.0032
Test:  [ 160/3316]  eta: 1 day, 2:19:41  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 30.6840  data: 0.0028
Test:  [ 160/3316]  eta: 1 day, 2:32:04  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 31.0110  data: 0.0030
Test:  [ 160/3316]  eta: 1 day, 2:32:12  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 31.2098  data: 0.0029
Test:  [ 160/3316]  eta: 1 day, 2:32:35  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 31.1389  data: 0.0032
Test:  [ 160/3316]  eta: 1 day, 2:38:12  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 31.3373  data: 0.0032
Test:  [ 160/3316]  eta: 1 day, 2:39:18  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 31.0871  data: 0.0031
Test:  [ 170/3316]  eta: 1 day, 1:43:28  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.7378  data: 0.0028
Test:  [ 170/3316]  eta: 1 day, 1:46:41  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.5903  data: 0.0030
Test:  [ 170/3316]  eta: 1 day, 2:12:19  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.3995  data: 0.0028
Test:  [ 170/3316]  eta: 1 day, 2:25:17  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.7808  data: 0.0031
Test:  [ 170/3316]  eta: 1 day, 2:25:35  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.9049  data: 0.0029
Test:  [ 170/3316]  eta: 1 day, 2:26:01  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.8876  data: 0.0029
Test:  [ 170/3316]  eta: 1 day, 2:30:58  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.9346  data: 0.0028
Test:  [ 170/3316]  eta: 1 day, 2:32:09  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.7677  data: 0.0029
Test:  [ 180/3316]  eta: 1 day, 1:37:11  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.0099  data: 0.0029
Test:  [ 180/3316]  eta: 1 day, 1:40:31  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.1117  data: 0.0028
Test:  [ 180/3316]  eta: 1 day, 2:05:57  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.3863  data: 0.0029
Test:  [ 180/3316]  eta: 1 day, 2:19:06  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.7712  data: 0.0031
Test:  [ 180/3316]  eta: 1 day, 2:19:30  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.8172  data: 0.0028
Test:  [ 180/3316]  eta: 1 day, 2:20:11  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.8788  data: 0.0028
Test:  [ 180/3316]  eta: 1 day, 2:25:01  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.8554  data: 0.0028
Test:  [ 180/3316]  eta: 1 day, 2:25:46  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.8168  data: 0.0029
Test:  [ 190/3316]  eta: 1 day, 1:31:27  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.0314  data: 0.0031
Test:  [ 190/3316]  eta: 1 day, 1:34:46  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.1152  data: 0.0029
Test:  [ 190/3316]  eta: 1 day, 1:59:34  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.4840  data: 0.0031
Test:  [ 190/3316]  eta: 1 day, 2:12:42  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.7745  data: 0.0032
Test:  [ 190/3316]  eta: 1 day, 2:13:09  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.8091  data: 0.0028
Test:  [ 190/3316]  eta: 1 day, 2:13:11  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.7451  data: 0.0029
Test:  [ 190/3316]  eta: 1 day, 2:18:05  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.8335  data: 0.0030
Test:  [ 190/3316]  eta: 1 day, 2:18:47  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.7709  data: 0.0033
Test:  [ 200/3316]  eta: 1 day, 1:25:09  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 28.9791  data: 0.0030
Test:  [ 200/3316]  eta: 1 day, 1:28:32  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.0553  data: 0.0030
Test:  [ 200/3316]  eta: 1 day, 1:53:24  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.4655  data: 0.0032
Test:  [ 200/3316]  eta: 1 day, 2:07:22  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.8898  data: 0.0030
Test:  [ 200/3316]  eta: 1 day, 2:08:03  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.8340  data: 0.0027
Test:  [ 200/3316]  eta: 1 day, 2:08:15  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.9918  data: 0.0028
Test:  [ 200/3316]  eta: 1 day, 2:12:34  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.8704  data: 0.0030
Test:  [ 200/3316]  eta: 1 day, 2:13:23  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.8999  data: 0.0031
Test:  [ 210/3316]  eta: 1 day, 1:21:09  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 29.2913  data: 0.0029
Test:  [ 210/3316]  eta: 1 day, 1:24:41  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 29.4047  data: 0.0030
Test:  [ 210/3316]  eta: 1 day, 1:48:25  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 29.6935  data: 0.0033
Test:  [ 210/3316]  eta: 1 day, 2:02:31  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 30.1617  data: 0.0030
Test:  [ 210/3316]  eta: 1 day, 2:03:02  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 30.1790  data: 0.0028
Test:  [ 210/3316]  eta: 1 day, 2:03:20  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 30.2433  data: 0.0030
Test:  [ 210/3316]  eta: 1 day, 2:07:04  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 30.1019  data: 0.0031
Test:  [ 210/3316]  eta: 1 day, 2:08:08  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 30.1925  data: 0.0030
Test:  [ 220/3316]  eta: 1 day, 1:17:15  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 29.7726  data: 0.0030
Test:  [ 220/3316]  eta: 1 day, 1:21:10  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 29.9557  data: 0.0030
Test:  [ 220/3316]  eta: 1 day, 1:44:05  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.0539  data: 0.0033
Test:  [ 220/3316]  eta: 1 day, 1:58:49  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.5047  data: 0.0033
Test:  [ 220/3316]  eta: 1 day, 1:58:54  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.3913  data: 0.0030
Test:  [ 220/3316]  eta: 1 day, 1:59:18  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.4371  data: 0.0032
Test:  [ 220/3316]  eta: 1 day, 2:02:48  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.3520  data: 0.0032
Test:  [ 220/3316]  eta: 1 day, 2:03:54  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.4289  data: 0.0033
Test:  [ 230/3316]  eta: 1 day, 1:12:17  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 29.5940  data: 0.0030
Test:  [ 230/3316]  eta: 1 day, 1:16:28  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 29.8129  data: 0.0029
Test:  [ 230/3316]  eta: 1 day, 1:38:17  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 29.8776  data: 0.0032
Test:  [ 230/3316]  eta: 1 day, 1:53:03  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 30.2090  data: 0.0030
Test:  [ 230/3316]  eta: 1 day, 1:53:41  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 30.4619  data: 0.0033
Test:  [ 230/3316]  eta: 1 day, 1:53:49  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 30.3229  data: 0.0031
Test:  [ 230/3316]  eta: 1 day, 1:56:55  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 30.2596  data: 0.0032
Test:  [ 230/3316]  eta: 1 day, 1:58:02  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 30.2921  data: 0.0030
Test:  [ 240/3316]  eta: 1 day, 1:06:43  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.2320  data: 0.0031
Test:  [ 240/3316]  eta: 1 day, 1:11:05  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.4191  data: 0.0030
Test:  [ 240/3316]  eta: 1 day, 1:32:03  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.4380  data: 0.0029
Test:  [ 240/3316]  eta: 1 day, 1:46:28  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.6554  data: 0.0027
Test:  [ 240/3316]  eta: 1 day, 1:47:19  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.8724  data: 0.0029
Test:  [ 240/3316]  eta: 1 day, 1:47:25  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.7924  data: 0.0029
Test:  [ 240/3316]  eta: 1 day, 1:50:16  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.7137  data: 0.0030
Test:  [ 240/3316]  eta: 1 day, 1:51:30  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.7687  data: 0.0028
Test:  [ 250/3316]  eta: 1 day, 1:00:56  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.0222  data: 0.0029
Test:  [ 250/3316]  eta: 1 day, 1:05:49  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.2834  data: 0.0029
Test:  [ 250/3316]  eta: 1 day, 1:26:34  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.4800  data: 0.0027
Test:  [ 250/3316]  eta: 1 day, 1:40:58  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.7023  data: 0.0026
Test:  [ 250/3316]  eta: 1 day, 1:41:50  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.7707  data: 0.0029
Test:  [ 250/3316]  eta: 1 day, 1:42:15  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.8444  data: 0.0029
Test:  [ 250/3316]  eta: 1 day, 1:44:48  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.7730  data: 0.0028
Test:  [ 250/3316]  eta: 1 day, 1:46:17  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.8855  data: 0.0029
Test:  [ 260/3316]  eta: 1 day, 0:55:57  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.1400  data: 0.0029
Test:  [ 260/3316]  eta: 1 day, 1:00:32  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.2923  data: 0.0031
Test:  [ 260/3316]  eta: 1 day, 1:20:44  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.5373  data: 0.0028
Test:  [ 260/3316]  eta: 1 day, 1:34:52  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.7714  data: 0.0027
Test:  [ 260/3316]  eta: 1 day, 1:35:44  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.7901  data: 0.0034
Test:  [ 260/3316]  eta: 1 day, 1:36:14  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.8991  data: 0.0029
Test:  [ 260/3316]  eta: 1 day, 1:38:25  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.7881  data: 0.0029
Test:  [ 260/3316]  eta: 1 day, 1:40:07  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.9316  data: 0.0032
Test:  [ 270/3316]  eta: 1 day, 0:49:57  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.0532  data: 0.0030
Test:  [ 270/3316]  eta: 1 day, 0:54:37  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.0977  data: 0.0030
Test:  [ 270/3316]  eta: 1 day, 1:14:51  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.4037  data: 0.0028
Test:  [ 270/3316]  eta: 1 day, 1:29:29  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.7746  data: 0.0028
Test:  [ 270/3316]  eta: 1 day, 1:30:28  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.8210  data: 0.0033
Test:  [ 270/3316]  eta: 1 day, 1:31:07  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.8959  data: 0.0029
Test:  [ 270/3316]  eta: 1 day, 1:33:03  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.7849  data: 0.0030
Test:  [ 270/3316]  eta: 1 day, 1:34:53  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.9086  data: 0.0032
Test:  [ 280/3316]  eta: 1 day, 0:45:06  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 29.0803  data: 0.0028
Test:  [ 280/3316]  eta: 1 day, 0:50:09  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 29.3023  data: 0.0031
Test:  [ 280/3316]  eta: 1 day, 1:10:01  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 29.6477  data: 0.0029
Test:  [ 280/3316]  eta: 1 day, 1:24:02  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 29.9135  data: 0.0028
Test:  [ 280/3316]  eta: 1 day, 1:25:16  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 30.0400  data: 0.0031
Test:  [ 280/3316]  eta: 1 day, 1:25:40  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 30.0163  data: 0.0030
Test:  [ 280/3316]  eta: 1 day, 1:27:30  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 29.9671  data: 0.0031
Test:  [ 280/3316]  eta: 1 day, 1:29:21  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 30.0478  data: 0.0031
Test:  [ 290/3316]  eta: 1 day, 0:39:46  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 29.2336  data: 0.0028
Test:  [ 290/3316]  eta: 1 day, 0:44:52  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 29.4619  data: 0.0033
Test:  [ 290/3316]  eta: 1 day, 1:06:23  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.2662  data: 0.0032
Test:  [ 290/3316]  eta: 1 day, 1:20:37  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.4662  data: 0.0036
Test:  [ 290/3316]  eta: 1 day, 1:22:00  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.6061  data: 0.0037
Test:  [ 290/3316]  eta: 1 day, 1:22:29  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.5688  data: 0.0037
Test:  [ 290/3316]  eta: 1 day, 1:24:07  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.5264  data: 0.0040
Test:  [ 290/3316]  eta: 1 day, 1:25:59  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.5741  data: 0.0037
Test:  [ 300/3316]  eta: 1 day, 0:36:50  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 29.8043  data: 0.0039
Test:  [ 300/3316]  eta: 1 day, 0:42:04  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 29.9682  data: 0.0043
Test:  [ 300/3316]  eta: 1 day, 1:01:42  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.3309  data: 0.0041
Test:  [ 300/3316]  eta: 1 day, 1:15:25  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.5466  data: 0.0042
Test:  [ 300/3316]  eta: 1 day, 1:16:59  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.6702  data: 0.0041
Test:  [ 300/3316]  eta: 1 day, 1:17:32  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.7302  data: 0.0039
Test:  [ 300/3316]  eta: 1 day, 1:19:00  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.6554  data: 0.0043
Test:  [ 300/3316]  eta: 1 day, 1:20:36  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.6222  data: 0.0040
Test:  [ 310/3316]  eta: 1 day, 0:31:39  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 29.8573  data: 0.0040
Test:  [ 310/3316]  eta: 1 day, 0:37:02  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.0546  data: 0.0043
Test:  [ 310/3316]  eta: 1 day, 0:56:33  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 29.9045  data: 0.0039
Test:  [ 310/3316]  eta: 1 day, 1:10:07  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.0119  data: 0.0035
Test:  [ 310/3316]  eta: 1 day, 1:11:57  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.1831  data: 0.0036
Test:  [ 310/3316]  eta: 1 day, 1:12:40  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.2626  data: 0.0033
Test:  [ 310/3316]  eta: 1 day, 1:14:06  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.2381  data: 0.0033
Test:  [ 310/3316]  eta: 1 day, 1:15:32  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.1454  data: 0.0032
Test:  [ 320/3316]  eta: 1 day, 0:26:57  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 29.3513  data: 0.0032
Test:  [ 320/3316]  eta: 1 day, 0:32:22  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 29.5280  data: 0.0030
Test:  [ 320/3316]  eta: 1 day, 0:51:52  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 29.9155  data: 0.0033
Test:  [ 320/3316]  eta: 1 day, 1:05:17  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 30.1205  data: 0.0030
Test:  [ 320/3316]  eta: 1 day, 1:07:15  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 30.2831  data: 0.0033
Test:  [ 320/3316]  eta: 1 day, 1:07:49  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 30.2994  data: 0.0031
Test:  [ 320/3316]  eta: 1 day, 1:09:05  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 30.2677  data: 0.0030
Test:  [ 320/3316]  eta: 1 day, 1:10:34  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 30.2647  data: 0.0032
Test:  [ 330/3316]  eta: 1 day, 0:22:03  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 29.4395  data: 0.0031
Test:  [ 330/3316]  eta: 1 day, 0:27:29  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 29.5730  data: 0.0033
Test:  [ 330/3316]  eta: 1 day, 0:48:12  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.4053  data: 0.0037
Test:  [ 330/3316]  eta: 1 day, 1:01:20  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.5644  data: 0.0040
Test:  [ 330/3316]  eta: 1 day, 1:03:46  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.8034  data: 0.0038
Test:  [ 330/3316]  eta: 1 day, 1:04:24  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.7917  data: 0.0040
Test:  [ 330/3316]  eta: 1 day, 1:05:25  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.6854  data: 0.0039
Test:  [ 330/3316]  eta: 1 day, 1:07:00  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.7643  data: 0.0046
Test:  [ 340/3316]  eta: 1 day, 0:18:44  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 29.9162  data: 0.0039
Test:  [ 340/3316]  eta: 1 day, 0:24:10  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.0469  data: 0.0041
Test:  [ 340/3316]  eta: 1 day, 0:43:10  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.3094  data: 0.0041
Test:  [ 340/3316]  eta: 1 day, 0:56:39  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.6325  data: 0.0044
Test:  [ 340/3316]  eta: 1 day, 0:59:09  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.8597  data: 0.0039
Test:  [ 340/3316]  eta: 1 day, 0:59:34  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.8182  data: 0.0043
Test:  [ 340/3316]  eta: 1 day, 1:00:30  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.7380  data: 0.0042
Test:  [ 350/3316]  eta: 1 day, 0:13:59  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 29.9938  data: 0.0041
Test:  [ 340/3316]  eta: 1 day, 1:02:07  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.8121  data: 0.0047
Test:  [ 350/3316]  eta: 1 day, 0:19:18  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.0743  data: 0.0040
Test:  [ 350/3316]  eta: 1 day, 0:37:51  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 29.7707  data: 0.0038
Test:  [ 350/3316]  eta: 1 day, 0:50:55  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.0373  data: 0.0035
Test:  [ 350/3316]  eta: 1 day, 0:53:23  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.1046  data: 0.0033
Test:  [ 350/3316]  eta: 1 day, 0:53:51  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.0622  data: 0.0033
Test:  [ 360/3316]  eta: 1 day, 0:08:25  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.2234  data: 0.0030
Test:  [ 350/3316]  eta: 1 day, 0:54:48  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.0590  data: 0.0031
Test:  [ 350/3316]  eta: 1 day, 0:56:18  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.0703  data: 0.0032
Test:  [ 360/3316]  eta: 1 day, 0:13:52  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.3503  data: 0.0030
Test:  [ 360/3316]  eta: 1 day, 0:32:07  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.5017  data: 0.0031
Test:  [ 360/3316]  eta: 1 day, 0:45:27  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.7562  data: 0.0029
Test:  [ 360/3316]  eta: 1 day, 0:48:13  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.9128  data: 0.0029
Test:  [ 370/3316]  eta: 1 day, 0:03:21  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 29.0974  data: 0.0029
Test:  [ 360/3316]  eta: 1 day, 0:48:37  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.9162  data: 0.0027
Test:  [ 360/3316]  eta: 1 day, 0:49:23  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.8746  data: 0.0029
Test:  [ 360/3316]  eta: 1 day, 0:50:48  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.8378  data: 0.0028
Test:  [ 370/3316]  eta: 1 day, 0:08:39  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 29.2127  data: 0.0028
Test:  [ 370/3316]  eta: 1 day, 0:27:33  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 29.7675  data: 0.0031
Test:  [ 370/3316]  eta: 1 day, 0:40:24  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 29.9866  data: 0.0034
Test:  [ 380/3316]  eta: 23:58:39  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.4128  data: 0.0039
Test:  [ 370/3316]  eta: 1 day, 0:43:08  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 30.1388  data: 0.0037
Test:  [ 370/3316]  eta: 1 day, 0:43:33  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 30.1348  data: 0.0035
Test:  [ 370/3316]  eta: 1 day, 0:44:25  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 30.1259  data: 0.0034
Test:  [ 370/3316]  eta: 1 day, 0:45:46  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 30.0967  data: 0.0035
Test:  [ 380/3316]  eta: 1 day, 0:03:45  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.3976  data: 0.0035
Test:  [ 380/3316]  eta: 1 day, 0:21:31  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.6334  data: 0.0038
Test:  [ 380/3316]  eta: 1 day, 0:34:37  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.8442  data: 0.0039
Test:  [ 390/3316]  eta: 23:53:07  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 29.2236  data: 0.0037
Test:  [ 380/3316]  eta: 1 day, 0:37:21  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.8975  data: 0.0039
Test:  [ 380/3316]  eta: 1 day, 0:37:55  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.9681  data: 0.0037
Test:  [ 380/3316]  eta: 1 day, 0:38:56  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 30.0859  data: 0.0034
Test:  [ 380/3316]  eta: 1 day, 0:40:07  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 30.0292  data: 0.0036
Test:  [ 390/3316]  eta: 23:58:14  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 29.2725  data: 0.0034
Test:  [ 390/3316]  eta: 1 day, 0:16:26  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 29.4310  data: 0.0037
Test:  [ 390/3316]  eta: 1 day, 0:29:25  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 29.7738  data: 0.0035
Test:  [ 400/3316]  eta: 23:48:07  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 29.0970  data: 0.0028
Test:  [ 390/3316]  eta: 1 day, 0:33:29  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 30.3748  data: 0.0034
Test:  [ 390/3316]  eta: 1 day, 0:34:02  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 30.4386  data: 0.0030
Test:  [ 390/3316]  eta: 1 day, 0:35:00  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 30.4941  data: 0.0031
Test:  [ 390/3316]  eta: 1 day, 0:36:06  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 30.4318  data: 0.0034
Test:  [ 400/3316]  eta: 23:54:24  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 29.7073  data: 0.0030
Test:  [ 400/3316]  eta: 1 day, 0:12:33  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.2752  data: 0.0041
Test:  [ 410/3316]  eta: 23:43:55  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.6408  data: 0.0040
Test:  [ 400/3316]  eta: 1 day, 0:25:07  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.3657  data: 0.0044
Test:  [ 400/3316]  eta: 1 day, 0:27:55  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.4464  data: 0.0047
Test:  [ 400/3316]  eta: 1 day, 0:28:34  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.5047  data: 0.0044
Test:  [ 400/3316]  eta: 1 day, 0:29:30  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.4837  data: 0.0039
Test:  [ 410/3316]  eta: 23:49:08  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.8045  data: 0.0041
Test:  [ 400/3316]  eta: 1 day, 0:30:23  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.3894  data: 0.0043
Test:  [ 410/3316]  eta: 1 day, 0:07:09  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 30.1475  data: 0.0043
Test:  [ 420/3316]  eta: 23:38:42  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.5488  data: 0.0040
Test:  [ 410/3316]  eta: 1 day, 0:19:37  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 30.2342  data: 0.0042
Test:  [ 410/3316]  eta: 1 day, 0:22:45  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.9350  data: 0.0047
Test:  [ 410/3316]  eta: 1 day, 0:23:19  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.9501  data: 0.0046
Test:  [ 420/3316]  eta: 23:44:04  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.2977  data: 0.0040
Test:  [ 410/3316]  eta: 1 day, 0:24:10  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.9292  data: 0.0038
Test:  [ 410/3316]  eta: 1 day, 0:24:54  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.7961  data: 0.0042
Test:  [ 420/3316]  eta: 1 day, 0:01:30  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.4142  data: 0.0031
Test:  [ 430/3316]  eta: 23:33:13  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 28.9938  data: 0.0027
Test:  [ 420/3316]  eta: 1 day, 0:13:47  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.5827  data: 0.0027
Test:  [ 420/3316]  eta: 1 day, 0:16:59  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.8288  data: 0.0031
Test:  [ 430/3316]  eta: 23:38:29  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.1395  data: 0.0030
Test:  [ 420/3316]  eta: 1 day, 0:17:35  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.8151  data: 0.0029
Test:  [ 420/3316]  eta: 1 day, 0:18:22  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.7828  data: 0.0027
Test:  [ 420/3316]  eta: 1 day, 0:19:02  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.7041  data: 0.0029
Test:  [ 430/3316]  eta: 23:56:02  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.3650  data: 0.0026
Test:  [ 440/3316]  eta: 23:28:06  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.0296  data: 0.0027
Test:  [ 430/3316]  eta: 1 day, 0:08:21  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.5894  data: 0.0025
Test:  [ 440/3316]  eta: 23:33:16  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.0634  data: 0.0030
Test:  [ 430/3316]  eta: 1 day, 0:11:39  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.7424  data: 0.0030
Test:  [ 430/3316]  eta: 1 day, 0:12:21  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.8129  data: 0.0029
Test:  [ 430/3316]  eta: 1 day, 0:13:30  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.9653  data: 0.0027
Test:  [ 430/3316]  eta: 1 day, 0:14:17  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 30.0053  data: 0.0027
Test:  [ 440/3316]  eta: 23:51:12  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.7087  data: 0.0029
Test:  [ 450/3316]  eta: 23:23:05  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.2210  data: 0.0032
Test:  [ 440/3316]  eta: 1 day, 0:03:09  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.8467  data: 0.0028
Test:  [ 450/3316]  eta: 23:28:10  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.2642  data: 0.0035
Test:  [ 440/3316]  eta: 1 day, 0:06:10  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.8424  data: 0.0034
Test:  [ 440/3316]  eta: 1 day, 0:07:07  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 30.0198  data: 0.0031
Test:  [ 440/3316]  eta: 1 day, 0:07:45  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.9605  data: 0.0033
Test:  [ 440/3316]  eta: 1 day, 0:08:33  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 30.0402  data: 0.0032
Test:  [ 450/3316]  eta: 23:45:48  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.7185  data: 0.0032
Test:  [ 460/3316]  eta: 23:18:01  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 29.2350  data: 0.0033
Test:  [ 450/3316]  eta: 23:57:52  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.9001  data: 0.0030
Test:  [ 460/3316]  eta: 23:23:08  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 29.3351  data: 0.0034
Test:  [ 450/3316]  eta: 1 day, 0:01:09  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.9673  data: 0.0034
Test:  [ 450/3316]  eta: 1 day, 0:02:06  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 30.1098  data: 0.0031
Test:  [ 450/3316]  eta: 1 day, 0:02:29  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.7709  data: 0.0034
Test:  [ 450/3316]  eta: 1 day, 0:03:21  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.8342  data: 0.0034
Test:  [ 460/3316]  eta: 23:41:19  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 29.8940  data: 0.0031
Test:  [ 470/3316]  eta: 23:13:54  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 29.6807  data: 0.0038
Test:  [ 460/3316]  eta: 23:53:15  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 30.1711  data: 0.0041
Test:  [ 470/3316]  eta: 23:18:50  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 29.7262  data: 0.0046
Test:  [ 460/3316]  eta: 23:56:39  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 30.4298  data: 0.0046
Test:  [ 460/3316]  eta: 23:57:44  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 30.5162  data: 0.0044
Test:  [ 460/3316]  eta: 23:57:57  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 30.3356  data: 0.0042
Test:  [ 460/3316]  eta: 23:58:54  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 30.4263  data: 0.0045
Test:  [ 470/3316]  eta: 23:35:57  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 29.9005  data: 0.0049
Test:  [ 480/3316]  eta: 23:08:28  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.4939  data: 0.0046
Test:  [ 470/3316]  eta: 23:47:30  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 29.9400  data: 0.0045
Test:  [ 480/3316]  eta: 23:13:22  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.5110  data: 0.0053
Test:  [ 470/3316]  eta: 23:51:14  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 30.2400  data: 0.0051
Test:  [ 470/3316]  eta: 23:52:15  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 30.2969  data: 0.0050
Test:  [ 470/3316]  eta: 23:52:25  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 30.2014  data: 0.0046
Test:  [ 470/3316]  eta: 23:53:25  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 30.2865  data: 0.0048
Test:  [ 480/3316]  eta: 23:30:49  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.5810  data: 0.0050
Test:  [ 490/3316]  eta: 23:03:13  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 28.9194  data: 0.0036
Test:  [ 490/3316]  eta: 23:08:04  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 28.9984  data: 0.0037
Test:  [ 480/3316]  eta: 23:42:04  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.5273  data: 0.0034
Test:  [ 480/3316]  eta: 23:45:44  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.7456  data: 0.0035
Test:  [ 480/3316]  eta: 23:46:48  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.7683  data: 0.0036
Test:  [ 480/3316]  eta: 23:46:55  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.7155  data: 0.0035
Test:  [ 480/3316]  eta: 23:47:49  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.7174  data: 0.0036
Test:  [ 490/3316]  eta: 23:25:16  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.4722  data: 0.0031
Test:  [ 500/3316]  eta: 22:57:51  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 28.9282  data: 0.0028
Test:  [ 500/3316]  eta: 23:02:34  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 28.9575  data: 0.0028
Test:  [ 490/3316]  eta: 23:36:26  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.5546  data: 0.0030
Test:  [ 490/3316]  eta: 23:40:16  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.7070  data: 0.0029
Test:  [ 490/3316]  eta: 23:41:18  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.7347  data: 0.0030
Test:  [ 490/3316]  eta: 23:41:26  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.7177  data: 0.0028
Test:  [ 490/3316]  eta: 23:42:21  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.7044  data: 0.0030
Test:  [ 500/3316]  eta: 23:20:13  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 29.4962  data: 0.0029
Test:  [ 510/3316]  eta: 22:53:23  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 29.3440  data: 0.0029
Test:  [ 510/3316]  eta: 22:57:57  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 29.3150  data: 0.0030
Test:  [ 500/3316]  eta: 23:31:48  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 29.9646  data: 0.0034
Test:  [ 500/3316]  eta: 23:35:33  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 30.0922  data: 0.0034
Test:  [ 500/3316]  eta: 23:36:36  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 30.1094  data: 0.0037
Test:  [ 500/3316]  eta: 23:36:50  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 30.1772  data: 0.0033
Test:  [ 500/3316]  eta: 23:37:38  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 30.1508  data: 0.0039
Test:  [ 510/3316]  eta: 23:15:12  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 29.7628  data: 0.0033
Test:  [ 520/3316]  eta: 22:48:14  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 29.4537  data: 0.0037
Test:  [ 520/3316]  eta: 22:52:36  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 29.3748  data: 0.0032
Test:  [ 510/3316]  eta: 23:26:21  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 30.0434  data: 0.0036
Test:  [ 510/3316]  eta: 23:30:07  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 30.0999  data: 0.0035
Test:  [ 510/3316]  eta: 23:31:15  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 30.1777  data: 0.0041
Test:  [ 510/3316]  eta: 23:31:28  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 30.2293  data: 0.0038
Test:  [ 510/3316]  eta: 23:32:14  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 30.1685  data: 0.0041
Test:  [ 520/3316]  eta: 23:10:04  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 29.7228  data: 0.0033
Test:  [ 530/3316]  eta: 22:43:23  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 29.2459  data: 0.0037
Test:  [ 530/3316]  eta: 22:47:44  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 29.2328  data: 0.0032
Test:  [ 520/3316]  eta: 23:21:25  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 29.8801  data: 0.0030
Test:  [ 520/3316]  eta: 23:25:54  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 30.3942  data: 0.0031
Test:  [ 520/3316]  eta: 23:27:01  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 30.4547  data: 0.0034
Test:  [ 520/3316]  eta: 23:27:14  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 30.4521  data: 0.0033
Test:  [ 520/3316]  eta: 23:27:54  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 30.3925  data: 0.0033
Test:  [ 530/3316]  eta: 23:05:29  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 29.9654  data: 0.0039
Test:  [ 540/3316]  eta: 22:38:51  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 29.5978  data: 0.0046
Test:  [ 540/3316]  eta: 22:43:08  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 29.6600  data: 0.0047
Test:  [ 530/3316]  eta: 23:16:41  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 30.2746  data: 0.0047
Test:  [ 530/3316]  eta: 23:20:47  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 30.5592  data: 0.0049
Test:  [ 530/3316]  eta: 23:21:38  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 30.4333  data: 0.0047
Test:  [ 530/3316]  eta: 23:22:04  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 30.5583  data: 0.0047
Test:  [ 530/3316]  eta: 23:22:29  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 30.3791  data: 0.0044
Test:  [ 540/3316]  eta: 23:00:18  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 29.9338  data: 0.0047
Test:  [ 550/3316]  eta: 22:33:54  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 29.5482  data: 0.0049
Test:  [ 550/3316]  eta: 22:38:06  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 29.5745  data: 0.0046
Test:  [ 540/3316]  eta: 23:11:39  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 30.2220  data: 0.0047
Test:  [ 540/3316]  eta: 23:15:54  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 30.1973  data: 0.0047
Test:  [ 540/3316]  eta: 23:16:37  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 29.9954  data: 0.0047
Test:  [ 540/3316]  eta: 23:17:15  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 30.2441  data: 0.0048
Test:  [ 540/3316]  eta: 23:17:28  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 30.0027  data: 0.0044
Test:  [ 550/3316]  eta: 22:55:14  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 29.6563  data: 0.0037
Test:  [ 560/3316]  eta: 22:29:00  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 29.3389  data: 0.0035
Test:  [ 560/3316]  eta: 22:33:03  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 29.3067  data: 0.0034
Test:  [ 550/3316]  eta: 23:06:32  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 30.0104  data: 0.0032
Test:  [ 550/3316]  eta: 23:10:51  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 30.2371  data: 0.0029
Test:  [ 550/3316]  eta: 23:11:34  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 30.1844  data: 0.0032
Test:  [ 550/3316]  eta: 23:12:15  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 30.3513  data: 0.0032
Test:  [ 550/3316]  eta: 23:12:25  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 30.2056  data: 0.0031
Test:  [ 560/3316]  eta: 22:50:13  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 29.7516  data: 0.0030
Test:  [ 570/3316]  eta: 22:24:06  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 29.3575  data: 0.0033
Test:  [ 570/3316]  eta: 22:28:08  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 29.3622  data: 0.0035
Test:  [ 560/3316]  eta: 23:01:28  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 29.9815  data: 0.0032
Test:  [ 560/3316]  eta: 23:05:44  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 30.1039  data: 0.0030
Test:  [ 560/3316]  eta: 23:06:53  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 30.3870  data: 0.0034
Test:  [ 560/3316]  eta: 23:07:57  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 30.6640  data: 0.0032
Test:  [ 560/3316]  eta: 23:07:57  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 30.5398  data: 0.0031
Test:  [ 570/3316]  eta: 22:45:48  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.1564  data: 0.0041
Test:  [ 580/3316]  eta: 22:19:42  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 29.6773  data: 0.0045
Test:  [ 580/3316]  eta: 22:23:40  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 29.7265  data: 0.0049
Test:  [ 570/3316]  eta: 22:56:49  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.2603  data: 0.0056
Test:  [ 570/3316]  eta: 23:00:52  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.2194  data: 0.0051
Test:  [ 570/3316]  eta: 23:01:56  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.4459  data: 0.0058
Test:  [ 570/3316]  eta: 23:02:36  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.3563  data: 0.0053
Test:  [ 570/3316]  eta: 23:02:46  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.5590  data: 0.0054
Test:  [ 580/3316]  eta: 22:40:30  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 29.9686  data: 0.0047
Test:  [ 590/3316]  eta: 22:14:29  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.4841  data: 0.0047
Test:  [ 590/3316]  eta: 22:18:23  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.5072  data: 0.0050
Test:  [ 580/3316]  eta: 22:51:27  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 30.0806  data: 0.0054
Test:  [ 580/3316]  eta: 22:55:40  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 30.1650  data: 0.0050
Test:  [ 580/3316]  eta: 22:56:46  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 30.1563  data: 0.0056
Test:  [ 580/3316]  eta: 22:57:24  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 29.9150  data: 0.0052
Test:  [ 580/3316]  eta: 22:57:33  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 30.0127  data: 0.0054
Test:  [ 590/3316]  eta: 22:35:29  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.5987  data: 0.0038
Test:  [ 600/3316]  eta: 22:09:39  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.2056  data: 0.0034
Test:  [ 600/3316]  eta: 22:13:26  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.1975  data: 0.0033
Test:  [ 590/3316]  eta: 22:46:27  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.8673  data: 0.0031
Test:  [ 590/3316]  eta: 22:50:23  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.8928  data: 0.0029
Test:  [ 590/3316]  eta: 22:51:32  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.9857  data: 0.0030
Test:  [ 590/3316]  eta: 22:52:05  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.9245  data: 0.0030
Test:  [ 590/3316]  eta: 22:52:21  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.9895  data: 0.0032
Test:  [ 600/3316]  eta: 22:30:14  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.6203  data: 0.0031
Test:  [ 610/3316]  eta: 22:04:28  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 29.2233  data: 0.0031
Test:  [ 610/3316]  eta: 22:08:13  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 29.2231  data: 0.0030
Test:  [ 600/3316]  eta: 22:41:14  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.9526  data: 0.0032
Test:  [ 600/3316]  eta: 22:45:13  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.9030  data: 0.0028
Test:  [ 600/3316]  eta: 22:46:24  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.9923  data: 0.0029
Test:  [ 600/3316]  eta: 22:46:47  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.8364  data: 0.0028
Test:  [ 600/3316]  eta: 22:47:15  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 30.0565  data: 0.0032
Test:  [ 610/3316]  eta: 22:25:44  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 29.9722  data: 0.0041
Test:  [ 620/3316]  eta: 22:00:07  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 29.5548  data: 0.0040
Test:  [ 620/3316]  eta: 22:03:48  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 29.5880  data: 0.0042
Test:  [ 610/3316]  eta: 22:36:53  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 30.3906  data: 0.0051
Test:  [ 610/3316]  eta: 22:40:41  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 30.4083  data: 0.0057
Test:  [ 610/3316]  eta: 22:41:51  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 30.4412  data: 0.0049
Test:  [ 610/3316]  eta: 22:42:07  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 30.2692  data: 0.0049
Test:  [ 610/3316]  eta: 22:42:36  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 30.4269  data: 0.0049
Test:  [ 620/3316]  eta: 22:20:40  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.0960  data: 0.0054
Test:  [ 630/3316]  eta: 21:55:02  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 29.6110  data: 0.0052
Test:  [ 630/3316]  eta: 21:58:33  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 29.5477  data: 0.0050
Test:  [ 620/3316]  eta: 22:31:33  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.3122  data: 0.0053
Test:  [ 620/3316]  eta: 22:35:17  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.2483  data: 0.0059
Test:  [ 620/3316]  eta: 22:36:27  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.2631  data: 0.0049
Test:  [ 620/3316]  eta: 22:36:43  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.1901  data: 0.0050
Test:  [ 620/3316]  eta: 22:37:14  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.2355  data: 0.0048
Test:  [ 630/3316]  eta: 22:15:34  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 29.6756  data: 0.0043
Test:  [ 640/3316]  eta: 21:49:58  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 29.1240  data: 0.0042
Test:  [ 640/3316]  eta: 21:53:36  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 29.1874  data: 0.0038
Test:  [ 630/3316]  eta: 22:26:45  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 30.0046  data: 0.0034
Test:  [ 630/3316]  eta: 22:30:23  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 29.9952  data: 0.0033
Test:  [ 630/3316]  eta: 22:31:30  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 29.9974  data: 0.0033
Test:  [ 630/3316]  eta: 22:31:48  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 30.0238  data: 0.0032
Test:  [ 630/3316]  eta: 22:32:15  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 30.0181  data: 0.0032
Test:  [ 650/3316]  eta: 21:44:58  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.1707  data: 0.0031
Test:  [ 640/3316]  eta: 22:10:27  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 29.6344  data: 0.0031
Test:  [ 650/3316]  eta: 21:48:42  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.4334  data: 0.0035
Test:  [ 640/3316]  eta: 22:21:39  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 30.1635  data: 0.0033
Test:  [ 640/3316]  eta: 22:25:10  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 30.1178  data: 0.0035
Test:  [ 640/3316]  eta: 22:26:18  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 30.1255  data: 0.0042
Test:  [ 640/3316]  eta: 22:26:35  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 30.1551  data: 0.0038
Test:  [ 640/3316]  eta: 22:27:00  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 30.0962  data: 0.0038
Test:  [ 660/3316]  eta: 21:40:04  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 29.2840  data: 0.0036
Test:  [ 650/3316]  eta: 22:05:24  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.6700  data: 0.0030
Test:  [ 660/3316]  eta: 21:43:31  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 29.2558  data: 0.0036
Test:  [ 650/3316]  eta: 22:16:29  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.9125  data: 0.0032
Test:  [ 650/3316]  eta: 22:20:04  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.9739  data: 0.0031
Test:  [ 650/3316]  eta: 22:21:12  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 30.0121  data: 0.0038
Test:  [ 650/3316]  eta: 22:21:30  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 30.0291  data: 0.0039
Test:  [ 650/3316]  eta: 22:21:47  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.9111  data: 0.0037
Test:  [ 670/3316]  eta: 21:35:11  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 29.3787  data: 0.0034
Test:  [ 660/3316]  eta: 22:00:19  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 29.6987  data: 0.0028
Test:  [ 670/3316]  eta: 21:38:25  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 29.1074  data: 0.0033
Test:  [ 660/3316]  eta: 22:11:41  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 30.1268  data: 0.0031
Test:  [ 660/3316]  eta: 22:15:19  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 30.3125  data: 0.0033
Test:  [ 660/3316]  eta: 22:16:26  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 30.3378  data: 0.0036
Test:  [ 660/3316]  eta: 22:16:46  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 30.3700  data: 0.0039
Test:  [ 660/3316]  eta: 22:16:58  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 30.2187  data: 0.0039
Test:  [ 680/3316]  eta: 21:30:41  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 29.6858  data: 0.0041
Test:  [ 670/3316]  eta: 21:55:37  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 29.9551  data: 0.0041
Test:  [ 680/3316]  eta: 21:33:49  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 29.5332  data: 0.0047
Test:  [ 670/3316]  eta: 22:06:46  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 30.3152  data: 0.0053
Test:  [ 670/3316]  eta: 22:10:17  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 30.3616  data: 0.0047
Test:  [ 670/3316]  eta: 22:11:32  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 30.4904  data: 0.0048
Test:  [ 670/3316]  eta: 22:11:42  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 30.3841  data: 0.0047
Test:  [ 670/3316]  eta: 22:11:56  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 30.3563  data: 0.0050
Test:  [ 690/3316]  eta: 21:25:50  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 29.7216  data: 0.0050
Test:  [ 680/3316]  eta: 21:50:41  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.0636  data: 0.0047
Test:  [ 690/3316]  eta: 21:28:51  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 29.6382  data: 0.0053
Test:  [ 680/3316]  eta: 22:01:35  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.0315  data: 0.0053
Test:  [ 680/3316]  eta: 22:05:07  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.0453  data: 0.0041
Test:  [ 680/3316]  eta: 22:06:23  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.1998  data: 0.0043
Test:  [ 680/3316]  eta: 22:06:27  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.0027  data: 0.0042
Test:  [ 680/3316]  eta: 22:06:41  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.0448  data: 0.0043
Test:  [ 700/3316]  eta: 21:20:45  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 29.2755  data: 0.0039
Test:  [ 690/3316]  eta: 21:45:36  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 29.7759  data: 0.0036
Test:  [ 700/3316]  eta: 21:23:47  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 29.2754  data: 0.0038
Test:  [ 690/3316]  eta: 21:56:34  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 29.9454  data: 0.0031
Test:  [ 690/3316]  eta: 22:00:05  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 30.0501  data: 0.0030
Test:  [ 690/3316]  eta: 22:01:26  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 30.1649  data: 0.0031
Test:  [ 690/3316]  eta: 22:01:28  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 30.0733  data: 0.0033
Test:  [ 690/3316]  eta: 22:01:42  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 30.0765  data: 0.0031
Test:  [ 710/3316]  eta: 21:15:55  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 29.2762  data: 0.0031
Test:  [ 700/3316]  eta: 21:40:41  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 29.7915  data: 0.0032
Test:  [ 710/3316]  eta: 21:19:01  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 29.4348  data: 0.0030
Test:  [ 700/3316]  eta: 21:51:45  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 30.2290  data: 0.0031
Test:  [ 700/3316]  eta: 21:55:27  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 30.4746  data: 0.0037
Test:  [ 700/3316]  eta: 21:56:46  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 30.5029  data: 0.0040
Test:  [ 700/3316]  eta: 21:56:47  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 30.5677  data: 0.0038
Test:  [ 700/3316]  eta: 21:56:56  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 30.4506  data: 0.0039
Test:  [ 720/3316]  eta: 21:11:18  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6524  data: 0.0042
Test:  [ 710/3316]  eta: 21:35:56  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.0629  data: 0.0050
Test:  [ 720/3316]  eta: 21:14:15  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6840  data: 0.0040
Test:  [ 710/3316]  eta: 21:46:41  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.1981  data: 0.0050
Test:  [ 710/3316]  eta: 21:50:01  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.1557  data: 0.0048
Test:  [ 710/3316]  eta: 21:51:15  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.0594  data: 0.0047
Test:  [ 710/3316]  eta: 21:51:18  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.1331  data: 0.0048
Test:  [ 710/3316]  eta: 21:51:26  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.0385  data: 0.0048
Test:  [ 730/3316]  eta: 21:06:04  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.3304  data: 0.0049
Test:  [ 720/3316]  eta: 21:30:34  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6881  data: 0.0052
Test:  [ 730/3316]  eta: 21:09:04  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.3213  data: 0.0047
Test:  [ 720/3316]  eta: 21:41:35  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.9735  data: 0.0046
Test:  [ 720/3316]  eta: 21:44:55  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.7736  data: 0.0040
Test:  [ 720/3316]  eta: 21:46:01  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6261  data: 0.0040
Test:  [ 720/3316]  eta: 21:46:07  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6921  data: 0.0038
Test:  [ 740/3316]  eta: 21:01:04  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 28.9983  data: 0.0037
Test:  [ 720/3316]  eta: 21:46:13  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6637  data: 0.0038
Test:  [ 730/3316]  eta: 21:25:16  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.2185  data: 0.0034
Test:  [ 740/3316]  eta: 21:03:55  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 29.0103  data: 0.0037
Test:  [ 730/3316]  eta: 21:36:13  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.7143  data: 0.0029
Test:  [ 730/3316]  eta: 21:39:35  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.8350  data: 0.0028
Test:  [ 750/3316]  eta: 20:55:53  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 29.0323  data: 0.0029
Test:  [ 730/3316]  eta: 21:40:35  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.6889  data: 0.0030
Test:  [ 730/3316]  eta: 21:40:44  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.7710  data: 0.0029
Test:  [ 730/3316]  eta: 21:40:48  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.7093  data: 0.0027
Test:  [ 740/3316]  eta: 21:20:07  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 29.3935  data: 0.0030
Test:  [ 750/3316]  eta: 20:58:51  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 29.0959  data: 0.0030
Test:  [ 740/3316]  eta: 21:31:14  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 29.8104  data: 0.0029
Test:  [ 740/3316]  eta: 21:34:56  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 30.2250  data: 0.0030
Test:  [ 760/3316]  eta: 20:51:17  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 29.3906  data: 0.0034
Test:  [ 740/3316]  eta: 21:35:55  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 30.1644  data: 0.0036
Test:  [ 740/3316]  eta: 21:36:02  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 30.0884  data: 0.0034
Test:  [ 740/3316]  eta: 21:36:07  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 30.2435  data: 0.0035
Test:  [ 750/3316]  eta: 21:15:27  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 29.9260  data: 0.0040
Test:  [ 760/3316]  eta: 20:54:08  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 29.4633  data: 0.0036
Test:  [ 750/3316]  eta: 21:26:18  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 30.1746  data: 0.0043
Test:  [ 770/3316]  eta: 20:46:12  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.4499  data: 0.0042
Test:  [ 750/3316]  eta: 21:29:44  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 30.3302  data: 0.0039
Test:  [ 750/3316]  eta: 21:30:40  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 30.2878  data: 0.0043
Test:  [ 750/3316]  eta: 21:30:43  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 30.1611  data: 0.0040
Test:  [ 750/3316]  eta: 21:30:53  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 30.3619  data: 0.0043
Test:  [ 770/3316]  eta: 20:48:57  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.3534  data: 0.0040
Test:  [ 760/3316]  eta: 21:10:22  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 29.9732  data: 0.0046
Test:  [ 760/3316]  eta: 21:21:16  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 30.1268  data: 0.0045
Test:  [ 780/3316]  eta: 20:41:17  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.1700  data: 0.0037
Test:  [ 760/3316]  eta: 21:24:45  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 30.0395  data: 0.0039
Test:  [ 760/3316]  eta: 21:25:41  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 30.0276  data: 0.0037
Test:  [ 760/3316]  eta: 21:25:42  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 29.9335  data: 0.0036
Test:  [ 760/3316]  eta: 21:25:55  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 30.0622  data: 0.0038
Test:  [ 780/3316]  eta: 20:44:00  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.1366  data: 0.0034
Test:  [ 770/3316]  eta: 21:05:27  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.7689  data: 0.0035
Test:  [ 770/3316]  eta: 21:16:04  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.8877  data: 0.0030
Test:  [ 790/3316]  eta: 20:36:14  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.2141  data: 0.0028
Test:  [ 770/3316]  eta: 21:19:31  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 30.0166  data: 0.0032
Test:  [ 770/3316]  eta: 21:20:23  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.9263  data: 0.0031
Test:  [ 770/3316]  eta: 21:20:24  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 30.0031  data: 0.0031
Test:  [ 770/3316]  eta: 21:20:36  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.9821  data: 0.0030
Test:  [ 790/3316]  eta: 20:38:52  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.1865  data: 0.0029
Test:  [ 780/3316]  eta: 21:00:21  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.7545  data: 0.0030
Test:  [ 780/3316]  eta: 21:11:05  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.9260  data: 0.0041
Test:  [ 800/3316]  eta: 20:31:25  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.3061  data: 0.0043
Test:  [ 780/3316]  eta: 21:14:35  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 30.0677  data: 0.0047
Test:  [ 780/3316]  eta: 21:15:21  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.9160  data: 0.0049
Test:  [ 780/3316]  eta: 21:15:24  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.9786  data: 0.0048
Test:  [ 780/3316]  eta: 21:15:38  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.9892  data: 0.0047
Test:  [ 800/3316]  eta: 20:34:04  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.3261  data: 0.0047
Test:  [ 790/3316]  eta: 20:55:25  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.7478  data: 0.0053
Test:  [ 790/3316]  eta: 21:05:51  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.8944  data: 0.0045
Test:  [ 810/3316]  eta: 20:26:15  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.1717  data: 0.0047
Test:  [ 790/3316]  eta: 21:09:15  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.9449  data: 0.0046
Test:  [ 810/3316]  eta: 20:28:50  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.1989  data: 0.0048
Test:  [ 790/3316]  eta: 21:09:58  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.8403  data: 0.0049
Test:  [ 790/3316]  eta: 21:10:01  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.8612  data: 0.0047
Test:  [ 790/3316]  eta: 21:10:11  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.8418  data: 0.0048
Test:  [ 800/3316]  eta: 20:50:07  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.5327  data: 0.0054
Test:  [ 800/3316]  eta: 21:00:20  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.3901  data: 0.0031
Test:  [ 820/3316]  eta: 20:20:58  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 28.7086  data: 0.0032
Test:  [ 800/3316]  eta: 21:03:51  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.4963  data: 0.0027
Test:  [ 820/3316]  eta: 20:23:35  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 28.7501  data: 0.0028
Test:  [ 800/3316]  eta: 21:04:36  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.5185  data: 0.0027
Test:  [ 800/3316]  eta: 21:04:38  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.4936  data: 0.0028
Test:  [ 800/3316]  eta: 21:04:47  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.4311  data: 0.0029
Test:  [ 810/3316]  eta: 20:44:53  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.2335  data: 0.0029
Test:  [ 810/3316]  eta: 20:55:10  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.4215  data: 0.0028
Test:  [ 830/3316]  eta: 20:15:56  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 28.8321  data: 0.0031
Test:  [ 830/3316]  eta: 20:18:24  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 28.8013  data: 0.0027
Test:  [ 810/3316]  eta: 20:58:34  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.5363  data: 0.0028
Test:  [ 810/3316]  eta: 20:59:21  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.6222  data: 0.0026
Test:  [ 810/3316]  eta: 20:59:23  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.6086  data: 0.0028
Test:  [ 810/3316]  eta: 20:59:31  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.5745  data: 0.0027
Test:  [ 820/3316]  eta: 20:39:39  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.2937  data: 0.0029
Test:  [ 820/3316]  eta: 20:50:07  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.8561  data: 0.0041
Test:  [ 840/3316]  eta: 20:11:01  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.1743  data: 0.0042
Test:  [ 840/3316]  eta: 20:13:27  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.0901  data: 0.0043
Test:  [ 820/3316]  eta: 20:53:26  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.7808  data: 0.0043
Test:  [ 820/3316]  eta: 20:54:14  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.8443  data: 0.0045
Test:  [ 820/3316]  eta: 20:54:17  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.8653  data: 0.0046
Test:  [ 820/3316]  eta: 20:54:28  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.8978  data: 0.0047
Test:  [ 830/3316]  eta: 20:34:41  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 29.5379  data: 0.0044
Test:  [ 830/3316]  eta: 20:45:00  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 29.9022  data: 0.0046
Test:  [ 850/3316]  eta: 20:06:00  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.1879  data: 0.0043
Test:  [ 850/3316]  eta: 20:08:28  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.2717  data: 0.0048
Test:  [ 830/3316]  eta: 20:48:23  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 30.0003  data: 0.0044
Test:  [ 830/3316]  eta: 20:49:06  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 29.9590  data: 0.0047
Test:  [ 830/3316]  eta: 20:49:08  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 29.9605  data: 0.0049
Test:  [ 830/3316]  eta: 20:49:19  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 30.0051  data: 0.0049
Test:  [ 840/3316]  eta: 20:29:39  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.7375  data: 0.0044
Test:  [ 860/3316]  eta: 20:00:58  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 29.0676  data: 0.0031
Test:  [ 840/3316]  eta: 20:39:47  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.7319  data: 0.0034
Test:  [ 860/3316]  eta: 20:03:23  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 29.1180  data: 0.0032
Test:  [ 840/3316]  eta: 20:43:08  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.8585  data: 0.0030
Test:  [ 840/3316]  eta: 20:43:47  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.7475  data: 0.0030
Test:  [ 840/3316]  eta: 20:43:53  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.8009  data: 0.0033
Test:  [ 840/3316]  eta: 20:44:05  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.8119  data: 0.0030
Test:  [ 850/3316]  eta: 20:24:30  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.5548  data: 0.0029
Test:  [ 870/3316]  eta: 19:56:11  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 29.3034  data: 0.0032
Test:  [ 850/3316]  eta: 20:34:50  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.8957  data: 0.0028
Test:  [ 870/3316]  eta: 19:58:34  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 29.2896  data: 0.0028
Test:  [ 850/3316]  eta: 20:38:05  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.8607  data: 0.0031
Test:  [ 850/3316]  eta: 20:38:48  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.8896  data: 0.0035
Test:  [ 850/3316]  eta: 20:38:56  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.9815  data: 0.0030
Test:  [ 860/3316]  eta: 20:19:39  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 29.7311  data: 0.0028
Test:  [ 850/3316]  eta: 20:39:08  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 30.0217  data: 0.0030
Test:  [ 880/3316]  eta: 19:51:19  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.4882  data: 0.0048
Test:  [ 860/3316]  eta: 20:29:48  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 30.0687  data: 0.0040
Test:  [ 880/3316]  eta: 19:53:43  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.5400  data: 0.0036
Test:  [ 860/3316]  eta: 20:32:55  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 29.9513  data: 0.0044
Test:  [ 860/3316]  eta: 20:33:47  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 30.1843  data: 0.0051
Test:  [ 860/3316]  eta: 20:33:50  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 30.1382  data: 0.0042
Test:  [ 870/3316]  eta: 20:14:44  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 29.9713  data: 0.0040
Test:  [ 860/3316]  eta: 20:34:05  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 30.2025  data: 0.0044
Test:  [ 890/3316]  eta: 19:46:18  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.2265  data: 0.0051
Test:  [ 870/3316]  eta: 20:24:39  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 29.8675  data: 0.0044
Test:  [ 890/3316]  eta: 19:48:44  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.3740  data: 0.0038
Test:  [ 870/3316]  eta: 20:27:45  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 29.8067  data: 0.0044
Test:  [ 870/3316]  eta: 20:28:43  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 30.0962  data: 0.0048
Test:  [ 880/3316]  eta: 20:09:50  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.9132  data: 0.0042
Test:  [ 870/3316]  eta: 20:28:45  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 30.0082  data: 0.0045
Test:  [ 870/3316]  eta: 20:29:02  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 30.0905  data: 0.0045
Test:  [ 900/3316]  eta: 19:41:14  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.0025  data: 0.0035
Test:  [ 880/3316]  eta: 20:19:23  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.6164  data: 0.0032
Test:  [ 900/3316]  eta: 19:43:37  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.0826  data: 0.0033
Test:  [ 880/3316]  eta: 20:22:22  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.5552  data: 0.0031
Test:  [ 890/3316]  eta: 20:04:34  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.5250  data: 0.0031
Test:  [ 880/3316]  eta: 20:23:26  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.7508  data: 0.0032
Test:  [ 880/3316]  eta: 20:23:27  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.8255  data: 0.0031
Test:  [ 880/3316]  eta: 20:23:40  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.7555  data: 0.0031
Test:  [ 910/3316]  eta: 19:36:14  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.0221  data: 0.0028
Test:  [ 890/3316]  eta: 20:14:12  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.5636  data: 0.0032
Test:  [ 910/3316]  eta: 19:38:41  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.1073  data: 0.0033
Test:  [ 890/3316]  eta: 20:17:12  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.5620  data: 0.0037
Test:  [ 900/3316]  eta: 19:59:40  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.5383  data: 0.0037
Test:  [ 890/3316]  eta: 20:18:23  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.7933  data: 0.0039
Test:  [ 890/3316]  eta: 20:18:26  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.8823  data: 0.0038
Test:  [ 890/3316]  eta: 20:18:38  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.7759  data: 0.0039
Test:  [ 920/3316]  eta: 19:31:14  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 29.0814  data: 0.0043
Test:  [ 900/3316]  eta: 20:08:55  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.5167  data: 0.0045
Test:  [ 920/3316]  eta: 19:33:38  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 29.1708  data: 0.0043
Test:  [ 900/3316]  eta: 20:11:51  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.5843  data: 0.0044
Test:  [ 910/3316]  eta: 19:54:27  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.5788  data: 0.0044
Test:  [ 900/3316]  eta: 20:13:04  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.7773  data: 0.0045
Test:  [ 900/3316]  eta: 20:13:08  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.8433  data: 0.0045
Test:  [ 900/3316]  eta: 20:13:17  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.7532  data: 0.0046
Test:  [ 930/3316]  eta: 19:26:02  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 28.8268  data: 0.0043
Test:  [ 910/3316]  eta: 20:03:33  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.2974  data: 0.0041
Test:  [ 930/3316]  eta: 19:28:32  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 28.9835  data: 0.0037
Test:  [ 910/3316]  eta: 20:06:32  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.3905  data: 0.0032
Test:  [ 920/3316]  eta: 19:49:16  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 29.2486  data: 0.0034
Test:  [ 910/3316]  eta: 20:07:51  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.5708  data: 0.0033
Test:  [ 910/3316]  eta: 20:07:56  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.6142  data: 0.0035
Test:  [ 910/3316]  eta: 20:08:06  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.5785  data: 0.0034
Test:  [ 940/3316]  eta: 19:21:10  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 28.9816  data: 0.0027
Test:  [ 920/3316]  eta: 19:58:33  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 29.6136  data: 0.0030
Test:  [ 940/3316]  eta: 19:23:42  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.2210  data: 0.0030
Test:  [ 920/3316]  eta: 20:01:38  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 29.8760  data: 0.0028
Test:  [ 930/3316]  eta: 19:44:23  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.6329  data: 0.0032
Test:  [ 920/3316]  eta: 20:02:56  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 30.0248  data: 0.0035
Test:  [ 920/3316]  eta: 20:03:00  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 30.0230  data: 0.0038
Test:  [ 920/3316]  eta: 20:03:10  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 30.0528  data: 0.0030
Test:  [ 950/3316]  eta: 19:16:14  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.3059  data: 0.0041
Test:  [ 930/3316]  eta: 19:53:25  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.8601  data: 0.0044
Test:  [ 950/3316]  eta: 19:18:45  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.3941  data: 0.0044
Test:  [ 930/3316]  eta: 19:56:17  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.8124  data: 0.0038
Test:  [ 940/3316]  eta: 19:39:10  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.5911  data: 0.0043
Test:  [ 930/3316]  eta: 19:57:36  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.8825  data: 0.0045
Test:  [ 930/3316]  eta: 19:57:44  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.9355  data: 0.0048
Test:  [ 930/3316]  eta: 19:57:53  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.9279  data: 0.0039
Test:  [ 960/3316]  eta: 19:11:16  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.1710  data: 0.0042
Test:  [ 940/3316]  eta: 19:48:20  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.7636  data: 0.0043
Test:  [ 960/3316]  eta: 19:13:51  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.3218  data: 0.0042
Test:  [ 940/3316]  eta: 19:51:16  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.6813  data: 0.0037
Test:  [ 950/3316]  eta: 19:34:10  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.4371  data: 0.0043
Test:  [ 940/3316]  eta: 19:52:31  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.6814  data: 0.0037
Test:  [ 940/3316]  eta: 19:52:42  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.8054  data: 0.0039
Test:  [ 940/3316]  eta: 19:52:46  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.7104  data: 0.0037
Test:  [ 970/3316]  eta: 19:06:11  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 28.9761  data: 0.0028
Test:  [ 970/3316]  eta: 19:08:45  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.1445  data: 0.0029
Test:  [ 950/3316]  eta: 19:43:02  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.5495  data: 0.0030
Test:  [ 950/3316]  eta: 19:46:00  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.7844  data: 0.0029
Test:  [ 960/3316]  eta: 19:28:55  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.3668  data: 0.0032
Test:  [ 950/3316]  eta: 19:47:11  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.6625  data: 0.0028
Test:  [ 950/3316]  eta: 19:47:23  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.7384  data: 0.0029
Test:  [ 950/3316]  eta: 19:47:26  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.6347  data: 0.0029
Test:  [ 980/3316]  eta: 19:01:17  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.0621  data: 0.0033
Test:  [ 980/3316]  eta: 19:03:55  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.2264  data: 0.0040
Test:  [ 960/3316]  eta: 19:38:07  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.7392  data: 0.0041
Test:  [ 960/3316]  eta: 19:41:07  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.9449  data: 0.0045
Test:  [ 970/3316]  eta: 19:24:07  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.6154  data: 0.0048
Test:  [ 960/3316]  eta: 19:42:16  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.8444  data: 0.0045
Test:  [ 960/3316]  eta: 19:42:27  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.8646  data: 0.0047
Test:  [ 960/3316]  eta: 19:42:35  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.9348  data: 0.0046
Test:  [ 990/3316]  eta: 18:56:20  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 29.2089  data: 0.0042
Test:  [ 990/3316]  eta: 18:58:55  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 29.3162  data: 0.0045
Test:  [ 970/3316]  eta: 19:32:56  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.8855  data: 0.0041
Test:  [ 970/3316]  eta: 19:35:53  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.9561  data: 0.0045
Test:  [ 980/3316]  eta: 19:18:57  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.7199  data: 0.0049
Test:  [ 970/3316]  eta: 19:37:02  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.9648  data: 0.0046
Test:  [ 970/3316]  eta: 19:37:12  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.9379  data: 0.0047
Test:  [ 970/3316]  eta: 19:37:23  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 30.0983  data: 0.0047
Test:  [1000/3316]  eta: 18:51:22  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.1065  data: 0.0036
Test:  [1000/3316]  eta: 18:54:00  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.2050  data: 0.0033
Test:  [ 980/3316]  eta: 19:27:53  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.7244  data: 0.0029
Test:  [ 990/3316]  eta: 19:14:02  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 29.5776  data: 0.0030
Test:  [ 980/3316]  eta: 19:30:59  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.9347  data: 0.0030
Test:  [ 980/3316]  eta: 19:32:04  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.9020  data: 0.0029
Test:  [ 980/3316]  eta: 19:32:11  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.8238  data: 0.0029
Test:  [ 980/3316]  eta: 19:32:25  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.9552  data: 0.0029
Test:  [1010/3316]  eta: 18:46:46  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.5875  data: 0.0029
Test:  [1010/3316]  eta: 18:49:16  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.5819  data: 0.0033
Test:  [ 990/3316]  eta: 19:22:59  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 30.0663  data: 0.0035
Test:  [1000/3316]  eta: 19:09:09  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.9279  data: 0.0050
Test:  [ 990/3316]  eta: 19:25:56  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 30.1815  data: 0.0041
Test:  [ 990/3316]  eta: 19:27:04  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 30.1747  data: 0.0048
Test:  [ 990/3316]  eta: 19:27:09  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 30.1004  data: 0.0050
Test:  [ 990/3316]  eta: 19:27:25  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 30.1941  data: 0.0040
Test:  [1020/3316]  eta: 18:41:42  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.4471  data: 0.0041
Test:  [1020/3316]  eta: 18:44:11  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.3493  data: 0.0041
Test:  [1000/3316]  eta: 19:17:44  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.7886  data: 0.0038
Test:  [1010/3316]  eta: 19:04:05  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.7383  data: 0.0052
Test:  [1000/3316]  eta: 19:20:48  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.8902  data: 0.0042
Test:  [1000/3316]  eta: 19:21:58  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 30.0139  data: 0.0049
Test:  [1000/3316]  eta: 19:22:00  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.9299  data: 0.0052
Test:  [1000/3316]  eta: 19:22:19  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 30.0475  data: 0.0043
Test:  [1030/3316]  eta: 18:36:49  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.0589  data: 0.0040
Test:  [1030/3316]  eta: 18:39:12  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.0170  data: 0.0036
Test:  [1010/3316]  eta: 19:12:35  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.4685  data: 0.0035
Test:  [1020/3316]  eta: 18:58:57  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.3883  data: 0.0033
Test:  [1010/3316]  eta: 19:15:36  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.6742  data: 0.0032
Test:  [1010/3316]  eta: 19:16:46  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.6311  data: 0.0033
Test:  [1010/3316]  eta: 19:16:46  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.7705  data: 0.0031
Test:  [1010/3316]  eta: 19:17:06  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.7509  data: 0.0034
Test:  [1040/3316]  eta: 18:31:42  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 28.9977  data: 0.0027
Test:  [1040/3316]  eta: 18:34:05  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 28.9685  data: 0.0029
Test:  [1020/3316]  eta: 19:07:25  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.5727  data: 0.0030
Test:  [1030/3316]  eta: 18:53:56  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.4461  data: 0.0035
Test:  [1020/3316]  eta: 19:10:35  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.8086  data: 0.0033
Test:  [1020/3316]  eta: 19:11:44  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.7929  data: 0.0043
Test:  [1020/3316]  eta: 19:11:45  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.8769  data: 0.0036
Test:  [1020/3316]  eta: 19:12:05  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.8426  data: 0.0039
Test:  [1050/3316]  eta: 18:26:55  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.1354  data: 0.0040
Test:  [1050/3316]  eta: 18:29:17  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.2021  data: 0.0045
Test:  [1030/3316]  eta: 19:02:23  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.7218  data: 0.0041
Test:  [1040/3316]  eta: 18:48:51  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.5382  data: 0.0044
Test:  [1030/3316]  eta: 19:05:29  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.9256  data: 0.0042
Test:  [1030/3316]  eta: 19:06:35  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.9098  data: 0.0050
Test:  [1030/3316]  eta: 19:06:37  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.9306  data: 0.0039
Test:  [1030/3316]  eta: 19:06:54  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.8910  data: 0.0042
Test:  [1060/3316]  eta: 18:21:52  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 29.2224  data: 0.0044
Test:  [1060/3316]  eta: 18:24:10  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 29.1894  data: 0.0047
Test:  [1040/3316]  eta: 18:57:07  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.5621  data: 0.0041
Test:  [1050/3316]  eta: 18:43:39  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.2561  data: 0.0038
Test:  [1040/3316]  eta: 19:00:09  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.4909  data: 0.0037
Test:  [1040/3316]  eta: 19:01:23  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.6552  data: 0.0036
Test:  [1040/3316]  eta: 19:01:25  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.6793  data: 0.0032
Test:  [1040/3316]  eta: 19:01:41  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.6162  data: 0.0034
Test:  [1070/3316]  eta: 18:16:51  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 28.8848  data: 0.0032
Test:  [1070/3316]  eta: 18:19:12  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 28.9331  data: 0.0030
Test:  [1050/3316]  eta: 18:52:08  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.6155  data: 0.0028
Test:  [1060/3316]  eta: 18:38:40  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 29.3694  data: 0.0030
Test:  [1050/3316]  eta: 18:55:10  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.6688  data: 0.0029
Test:  [1050/3316]  eta: 18:56:29  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.9989  data: 0.0035
Test:  [1050/3316]  eta: 18:56:34  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 30.0632  data: 0.0035
Test:  [1050/3316]  eta: 18:56:49  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 30.0185  data: 0.0038
Test:  [1080/3316]  eta: 18:12:03  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.2506  data: 0.0039
Test:  [1080/3316]  eta: 18:14:23  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.3595  data: 0.0045
Test:  [1060/3316]  eta: 18:47:15  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 30.1604  data: 0.0044
Test:  [1070/3316]  eta: 18:33:46  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 29.8134  data: 0.0049
Test:  [1060/3316]  eta: 18:49:58  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 29.8443  data: 0.0048
Test:  [1060/3316]  eta: 18:51:22  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 30.1091  data: 0.0050
Test:  [1060/3316]  eta: 18:51:25  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 30.1318  data: 0.0049
Test:  [1060/3316]  eta: 18:51:43  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 30.1823  data: 0.0050
Test:  [1090/3316]  eta: 18:07:06  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.3259  data: 0.0047
Test:  [1090/3316]  eta: 18:09:26  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.3978  data: 0.0051
Test:  [1070/3316]  eta: 18:42:13  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 30.0957  data: 0.0046
Test:  [1080/3316]  eta: 18:28:42  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.6954  data: 0.0048
Test:  [1070/3316]  eta: 18:44:51  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 29.6119  data: 0.0048
Test:  [1070/3316]  eta: 18:46:17  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 29.8456  data: 0.0043
Test:  [1070/3316]  eta: 18:46:20  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 29.8188  data: 0.0043
Test:  [1070/3316]  eta: 18:46:39  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 29.9216  data: 0.0042
Test:  [1100/3316]  eta: 18:02:08  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.0669  data: 0.0038
Test:  [1100/3316]  eta: 18:04:23  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.0518  data: 0.0037
Test:  [1080/3316]  eta: 18:37:04  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.6988  data: 0.0032
Test:  [1090/3316]  eta: 18:23:35  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.3657  data: 0.0029
Test:  [1080/3316]  eta: 18:39:41  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.6495  data: 0.0031
Test:  [1080/3316]  eta: 18:41:01  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.6160  data: 0.0030
Test:  [1080/3316]  eta: 18:41:05  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.6528  data: 0.0030
Test:  [1080/3316]  eta: 18:41:27  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.7556  data: 0.0031
Test:  [1110/3316]  eta: 17:57:08  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 29.0081  data: 0.0029
Test:  [1110/3316]  eta: 17:59:23  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 28.9733  data: 0.0034
Test:  [1090/3316]  eta: 18:32:01  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.6596  data: 0.0033
Test:  [1100/3316]  eta: 18:18:39  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.5451  data: 0.0029
Test:  [1090/3316]  eta: 18:34:39  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.7970  data: 0.0035
Test:  [1090/3316]  eta: 18:35:59  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.6981  data: 0.0035
Test:  [1090/3316]  eta: 18:36:03  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.7045  data: 0.0036
Test:  [1090/3316]  eta: 18:36:28  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.8758  data: 0.0034
Test:  [1120/3316]  eta: 17:52:12  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 29.0464  data: 0.0033
Test:  [1120/3316]  eta: 17:54:20  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 28.9633  data: 0.0035
Test:  [1100/3316]  eta: 18:26:48  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.5606  data: 0.0031
Test:  [1110/3316]  eta: 18:13:30  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 29.4896  data: 0.0030
Test:  [1100/3316]  eta: 18:29:21  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.5712  data: 0.0034
Test:  [1100/3316]  eta: 18:30:47  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.7829  data: 0.0033
Test:  [1100/3316]  eta: 18:30:48  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.6910  data: 0.0035
Test:  [1130/3316]  eta: 17:47:08  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 28.9387  data: 0.0032
Test:  [1100/3316]  eta: 18:31:16  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.8692  data: 0.0032
Test:  [1130/3316]  eta: 17:49:14  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 28.7907  data: 0.0030
Test:  [1110/3316]  eta: 18:21:52  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 29.7316  data: 0.0027
Test:  [1120/3316]  eta: 18:08:43  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 29.7123  data: 0.0034
Test:  [1110/3316]  eta: 18:24:22  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 29.6352  data: 0.0044
Test:  [1110/3316]  eta: 18:25:55  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 30.0180  data: 0.0037
Test:  [1110/3316]  eta: 18:25:55  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 29.9389  data: 0.0040
Test:  [1140/3316]  eta: 17:42:22  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 29.1963  data: 0.0044
Test:  [1110/3316]  eta: 18:26:23  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 30.0218  data: 0.0043
Test:  [1140/3316]  eta: 17:44:25  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 29.1560  data: 0.0047
Test:  [1120/3316]  eta: 18:16:45  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 29.8750  data: 0.0047
Test:  [1130/3316]  eta: 18:03:37  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.7983  data: 0.0047
Test:  [1120/3316]  eta: 18:19:08  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 29.7391  data: 0.0052
Test:  [1150/3316]  eta: 17:37:22  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 29.2813  data: 0.0049
Test:  [1120/3316]  eta: 18:20:45  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 30.0540  data: 0.0048
Test:  [1120/3316]  eta: 18:20:47  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 30.1077  data: 0.0046
Test:  [1120/3316]  eta: 18:21:13  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 30.0601  data: 0.0050
Test:  [1150/3316]  eta: 17:39:21  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 29.1984  data: 0.0051
Test:  [1130/3316]  eta: 18:11:41  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.6758  data: 0.0048
Test:  [1140/3316]  eta: 17:58:36  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 29.4381  data: 0.0043
Test:  [1130/3316]  eta: 18:13:58  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.4211  data: 0.0038
Test:  [1160/3316]  eta: 17:32:28  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 29.0625  data: 0.0031
Test:  [1130/3316]  eta: 18:15:43  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.7907  data: 0.0037
Test:  [1130/3316]  eta: 18:15:45  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.8558  data: 0.0035
Test:  [1130/3316]  eta: 18:16:10  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.7913  data: 0.0035
Test:  [1160/3316]  eta: 17:34:24  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 28.9707  data: 0.0032
Test:  [1140/3316]  eta: 18:06:37  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 29.7340  data: 0.0030
Test:  [1150/3316]  eta: 17:53:28  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 29.3597  data: 0.0031
Test:  [1140/3316]  eta: 18:08:53  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 29.6387  data: 0.0030
Test:  [1170/3316]  eta: 17:27:37  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 29.2986  data: 0.0029
Test:  [1140/3316]  eta: 18:10:47  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 30.1538  data: 0.0042
Test:  [1140/3316]  eta: 18:10:48  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 30.1485  data: 0.0042
Test:  [1140/3316]  eta: 18:11:15  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 30.1742  data: 0.0038
Test:  [1170/3316]  eta: 17:29:38  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 29.4400  data: 0.0040
Test:  [1150/3316]  eta: 18:01:47  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 30.1045  data: 0.0043
Test:  [1160/3316]  eta: 17:48:50  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 29.9832  data: 0.0039
Test:  [1150/3316]  eta: 18:04:13  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 30.4414  data: 0.0041
Test:  [1180/3316]  eta: 17:23:10  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 30.0606  data: 0.0039
Test:  [1150/3316]  eta: 18:06:22  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 31.1481  data: 0.0051
Test:  [1150/3316]  eta: 18:06:24  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 31.1627  data: 0.0053
Test:  [1150/3316]  eta: 18:06:51  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 31.2265  data: 0.0048
Test:  [1180/3316]  eta: 17:25:20  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 30.5312  data: 0.0049
Test:  [1160/3316]  eta: 17:57:25  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 31.2450  data: 0.0047
Test:  [1170/3316]  eta: 17:44:27  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 31.1845  data: 0.0045
Test:  [1160/3316]  eta: 17:59:32  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 31.1027  data: 0.0044
Test:  [1190/3316]  eta: 17:18:30  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 30.3922  data: 0.0042
Test:  [1160/3316]  eta: 18:01:24  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 31.1146  data: 0.0043
Test:  [1160/3316]  eta: 18:01:28  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 31.1911  data: 0.0041
Test:  [1160/3316]  eta: 18:01:49  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 31.0560  data: 0.0041
Test:  [1190/3316]  eta: 17:20:20  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 30.1555  data: 0.0039
Test:  [1170/3316]  eta: 17:52:12  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 30.6445  data: 0.0033
Test:  [1180/3316]  eta: 17:39:17  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 30.3543  data: 0.0034
Test:  [1170/3316]  eta: 17:54:15  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 30.1166  data: 0.0030
Test:  [1200/3316]  eta: 17:13:27  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.4073  data: 0.0033
Test:  [1170/3316]  eta: 17:56:26  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 30.2651  data: 0.0032
Test:  [1170/3316]  eta: 17:56:30  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 30.3112  data: 0.0032
Test:  [1170/3316]  eta: 17:56:50  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 30.1330  data: 0.0030
Test:  [1200/3316]  eta: 17:15:29  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.2614  data: 0.0036
Test:  [1180/3316]  eta: 17:47:21  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 29.8673  data: 0.0043
Test:  [1190/3316]  eta: 17:34:29  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.6947  data: 0.0045
Test:  [1180/3316]  eta: 17:49:16  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 29.6249  data: 0.0050
Test:  [1210/3316]  eta: 17:08:43  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.2903  data: 0.0046
Test:  [1180/3316]  eta: 17:51:19  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 30.0090  data: 0.0046
Test:  [1180/3316]  eta: 17:51:21  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 29.9651  data: 0.0049
Test:  [1180/3316]  eta: 17:51:40  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 29.9267  data: 0.0048
Test:  [1210/3316]  eta: 17:10:27  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.2059  data: 0.0050
Test:  [1190/3316]  eta: 17:42:08  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.8317  data: 0.0048
Test:  [1200/3316]  eta: 17:29:18  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.6490  data: 0.0048
Test:  [1190/3316]  eta: 17:43:57  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.5635  data: 0.0051
Test:  [1220/3316]  eta: 17:03:37  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.1850  data: 0.0046
Test:  [1190/3316]  eta: 17:46:07  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.6077  data: 0.0047
Test:  [1190/3316]  eta: 17:46:10  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.5744  data: 0.0047
Test:  [1220/3316]  eta: 17:05:25  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 28.8748  data: 0.0041
Test:  [1190/3316]  eta: 17:46:27  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.5222  data: 0.0048
Test:  [1200/3316]  eta: 17:37:03  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.4546  data: 0.0035
Test:  [1210/3316]  eta: 17:24:14  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.2012  data: 0.0030
Test:  [1200/3316]  eta: 17:38:51  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.3393  data: 0.0030
Test:  [1230/3316]  eta: 16:58:40  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 28.8144  data: 0.0027
Test:  [1230/3316]  eta: 17:00:25  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 28.9132  data: 0.0027
Test:  [1200/3316]  eta: 17:41:00  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.6280  data: 0.0029
Test:  [1200/3316]  eta: 17:41:03  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.6458  data: 0.0028
Test:  [1200/3316]  eta: 17:41:18  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.5420  data: 0.0031
Test:  [1210/3316]  eta: 17:31:55  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.6128  data: 0.0033
Test:  [1220/3316]  eta: 17:19:10  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.3595  data: 0.0030
Test:  [1210/3316]  eta: 17:33:41  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.5650  data: 0.0033
Test:  [1240/3316]  eta: 16:53:41  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 28.9944  data: 0.0029
Test:  [1240/3316]  eta: 16:55:23  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 28.8945  data: 0.0033
Test:  [1210/3316]  eta: 17:35:51  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.6972  data: 0.0035
Test:  [1210/3316]  eta: 17:35:53  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.6611  data: 0.0037
Test:  [1210/3316]  eta: 17:36:10  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.6599  data: 0.0034
Test:  [1220/3316]  eta: 17:26:47  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.5095  data: 0.0030
Test:  [1230/3316]  eta: 17:14:06  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.3630  data: 0.0029
Test:  [1250/3316]  eta: 16:48:45  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.0233  data: 0.0033
Test:  [1220/3316]  eta: 17:28:35  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.5605  data: 0.0030
Test:  [1250/3316]  eta: 16:50:25  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 28.9695  data: 0.0036
Test:  [1220/3316]  eta: 17:30:46  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.7128  data: 0.0035
Test:  [1220/3316]  eta: 17:30:47  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.6693  data: 0.0037
Test:  [1220/3316]  eta: 17:31:04  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.7242  data: 0.0032
Test:  [1230/3316]  eta: 17:21:36  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.3647  data: 0.0027
Test:  [1240/3316]  eta: 17:08:57  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.2244  data: 0.0027
Test:  [1260/3316]  eta: 16:43:48  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.0713  data: 0.0029
Test:  [1230/3316]  eta: 17:23:28  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.6580  data: 0.0027
Test:  [1260/3316]  eta: 16:45:25  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.0154  data: 0.0036
Test:  [1230/3316]  eta: 17:25:39  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.7079  data: 0.0038
Test:  [1230/3316]  eta: 17:25:39  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.7684  data: 0.0040
Test:  [1230/3316]  eta: 17:25:57  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.7711  data: 0.0034
Test:  [1240/3316]  eta: 17:16:37  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.6448  data: 0.0040
Test:  [1250/3316]  eta: 17:04:03  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.4786  data: 0.0040
Test:  [1270/3316]  eta: 16:38:51  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.0505  data: 0.0044
Test:  [1240/3316]  eta: 17:18:22  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.6517  data: 0.0041
Test:  [1270/3316]  eta: 16:40:27  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.0002  data: 0.0046
Test:  [1240/3316]  eta: 17:20:36  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.7955  data: 0.0046
Test:  [1240/3316]  eta: 17:20:37  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.8451  data: 0.0049
Test:  [1240/3316]  eta: 17:20:53  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.8091  data: 0.0043
Test:  [1250/3316]  eta: 17:11:28  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.7170  data: 0.0040
Test:  [1260/3316]  eta: 16:58:55  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.5178  data: 0.0040
Test:  [1280/3316]  eta: 16:33:49  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 28.8908  data: 0.0047
Test:  [1250/3316]  eta: 17:13:09  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.4559  data: 0.0042
Test:  [1280/3316]  eta: 16:35:21  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 28.8093  data: 0.0041
Test:  [1250/3316]  eta: 17:15:23  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.6358  data: 0.0039
Test:  [1250/3316]  eta: 17:15:23  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.6430  data: 0.0036
Test:  [1250/3316]  eta: 17:15:38  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.5473  data: 0.0037
Test:  [1260/3316]  eta: 17:06:22  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.4706  data: 0.0026
Test:  [1270/3316]  eta: 16:53:49  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.1679  data: 0.0027
Test:  [1290/3316]  eta: 16:28:51  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 28.8440  data: 0.0028
Test:  [1260/3316]  eta: 17:08:01  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.3628  data: 0.0029
Test:  [1290/3316]  eta: 16:30:24  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 28.7992  data: 0.0027
Test:  [1260/3316]  eta: 17:10:25  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.7802  data: 0.0028
Test:  [1260/3316]  eta: 17:10:26  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.8170  data: 0.0026
Test:  [1260/3316]  eta: 17:10:39  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.7339  data: 0.0028
Test:  [1270/3316]  eta: 17:01:23  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.7726  data: 0.0037
Test:  [1280/3316]  eta: 16:48:49  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.3654  data: 0.0040
Test:  [1300/3316]  eta: 16:23:57  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.0907  data: 0.0039
Test:  [1270/3316]  eta: 17:02:59  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.6824  data: 0.0043
Test:  [1300/3316]  eta: 16:25:26  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.0410  data: 0.0045
Test:  [1270/3316]  eta: 17:05:13  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.8100  data: 0.0041
Test:  [1270/3316]  eta: 17:05:14  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.8475  data: 0.0040
Test:  [1270/3316]  eta: 17:05:27  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.7802  data: 0.0042
Test:  [1280/3316]  eta: 16:56:15  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.7104  data: 0.0041
Test:  [1290/3316]  eta: 16:43:45  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.4082  data: 0.0039
Test:  [1310/3316]  eta: 16:18:57  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.0014  data: 0.0041
Test:  [1280/3316]  eta: 16:57:53  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.7703  data: 0.0042
Test:  [1310/3316]  eta: 16:20:27  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.0013  data: 0.0045
Test:  [1280/3316]  eta: 17:00:09  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.6110  data: 0.0042
Test:  [1280/3316]  eta: 17:00:10  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.5983  data: 0.0042
Test:  [1280/3316]  eta: 17:00:22  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.5740  data: 0.0043
Test:  [1290/3316]  eta: 16:51:10  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.4930  data: 0.0030
Test:  [1300/3316]  eta: 16:38:41  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.3079  data: 0.0029
Test:  [1320/3316]  eta: 16:13:58  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 28.8271  data: 0.0030
Test:  [1290/3316]  eta: 16:52:45  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.5678  data: 0.0028
Test:  [1320/3316]  eta: 16:15:26  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 28.8884  data: 0.0025
Test:  [1290/3316]  eta: 16:54:57  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.5852  data: 0.0031
Test:  [1290/3316]  eta: 16:54:59  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.6025  data: 0.0029
Test:  [1290/3316]  eta: 16:55:11  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.6068  data: 0.0029
Test:  [1300/3316]  eta: 16:46:10  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.7526  data: 0.0033
Test:  [1310/3316]  eta: 16:33:44  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.5185  data: 0.0034
Test:  [1330/3316]  eta: 16:09:04  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 29.0220  data: 0.0036
Test:  [1300/3316]  eta: 16:47:47  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.7958  data: 0.0038
Test:  [1330/3316]  eta: 16:10:37  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 29.2281  data: 0.0039
Test:  [1300/3316]  eta: 16:50:01  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.8480  data: 0.0046
Test:  [1300/3316]  eta: 16:50:06  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.9815  data: 0.0043
Test:  [1300/3316]  eta: 16:50:16  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.9366  data: 0.0047
Test:  [1310/3316]  eta: 16:41:07  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.8139  data: 0.0045
Test:  [1340/3316]  eta: 16:04:08  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 29.1433  data: 0.0046
Test:  [1320/3316]  eta: 16:28:45  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 29.6544  data: 0.0044
Test:  [1310/3316]  eta: 16:42:44  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.9286  data: 0.0047
Test:  [1340/3316]  eta: 16:05:39  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 29.3092  data: 0.0044
Test:  [1310/3316]  eta: 16:44:53  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.9677  data: 0.0048
Test:  [1310/3316]  eta: 16:44:59  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 30.1180  data: 0.0045
Test:  [1310/3316]  eta: 16:45:08  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 30.0114  data: 0.0051
Test:  [1350/3316]  eta: 15:59:11  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 29.0268  data: 0.0039
Test:  [1320/3316]  eta: 16:36:02  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 29.6223  data: 0.0041
Test:  [1330/3316]  eta: 16:23:46  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 29.6229  data: 0.0039
Test:  [1350/3316]  eta: 16:00:43  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 29.0617  data: 0.0032
Test:  [1320/3316]  eta: 16:37:42  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 29.8296  data: 0.0036
Test:  [1320/3316]  eta: 16:40:03  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 30.1522  data: 0.0033
Test:  [1320/3316]  eta: 16:40:13  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 30.3463  data: 0.0031
Test:  [1320/3316]  eta: 16:40:18  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 30.1734  data: 0.0034
Test:  [1360/3316]  eta: 15:54:34  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 29.6628  data: 0.0032
Test:  [1330/3316]  eta: 16:31:16  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 30.2118  data: 0.0032
Test:  [1340/3316]  eta: 16:19:08  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.3166  data: 0.0029
Test:  [1360/3316]  eta: 15:56:02  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 29.6437  data: 0.0032
Test:  [1330/3316]  eta: 16:32:55  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 30.3796  data: 0.0032
Test:  [1330/3316]  eta: 16:35:08  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 30.6094  data: 0.0034
Test:  [1330/3316]  eta: 16:35:19  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 30.7524  data: 0.0033
Test:  [1330/3316]  eta: 16:35:20  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 30.5381  data: 0.0035
Test:  [1370/3316]  eta: 15:49:48  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.0603  data: 0.0038
Test:  [1340/3316]  eta: 16:26:19  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.4870  data: 0.0043
Test:  [1350/3316]  eta: 16:14:18  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.6208  data: 0.0035
Test:  [1370/3316]  eta: 15:51:13  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 29.8775  data: 0.0042
Test:  [1340/3316]  eta: 16:27:59  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.5487  data: 0.0045
Test:  [1340/3316]  eta: 16:30:21  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.7037  data: 0.0050
Test:  [1340/3316]  eta: 16:30:31  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.5389  data: 0.0051
Test:  [1340/3316]  eta: 16:30:31  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.7070  data: 0.0050
Test:  [1380/3316]  eta: 15:45:02  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 29.7655  data: 0.0050
Test:  [1350/3316]  eta: 16:21:23  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.1579  data: 0.0054
Test:  [1360/3316]  eta: 16:09:27  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 30.2194  data: 0.0048
Test:  [1380/3316]  eta: 15:46:25  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 29.6846  data: 0.0051
Test:  [1350/3316]  eta: 16:23:06  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.3630  data: 0.0050
Test:  [1350/3316]  eta: 16:25:24  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.6597  data: 0.0051
Test:  [1350/3316]  eta: 16:25:34  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.5943  data: 0.0053
Test:  [1350/3316]  eta: 16:25:37  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.7435  data: 0.0053
Test:  [1390/3316]  eta: 15:40:08  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 29.5119  data: 0.0047
Test:  [1360/3316]  eta: 16:16:20  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 29.9442  data: 0.0044
Test:  [1370/3316]  eta: 16:04:30  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 29.9886  data: 0.0044
Test:  [1390/3316]  eta: 15:41:30  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 29.4640  data: 0.0043
Test:  [1360/3316]  eta: 16:18:05  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 30.2123  data: 0.0039
Test:  [1360/3316]  eta: 16:20:24  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 30.2225  data: 0.0034
Test:  [1360/3316]  eta: 16:20:35  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 30.2890  data: 0.0036
Test:  [1360/3316]  eta: 16:20:41  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 30.4768  data: 0.0034
Test:  [1400/3316]  eta: 15:35:25  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 29.6155  data: 0.0033
Test:  [1370/3316]  eta: 16:11:29  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.1222  data: 0.0032
Test:  [1380/3316]  eta: 15:59:47  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.2643  data: 0.0032
Test:  [1400/3316]  eta: 15:36:47  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 29.6434  data: 0.0032
Test:  [1370/3316]  eta: 16:13:18  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.4341  data: 0.0033
Test:  [1370/3316]  eta: 16:15:36  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.5315  data: 0.0032
Test:  [1370/3316]  eta: 16:15:45  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.5155  data: 0.0033
Test:  [1370/3316]  eta: 16:15:51  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.6397  data: 0.0032
Test:  [1410/3316]  eta: 15:30:42  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 30.0233  data: 0.0032
Test:  [1380/3316]  eta: 16:06:39  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.6004  data: 0.0041
Test:  [1390/3316]  eta: 15:55:02  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.6794  data: 0.0042
Test:  [1410/3316]  eta: 15:32:01  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 29.9831  data: 0.0038
Test:  [1380/3316]  eta: 16:08:22  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.6231  data: 0.0051
Test:  [1380/3316]  eta: 16:10:45  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.8832  data: 0.0047
Test:  [1380/3316]  eta: 16:10:54  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.7923  data: 0.0048
Test:  [1380/3316]  eta: 16:10:59  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.7918  data: 0.0046
Test:  [1420/3316]  eta: 15:25:52  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 29.7882  data: 0.0051
Test:  [1390/3316]  eta: 16:01:41  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.3493  data: 0.0055
Test:  [1400/3316]  eta: 15:50:08  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.3130  data: 0.0051
Test:  [1420/3316]  eta: 15:27:13  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 29.7862  data: 0.0048
Test:  [1390/3316]  eta: 16:03:27  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.3532  data: 0.0056
Test:  [1390/3316]  eta: 16:05:51  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.6936  data: 0.0050
Test:  [1390/3316]  eta: 16:05:59  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.6307  data: 0.0050
Test:  [1390/3316]  eta: 16:06:02  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.5556  data: 0.0049
Test:  [1430/3316]  eta: 15:21:03  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 29.5375  data: 0.0052
Test:  [1400/3316]  eta: 15:56:42  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.0367  data: 0.0048
Test:  [1410/3316]  eta: 15:45:13  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 29.9658  data: 0.0042
Test:  [1430/3316]  eta: 15:22:20  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 29.5523  data: 0.0042
Test:  [1400/3316]  eta: 15:58:28  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.2281  data: 0.0038
Test:  [1400/3316]  eta: 16:00:47  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.2466  data: 0.0035
Test:  [1400/3316]  eta: 16:00:56  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.2607  data: 0.0034
Test:  [1400/3316]  eta: 16:00:57  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.1001  data: 0.0035
Test:  [1440/3316]  eta: 15:16:10  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.4403  data: 0.0031
Test:  [1410/3316]  eta: 15:51:40  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 29.9077  data: 0.0032
Test:  [1420/3316]  eta: 15:40:15  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 29.8415  data: 0.0030
Test:  [1440/3316]  eta: 15:17:26  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.3695  data: 0.0031
Test:  [1410/3316]  eta: 15:53:31  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 30.1508  data: 0.0029
Test:  [1410/3316]  eta: 15:55:52  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 30.1993  data: 0.0031
Test:  [1410/3316]  eta: 15:55:58  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 29.9981  data: 0.0031
Test:  [1410/3316]  eta: 15:55:58  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 30.1598  data: 0.0031
Test:  [1450/3316]  eta: 15:11:18  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.3680  data: 0.0030
Test:  [1420/3316]  eta: 15:46:51  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 30.2736  data: 0.0037
Test:  [1450/3316]  eta: 15:12:42  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.6999  data: 0.0039
Test:  [1430/3316]  eta: 15:35:25  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.0591  data: 0.0037
Test:  [1420/3316]  eta: 15:48:36  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 30.3216  data: 0.0035
Test:  [1420/3316]  eta: 15:50:56  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 30.4558  data: 0.0048
Test:  [1420/3316]  eta: 15:51:01  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 30.3026  data: 0.0048
Test:  [1420/3316]  eta: 15:51:02  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 30.3550  data: 0.0047
Test:  [1460/3316]  eta: 15:06:29  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 29.5110  data: 0.0049
Test:  [1460/3316]  eta: 15:07:45  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 29.5364  data: 0.0049
Test:  [1430/3316]  eta: 15:41:45  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.1194  data: 0.0050
Test:  [1440/3316]  eta: 15:30:22  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.8468  data: 0.0051
Test:  [1430/3316]  eta: 15:43:34  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.1465  data: 0.0042
Test:  [1430/3316]  eta: 15:45:55  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.2558  data: 0.0049
Test:  [1430/3316]  eta: 15:46:00  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.2469  data: 0.0048
Test:  [1430/3316]  eta: 15:46:02  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.3078  data: 0.0049
Test:  [1470/3316]  eta: 15:01:38  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 29.5114  data: 0.0049
Test:  [1470/3316]  eta: 15:02:52  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 29.2202  data: 0.0041
Test:  [1440/3316]  eta: 15:36:46  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.7311  data: 0.0043
Test:  [1450/3316]  eta: 15:25:24  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.5358  data: 0.0044
Test:  [1440/3316]  eta: 15:38:32  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.8675  data: 0.0038
Test:  [1440/3316]  eta: 15:40:46  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.7996  data: 0.0031
Test:  [1440/3316]  eta: 15:40:50  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.7484  data: 0.0032
Test:  [1440/3316]  eta: 15:40:53  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.8357  data: 0.0031
Test:  [1480/3316]  eta: 14:56:36  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 29.0078  data: 0.0030
Test:  [1480/3316]  eta: 14:57:51  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 29.0700  data: 0.0029
Test:  [1450/3316]  eta: 15:31:36  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.5671  data: 0.0028
Test:  [1460/3316]  eta: 15:20:18  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 29.3931  data: 0.0029
Test:  [1450/3316]  eta: 15:33:27  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.7487  data: 0.0030
Test:  [1450/3316]  eta: 15:35:45  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.7907  data: 0.0032
Test:  [1450/3316]  eta: 15:35:47  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.6570  data: 0.0029
Test:  [1450/3316]  eta: 15:35:52  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.7551  data: 0.0033
Test:  [1490/3316]  eta: 14:51:41  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 28.8791  data: 0.0031
Test:  [1490/3316]  eta: 14:53:00  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 29.1454  data: 0.0030
Test:  [1470/3316]  eta: 15:15:26  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 29.6367  data: 0.0027
Test:  [1460/3316]  eta: 15:26:42  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 29.7802  data: 0.0027
Test:  [1460/3316]  eta: 15:28:32  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 30.0364  data: 0.0032
Test:  [1460/3316]  eta: 15:30:43  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 30.0516  data: 0.0041
Test:  [1460/3316]  eta: 15:30:44  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 29.9078  data: 0.0034
Test:  [1460/3316]  eta: 15:30:50  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 30.0515  data: 0.0040
Test:  [1500/3316]  eta: 14:46:47  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 29.1790  data: 0.0042
Test:  [1500/3316]  eta: 14:48:06  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 29.4167  data: 0.0039
Test:  [1480/3316]  eta: 15:10:22  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 29.7032  data: 0.0038
Test:  [1470/3316]  eta: 15:21:35  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 29.8753  data: 0.0033
Test:  [1470/3316]  eta: 15:23:28  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 30.0725  data: 0.0034
Test:  [1470/3316]  eta: 15:25:42  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 29.9622  data: 0.0036
Test:  [1470/3316]  eta: 15:25:43  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 30.0729  data: 0.0040
Test:  [1470/3316]  eta: 15:25:50  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 30.1131  data: 0.0038
Test:  [1510/3316]  eta: 14:41:55  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 29.2841  data: 0.0042
Test:  [1510/3316]  eta: 14:43:17  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 29.5006  data: 0.0037
Test:  [1490/3316]  eta: 15:05:29  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 29.6480  data: 0.0039
Test:  [1480/3316]  eta: 15:16:37  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 29.7306  data: 0.0034
Test:  [1480/3316]  eta: 15:18:30  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 29.9675  data: 0.0031
Test:  [1480/3316]  eta: 15:20:40  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 30.0001  data: 0.0037
Test:  [1480/3316]  eta: 15:20:43  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 30.1645  data: 0.0039
Test:  [1520/3316]  eta: 14:37:01  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 29.2946  data: 0.0037
Test:  [1480/3316]  eta: 15:20:49  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 30.1241  data: 0.0041
Test:  [1520/3316]  eta: 14:38:24  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 29.5674  data: 0.0031
Test:  [1500/3316]  eta: 15:00:32  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 29.9474  data: 0.0037
Test:  [1490/3316]  eta: 15:11:37  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 29.9869  data: 0.0037
Test:  [1490/3316]  eta: 15:13:32  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 30.2067  data: 0.0035
Test:  [1530/3316]  eta: 14:32:08  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 29.2644  data: 0.0039
Test:  [1490/3316]  eta: 15:15:39  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 30.0361  data: 0.0035
Test:  [1490/3316]  eta: 15:15:45  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 30.2439  data: 0.0036
Test:  [1490/3316]  eta: 15:15:50  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 30.1547  data: 0.0038
Test:  [1530/3316]  eta: 14:33:34  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 29.5296  data: 0.0033
Test:  [1510/3316]  eta: 14:55:39  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 29.9656  data: 0.0036
Test:  [1500/3316]  eta: 15:06:40  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 30.0481  data: 0.0039
Test:  [1500/3316]  eta: 15:08:36  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 30.2809  data: 0.0034
Test:  [1540/3316]  eta: 14:27:15  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 29.2947  data: 0.0035
Test:  [1500/3316]  eta: 15:10:39  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 30.1112  data: 0.0030
Test:  [1500/3316]  eta: 15:10:45  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 30.2083  data: 0.0029
Test:  [1500/3316]  eta: 15:10:49  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 30.1737  data: 0.0030
Test:  [1540/3316]  eta: 14:28:43  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 29.5777  data: 0.0030
Test:  [1520/3316]  eta: 14:50:41  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 29.9534  data: 0.0037
Test:  [1510/3316]  eta: 15:01:41  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 30.0964  data: 0.0035
Test:  [1510/3316]  eta: 15:03:37  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 30.2412  data: 0.0037
Test:  [1550/3316]  eta: 14:22:24  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 29.4096  data: 0.0044
Test:  [1510/3316]  eta: 15:05:40  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 30.2158  data: 0.0047
Test:  [1510/3316]  eta: 15:05:47  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 30.2247  data: 0.0045
Test:  [1510/3316]  eta: 15:05:52  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 30.2579  data: 0.0047
Test:  [1550/3316]  eta: 14:23:51  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 29.5114  data: 0.0042
Test:  [1530/3316]  eta: 14:45:47  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 29.8968  data: 0.0049
Test:  [1520/3316]  eta: 14:56:45  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 30.1162  data: 0.0045
Test:  [1520/3316]  eta: 14:58:47  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 30.5100  data: 0.0043
Test:  [1560/3316]  eta: 14:17:40  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.7900  data: 0.0047
Test:  [1520/3316]  eta: 15:00:47  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 30.5398  data: 0.0050
Test:  [1520/3316]  eta: 15:00:55  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 30.6196  data: 0.0049
Test:  [1520/3316]  eta: 15:01:00  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 30.6529  data: 0.0051
Test:  [1560/3316]  eta: 14:19:05  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.7648  data: 0.0046
Test:  [1540/3316]  eta: 14:40:54  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.1107  data: 0.0046
Test:  [1530/3316]  eta: 14:51:49  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 30.2544  data: 0.0043
Test:  [1530/3316]  eta: 14:53:49  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 30.5584  data: 0.0036
Test:  [1570/3316]  eta: 14:12:47  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 29.7036  data: 0.0035
Test:  [1530/3316]  eta: 14:55:47  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 30.4527  data: 0.0032
Test:  [1530/3316]  eta: 14:55:55  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 30.5338  data: 0.0034
Test:  [1530/3316]  eta: 14:55:59  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 30.4988  data: 0.0033
Test:  [1570/3316]  eta: 14:14:11  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 29.6649  data: 0.0035
Test:  [1550/3316]  eta: 14:35:57  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 29.9817  data: 0.0034
Test:  [1540/3316]  eta: 14:46:49  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.0760  data: 0.0030
Test:  [1540/3316]  eta: 14:48:54  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.3539  data: 0.0035
Test:  [1580/3316]  eta: 14:08:00  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 29.5895  data: 0.0043
Test:  [1540/3316]  eta: 14:50:52  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.3550  data: 0.0043
Test:  [1540/3316]  eta: 14:51:00  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.3957  data: 0.0046
Test:  [1540/3316]  eta: 14:51:04  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.3453  data: 0.0045
Test:  [1580/3316]  eta: 14:09:24  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 29.6080  data: 0.0044
Test:  [1560/3316]  eta: 14:31:03  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.9373  data: 0.0051
Test:  [1550/3316]  eta: 14:41:50  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 29.9741  data: 0.0042
Test:  [1550/3316]  eta: 14:43:46  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 29.8931  data: 0.0038
Test:  [1590/3316]  eta: 14:03:00  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 29.2474  data: 0.0043
Test:  [1550/3316]  eta: 14:45:43  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 30.0029  data: 0.0044
Test:  [1550/3316]  eta: 14:45:51  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 30.0145  data: 0.0045
Test:  [1550/3316]  eta: 14:45:56  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 30.0177  data: 0.0045
Test:  [1590/3316]  eta: 14:04:23  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 29.3202  data: 0.0042
Test:  [1570/3316]  eta: 14:25:59  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 29.6327  data: 0.0048
Test:  [1560/3316]  eta: 14:36:42  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.6230  data: 0.0040
Test:  [1560/3316]  eta: 14:38:41  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.4811  data: 0.0031
Test:  [1600/3316]  eta: 13:58:08  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 29.0380  data: 0.0031
Test:  [1560/3316]  eta: 14:40:44  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.8526  data: 0.0029
Test:  [1560/3316]  eta: 14:40:51  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.7677  data: 0.0028
Test:  [1560/3316]  eta: 14:40:57  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.8644  data: 0.0028
Test:  [1600/3316]  eta: 13:59:34  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 29.2252  data: 0.0030
Test:  [1580/3316]  eta: 14:21:03  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 29.5624  data: 0.0032
Test:  [1570/3316]  eta: 14:31:44  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 29.6509  data: 0.0029
Test:  [1570/3316]  eta: 14:33:40  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 29.7976  data: 0.0027
Test:  [1610/3316]  eta: 13:53:16  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 29.3984  data: 0.0032
Test:  [1570/3316]  eta: 14:35:46  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 30.3140  data: 0.0038
Test:  [1570/3316]  eta: 14:35:50  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 30.1344  data: 0.0041
Test:  [1570/3316]  eta: 14:35:57  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 30.2364  data: 0.0042
Test:  [1610/3316]  eta: 13:54:42  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 29.6379  data: 0.0039
Test:  [1590/3316]  eta: 14:16:07  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 29.9275  data: 0.0046
Test:  [1580/3316]  eta: 14:26:46  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 30.0948  data: 0.0046
Test:  [1580/3316]  eta: 14:28:40  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 29.9650  data: 0.0046
Test:  [1620/3316]  eta: 13:48:24  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 29.4155  data: 0.0041
Test:  [1580/3316]  eta: 14:30:50  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 30.4562  data: 0.0050
Test:  [1580/3316]  eta: 14:30:54  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 30.3613  data: 0.0051
Test:  [1620/3316]  eta: 13:49:55  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 29.7776  data: 0.0049
Test:  [1580/3316]  eta: 14:31:02  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 30.4360  data: 0.0050
Test:  [1600/3316]  eta: 14:11:16  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 30.1055  data: 0.0048
Test:  [1590/3316]  eta: 14:21:51  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 30.2312  data: 0.0051
Test:  [1590/3316]  eta: 14:23:43  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 30.1653  data: 0.0048
Test:  [1630/3316]  eta: 13:43:31  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 29.3778  data: 0.0042
Test:  [1590/3316]  eta: 14:25:45  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 30.1363  data: 0.0042
Test:  [1630/3316]  eta: 13:45:00  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 29.5765  data: 0.0040
Test:  [1590/3316]  eta: 14:25:50  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 30.1895  data: 0.0039
Test:  [1590/3316]  eta: 14:25:57  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 30.1996  data: 0.0036
Test:  [1610/3316]  eta: 14:06:15  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 29.8808  data: 0.0036
Test:  [1600/3316]  eta: 14:16:46  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 29.9484  data: 0.0035
Test:  [1600/3316]  eta: 14:18:42  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 30.1660  data: 0.0031
Test:  [1640/3316]  eta: 13:38:38  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.3147  data: 0.0032
Test:  [1640/3316]  eta: 13:40:13  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.5817  data: 0.0035
Test:  [1600/3316]  eta: 14:20:50  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 30.1890  data: 0.0030
Test:  [1600/3316]  eta: 14:20:54  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 30.1781  data: 0.0030
Test:  [1600/3316]  eta: 14:21:02  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 30.2065  data: 0.0030
Test:  [1620/3316]  eta: 14:01:25  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 29.9962  data: 0.0031
Test:  [1610/3316]  eta: 14:11:53  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 30.0535  data: 0.0034
Test:  [1610/3316]  eta: 14:13:48  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 30.2923  data: 0.0033
Test:  [1650/3316]  eta: 13:33:49  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.5231  data: 0.0033
Test:  [1650/3316]  eta: 13:35:16  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.5263  data: 0.0038
Test:  [1610/3316]  eta: 14:15:46  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 30.2395  data: 0.0029
Test:  [1610/3316]  eta: 14:15:51  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 30.2415  data: 0.0032
Test:  [1610/3316]  eta: 14:15:57  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 30.2200  data: 0.0030
Test:  [1630/3316]  eta: 13:56:24  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 29.9807  data: 0.0032
Test:  [1620/3316]  eta: 14:06:49  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 30.0319  data: 0.0034
Test:  [1620/3316]  eta: 14:08:51  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 30.4712  data: 0.0039
Test:  [1660/3316]  eta: 13:29:00  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 29.7161  data: 0.0034
Test:  [1660/3316]  eta: 13:30:29  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 29.5544  data: 0.0042
Test:  [1620/3316]  eta: 14:10:51  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 30.2297  data: 0.0043
Test:  [1620/3316]  eta: 14:10:58  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 30.3875  data: 0.0046
Test:  [1620/3316]  eta: 14:11:04  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 30.2781  data: 0.0043
Test:  [1640/3316]  eta: 13:51:34  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.9619  data: 0.0047
Test:  [1630/3316]  eta: 14:01:56  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 30.0597  data: 0.0045
Test:  [1670/3316]  eta: 13:24:07  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 29.4955  data: 0.0041
Test:  [1630/3316]  eta: 14:03:53  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 30.3100  data: 0.0051
Test:  [1670/3316]  eta: 13:25:32  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 29.5481  data: 0.0045
Test:  [1630/3316]  eta: 14:05:46  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 30.1739  data: 0.0051
Test:  [1630/3316]  eta: 14:05:52  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 30.2655  data: 0.0049
Test:  [1630/3316]  eta: 14:05:58  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 30.2581  data: 0.0049
Test:  [1650/3316]  eta: 13:46:29  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.8126  data: 0.0048
Test:  [1640/3316]  eta: 13:56:50  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.9543  data: 0.0044
Test:  [1680/3316]  eta: 13:19:07  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 28.9408  data: 0.0038
Test:  [1640/3316]  eta: 13:58:46  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.8022  data: 0.0045
Test:  [1680/3316]  eta: 13:20:33  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 28.9023  data: 0.0034
Test:  [1640/3316]  eta: 14:00:40  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.6656  data: 0.0036
Test:  [1640/3316]  eta: 14:00:49  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.7615  data: 0.0034
Test:  [1640/3316]  eta: 14:00:54  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.7664  data: 0.0035
Test:  [1660/3316]  eta: 13:41:30  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 29.3614  data: 0.0029
Test:  [1650/3316]  eta: 13:51:49  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.5925  data: 0.0027
Test:  [1690/3316]  eta: 13:14:14  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 28.9580  data: 0.0028
Test:  [1650/3316]  eta: 13:53:46  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.7277  data: 0.0028
Test:  [1690/3316]  eta: 13:15:38  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 28.9802  data: 0.0029
Test:  [1650/3316]  eta: 13:55:35  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.6647  data: 0.0031
Test:  [1650/3316]  eta: 13:55:45  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.8305  data: 0.0029
Test:  [1650/3316]  eta: 13:55:50  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.8033  data: 0.0029
Test:  [1670/3316]  eta: 13:36:28  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 29.4599  data: 0.0027
Test:  [1660/3316]  eta: 13:46:44  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 29.6490  data: 0.0027
Test:  [1700/3316]  eta: 13:09:21  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 29.3204  data: 0.0027
Test:  [1660/3316]  eta: 13:48:49  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 30.2280  data: 0.0035
Test:  [1700/3316]  eta: 13:10:46  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 29.3883  data: 0.0038
Test:  [1660/3316]  eta: 13:50:38  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 30.1030  data: 0.0042
Test:  [1660/3316]  eta: 13:50:51  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 30.2851  data: 0.0049
Test:  [1660/3316]  eta: 13:50:55  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 30.2185  data: 0.0047
Test:  [1680/3316]  eta: 13:31:39  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 29.9896  data: 0.0050
Test:  [1670/3316]  eta: 13:41:55  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 30.1733  data: 0.0044
Test:  [1710/3316]  eta: 13:04:33  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 29.5903  data: 0.0048
Test:  [1670/3316]  eta: 13:43:53  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 30.4070  data: 0.0051
Test:  [1710/3316]  eta: 13:05:55  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 29.6034  data: 0.0050
Test:  [1670/3316]  eta: 13:45:39  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 30.3711  data: 0.0051
Test:  [1670/3316]  eta: 13:45:51  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 30.5292  data: 0.0059
Test:  [1670/3316]  eta: 13:45:55  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 30.4428  data: 0.0054
Test:  [1690/3316]  eta: 13:26:41  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 30.1726  data: 0.0054
Test:  [1680/3316]  eta: 13:36:52  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 30.3124  data: 0.0049
Test:  [1720/3316]  eta: 12:59:37  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 29.4286  data: 0.0049
Test:  [1680/3316]  eta: 13:38:48  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 30.0183  data: 0.0045
Test:  [1720/3316]  eta: 13:00:58  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 29.2831  data: 0.0042
Test:  [1680/3316]  eta: 13:40:36  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 30.0794  data: 0.0040
Test:  [1680/3316]  eta: 13:40:46  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 30.0013  data: 0.0040
Test:  [1680/3316]  eta: 13:40:51  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 30.0276  data: 0.0037
Test:  [1700/3316]  eta: 13:21:42  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 29.6739  data: 0.0034
Test:  [1690/3316]  eta: 13:31:52  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 29.7823  data: 0.0034
Test:  [1730/3316]  eta: 12:54:47  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.3048  data: 0.0029
Test:  [1690/3316]  eta: 13:33:52  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 30.0232  data: 0.0031
Test:  [1730/3316]  eta: 12:56:08  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.3851  data: 0.0030
Test:  [1690/3316]  eta: 13:35:41  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 30.3437  data: 0.0032
Test:  [1690/3316]  eta: 13:35:51  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 30.2004  data: 0.0031
Test:  [1690/3316]  eta: 13:35:54  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 30.1908  data: 0.0034
Test:  [1710/3316]  eta: 13:16:49  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 29.9472  data: 0.0032
Test:  [1700/3316]  eta: 13:26:54  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 30.0291  data: 0.0032
Test:  [1740/3316]  eta: 12:49:55  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.5356  data: 0.0031
Test:  [1700/3316]  eta: 13:28:56  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 30.4693  data: 0.0032
Test:  [1740/3316]  eta: 12:51:17  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.7509  data: 0.0035
Test:  [1700/3316]  eta: 13:30:46  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 30.7029  data: 0.0046
Test:  [1700/3316]  eta: 13:30:53  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 30.5560  data: 0.0040
Test:  [1700/3316]  eta: 13:30:59  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 30.6208  data: 0.0047
Test:  [1720/3316]  eta: 13:11:55  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.2323  data: 0.0042
Test:  [1710/3316]  eta: 13:22:00  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 30.3589  data: 0.0046
Test:  [1750/3316]  eta: 12:45:16  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.1896  data: 0.0049
Test:  [1710/3316]  eta: 13:24:12  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 31.0881  data: 0.0057
Test:  [1750/3316]  eta: 12:46:38  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.3171  data: 0.0052
Test:  [1710/3316]  eta: 13:26:00  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 31.1971  data: 0.0055
Test:  [1710/3316]  eta: 13:26:07  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 31.0556  data: 0.0053
Test:  [1710/3316]  eta: 13:26:13  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 31.2357  data: 0.0057
Test:  [1730/3316]  eta: 13:07:12  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 30.7908  data: 0.0048
Test:  [1720/3316]  eta: 13:17:13  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.9307  data: 0.0053
Test:  [1760/3316]  eta: 12:40:27  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 30.3588  data: 0.0050
Test:  [1720/3316]  eta: 13:19:08  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.7034  data: 0.0055
Test:  [1760/3316]  eta: 12:41:39  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 29.8950  data: 0.0047
Test:  [1720/3316]  eta: 13:20:52  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.5523  data: 0.0041
Test:  [1720/3316]  eta: 13:20:58  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.5104  data: 0.0043
Test:  [1720/3316]  eta: 13:21:05  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.5535  data: 0.0043
Test:  [1740/3316]  eta: 13:02:08  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 30.2226  data: 0.0036
Test:  [1730/3316]  eta: 13:12:06  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 30.2289  data: 0.0037
Test:  [1770/3316]  eta: 12:35:30  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.3368  data: 0.0031
Test:  [1730/3316]  eta: 13:14:06  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.7924  data: 0.0029
Test:  [1770/3316]  eta: 12:36:46  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.1160  data: 0.0029
Test:  [1730/3316]  eta: 13:15:53  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.8713  data: 0.0034
Test:  [1730/3316]  eta: 13:15:59  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.8154  data: 0.0031
Test:  [1730/3316]  eta: 13:16:06  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.8611  data: 0.0029
Test:  [1750/3316]  eta: 12:57:14  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 29.6550  data: 0.0029
Test:  [1740/3316]  eta: 13:07:08  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.6703  data: 0.0034
Test:  [1780/3316]  eta: 12:30:39  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 29.2550  data: 0.0033
Test:  [1740/3316]  eta: 13:09:03  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.8122  data: 0.0030
Test:  [1780/3316]  eta: 12:31:50  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 29.2638  data: 0.0032
Test:  [1740/3316]  eta: 13:10:46  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.9053  data: 0.0033
Test:  [1740/3316]  eta: 13:10:51  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.8315  data: 0.0030
Test:  [1740/3316]  eta: 13:10:59  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.9493  data: 0.0029
Test:  [1760/3316]  eta: 12:52:11  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 29.6859  data: 0.0029
Test:  [1750/3316]  eta: 13:02:02  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 29.7052  data: 0.0033
Test:  [1790/3316]  eta: 12:25:42  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 29.2142  data: 0.0033
Test:  [1750/3316]  eta: 13:04:05  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.0115  data: 0.0039
Test:  [1790/3316]  eta: 12:27:00  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 29.4302  data: 0.0042
Test:  [1750/3316]  eta: 13:05:51  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.1048  data: 0.0048
Test:  [1750/3316]  eta: 13:05:56  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.0862  data: 0.0047
Test:  [1750/3316]  eta: 13:06:04  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.1407  data: 0.0046
Test:  [1770/3316]  eta: 12:47:19  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.8078  data: 0.0047
Test:  [1760/3316]  eta: 12:57:07  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 29.8446  data: 0.0044
Test:  [1800/3316]  eta: 12:20:56  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 29.5188  data: 0.0049
Test:  [1800/3316]  eta: 12:22:05  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 29.4778  data: 0.0051
Test:  [1760/3316]  eta: 12:59:01  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 29.9729  data: 0.0051
Test:  [1760/3316]  eta: 13:00:44  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 30.1124  data: 0.0053
Test:  [1760/3316]  eta: 13:00:49  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 30.1085  data: 0.0050
Test:  [1780/3316]  eta: 12:42:15  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 29.7518  data: 0.0049
Test:  [1760/3316]  eta: 13:00:57  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 30.1146  data: 0.0050
Test:  [1770/3316]  eta: 12:52:01  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.8563  data: 0.0048
Test:  [1810/3316]  eta: 12:15:58  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 29.4560  data: 0.0048
Test:  [1810/3316]  eta: 12:17:08  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 29.1143  data: 0.0041
Test:  [1770/3316]  eta: 12:53:57  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.6256  data: 0.0041
Test:  [1770/3316]  eta: 12:55:46  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.9647  data: 0.0035
Test:  [1770/3316]  eta: 12:55:51  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.9227  data: 0.0033
Test:  [1790/3316]  eta: 12:37:21  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 29.6108  data: 0.0031
Test:  [1770/3316]  eta: 12:56:00  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 30.0456  data: 0.0034
Test:  [1780/3316]  eta: 12:47:06  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 29.8809  data: 0.0032
Test:  [1820/3316]  eta: 12:11:08  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 29.2344  data: 0.0031
Test:  [1820/3316]  eta: 12:12:16  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 29.2757  data: 0.0030
Test:  [1780/3316]  eta: 12:48:57  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 29.8863  data: 0.0030
Test:  [1780/3316]  eta: 12:50:43  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 30.1895  data: 0.0030
Test:  [1780/3316]  eta: 12:50:47  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 30.1214  data: 0.0029
Test:  [1800/3316]  eta: 12:32:21  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 29.8862  data: 0.0029
Test:  [1780/3316]  eta: 12:50:58  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 30.3126  data: 0.0035
Test:  [1790/3316]  eta: 12:42:03  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 30.0709  data: 0.0029
Test:  [1830/3316]  eta: 12:06:12  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 29.3776  data: 0.0037
Test:  [1830/3316]  eta: 12:07:19  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 29.2795  data: 0.0031
Test:  [1790/3316]  eta: 12:43:55  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 29.9804  data: 0.0030
Test:  [1790/3316]  eta: 12:45:43  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 30.0609  data: 0.0029
Test:  [1790/3316]  eta: 12:45:47  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 30.0266  data: 0.0028
Test:  [1810/3316]  eta: 12:27:25  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 29.7881  data: 0.0028
Test:  [1790/3316]  eta: 12:46:00  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 30.2097  data: 0.0034
Test:  [1800/3316]  eta: 12:37:06  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 29.9071  data: 0.0029
Test:  [1840/3316]  eta: 12:01:22  loss: 1.0703 (1.1493)  acc1: 75.0000 (75.7299)  acc5: 87.5000 (91.4754)  time: 29.3528  data: 0.0036
Test:  [1840/3316]  eta: 12:02:33  loss: 1.0703 (1.1493)  acc1: 75.0000 (75.7299)  acc5: 87.5000 (91.4754)  time: 29.6397  data: 0.0035
Test:  [1800/3316]  eta: 12:39:01  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 30.3172  data: 0.0037
Test:  [1800/3316]  eta: 12:40:46  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 30.3623  data: 0.0043
Test:  [1820/3316]  eta: 12:22:31  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 30.0654  data: 0.0048
Test:  [1800/3316]  eta: 12:40:49  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 30.3668  data: 0.0043
Test:  [1800/3316]  eta: 12:41:02  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 30.5051  data: 0.0053
Test:  [1810/3316]  eta: 12:32:09  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 30.2583  data: 0.0052
Test:  [1850/3316]  eta: 11:56:32  loss: 1.0927 (1.1495)  acc1: 75.0000 (75.7226)  acc5: 87.5000 (91.4708)  time: 29.7393  data: 0.0052
Test:  [1850/3316]  eta: 11:57:37  loss: 1.0927 (1.1495)  acc1: 75.0000 (75.7226)  acc5: 87.5000 (91.4708)  time: 29.6927  data: 0.0047
Test:  [1810/3316]  eta: 12:33:58  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 30.2733  data: 0.0047
Test:  [1830/3316]  eta: 12:17:37  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 30.2158  data: 0.0052
Test:  [1810/3316]  eta: 12:35:49  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 30.5545  data: 0.0045
Test:  [1810/3316]  eta: 12:35:52  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 30.5444  data: 0.0044
Test:  [1810/3316]  eta: 12:36:05  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 30.5703  data: 0.0052
Test:  [1820/3316]  eta: 12:27:13  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 30.3447  data: 0.0052
Test:  [1860/3316]  eta: 11:51:43  loss: 1.2040 (1.1499)  acc1: 75.0000 (75.7019)  acc5: 93.7500 (91.4764)  time: 29.8228  data: 0.0049
