/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
Not using distributed mode
[08:45:48.392853] job dir: /home/ada/satmae/SatMAE
[08:45:48.392943] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=8,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='temporal',
device='cuda',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='/home/ada/satmae/temporal/evaluation/image_sequence_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type='temporal',
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/image_sequence_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/finetune_fmow_temporal.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[08:45:49.711129] <util.datasets.CustomDatasetFromImagesTemporal object at 0x7fe6b99f5590>
[08:45:49.899687] <util.datasets.CustomDatasetFromImagesTemporal object at 0x7fe72401c590>
[08:45:49.899794] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fe6acc8dc50>
[08:45:59.898385] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
[08:45:59.898468] number of params (M): 304.25
[08:45:59.898489] base lr: 1.00e-03
[08:45:59.898503] actual lr: 3.13e-05
[08:45:59.898516] accumulate grad iterations: 1
[08:45:59.898528] effective batch size: 8
[08:45:59.902613] criterion = LabelSmoothingCrossEntropy()
[08:46:01.742221] Resume checkpoint /home/ada/satmae/temporal/checkpoints/finetune_fmow_temporal.pth
[08:46:02.950772] Test:  [   0/6631]  eta: 2:13:13  loss: 0.9248 (0.9248)  acc1: 87.5000 (87.5000)  acc5: 87.5000 (87.5000)  time: 1.2055  data: 0.7016  max mem: 2094
[08:46:06.961928] Test:  [  10/6631]  eta: 0:52:19  loss: 0.9440 (1.2707)  acc1: 75.0000 (72.7273)  acc5: 87.5000 (87.5000)  time: 0.4742  data: 0.0639  max mem: 2094
[08:46:11.029335] Test:  [  20/6631]  eta: 0:48:42  loss: 1.4483 (1.4342)  acc1: 75.0000 (70.2381)  acc5: 87.5000 (86.9048)  time: 0.4038  data: 0.0002  max mem: 2094
[08:46:15.145386] Test:  [  30/6631]  eta: 0:47:32  loss: 1.4483 (1.3888)  acc1: 62.5000 (72.1774)  acc5: 87.5000 (88.3065)  time: 0.4091  data: 0.0002  max mem: 2094
[08:46:19.309115] Test:  [  40/6631]  eta: 0:47:02  loss: 1.2553 (1.3838)  acc1: 75.0000 (71.9512)  acc5: 87.5000 (88.7195)  time: 0.4139  data: 0.0002  max mem: 2094
[08:46:23.507469] Test:  [  50/6631]  eta: 0:46:47  loss: 1.3335 (1.4169)  acc1: 75.0000 (71.8137)  acc5: 87.5000 (87.5000)  time: 0.4180  data: 0.0002  max mem: 2094
[08:46:27.739322] Test:  [  60/6631]  eta: 0:46:39  loss: 1.2311 (1.3946)  acc1: 75.0000 (71.5164)  acc5: 87.5000 (88.1148)  time: 0.4214  data: 0.0002  max mem: 2094
[08:46:31.913790] Test:  [  70/6631]  eta: 0:46:27  loss: 1.3017 (1.3857)  acc1: 75.0000 (71.4789)  acc5: 87.5000 (88.0282)  time: 0.4202  data: 0.0002  max mem: 2094
[08:46:36.110399] Test:  [  80/6631]  eta: 0:46:18  loss: 1.4091 (1.4017)  acc1: 75.0000 (70.8333)  acc5: 87.5000 (88.1173)  time: 0.4185  data: 0.0002  max mem: 2094
[08:46:40.325731] Test:  [  90/6631]  eta: 0:46:12  loss: 1.4121 (1.4186)  acc1: 75.0000 (71.1538)  acc5: 87.5000 (87.3626)  time: 0.4205  data: 0.0002  max mem: 2094
[08:46:44.571882] Test:  [ 100/6631]  eta: 0:46:08  loss: 1.4121 (1.4225)  acc1: 75.0000 (71.0396)  acc5: 87.5000 (87.2525)  time: 0.4230  data: 0.0002  max mem: 2094
[08:46:48.838467] Test:  [ 110/6631]  eta: 0:46:06  loss: 1.4687 (1.4290)  acc1: 75.0000 (70.8333)  acc5: 87.5000 (87.0495)  time: 0.4255  data: 0.0002  max mem: 2094
[08:46:53.137823] Test:  [ 120/6631]  eta: 0:46:04  loss: 1.1666 (1.4218)  acc1: 75.0000 (71.0744)  acc5: 87.5000 (87.1901)  time: 0.4282  data: 0.0002  max mem: 2094
[08:46:57.472978] Test:  [ 130/6631]  eta: 0:46:04  loss: 1.1666 (1.4155)  acc1: 75.0000 (70.8969)  acc5: 87.5000 (87.3092)  time: 0.4316  data: 0.0002  max mem: 2094
[08:47:01.860672] Test:  [ 140/6631]  eta: 0:46:06  loss: 1.2756 (1.4243)  acc1: 75.0000 (70.9220)  acc5: 87.5000 (87.1454)  time: 0.4361  data: 0.0002  max mem: 2094
[08:47:06.280962] Test:  [ 150/6631]  eta: 0:46:09  loss: 1.2125 (1.4244)  acc1: 75.0000 (70.7781)  acc5: 87.5000 (87.4172)  time: 0.4403  data: 0.0002  max mem: 2094
[08:47:10.726572] Test:  [ 160/6631]  eta: 0:46:11  loss: 1.2227 (1.4237)  acc1: 75.0000 (70.9627)  acc5: 87.5000 (87.2671)  time: 0.4432  data: 0.0002  max mem: 2094
[08:47:15.187818] Test:  [ 170/6631]  eta: 0:46:14  loss: 1.2227 (1.4160)  acc1: 75.0000 (71.0526)  acc5: 87.5000 (87.3538)  time: 0.4452  data: 0.0002  max mem: 2094
[08:47:19.645750] Test:  [ 180/6631]  eta: 0:46:15  loss: 1.2765 (1.4151)  acc1: 62.5000 (71.0635)  acc5: 87.5000 (87.4309)  time: 0.4459  data: 0.0002  max mem: 2094
[08:47:24.102419] Test:  [ 190/6631]  eta: 0:46:16  loss: 1.2765 (1.4030)  acc1: 75.0000 (71.2696)  acc5: 87.5000 (87.6309)  time: 0.4456  data: 0.0002  max mem: 2094
[08:47:28.551634] Test:  [ 200/6631]  eta: 0:46:16  loss: 1.0718 (1.3912)  acc1: 75.0000 (71.6418)  acc5: 87.5000 (87.8109)  time: 0.4452  data: 0.0002  max mem: 2094
[08:47:32.985014] Test:  [ 210/6631]  eta: 0:46:15  loss: 1.3319 (1.3919)  acc1: 75.0000 (71.7417)  acc5: 87.5000 (87.6185)  time: 0.4440  data: 0.0002  max mem: 2094
[08:47:37.410696] Test:  [ 220/6631]  eta: 0:46:14  loss: 1.5002 (1.3929)  acc1: 75.0000 (71.8326)  acc5: 87.5000 (87.5566)  time: 0.4429  data: 0.0002  max mem: 2094
[08:47:41.824725] Test:  [ 230/6631]  eta: 0:46:12  loss: 1.3797 (1.3990)  acc1: 75.0000 (71.6991)  acc5: 87.5000 (87.2835)  time: 0.4419  data: 0.0002  max mem: 2094
[08:47:46.210207] Test:  [ 240/6631]  eta: 0:46:09  loss: 1.2202 (1.3879)  acc1: 75.0000 (72.0954)  acc5: 87.5000 (87.2407)  time: 0.4399  data: 0.0002  max mem: 2094
[08:47:50.582330] Test:  [ 250/6631]  eta: 0:46:06  loss: 1.3831 (1.3893)  acc1: 75.0000 (72.1116)  acc5: 87.5000 (87.2012)  time: 0.4378  data: 0.0002  max mem: 2094
[08:47:54.933639] Test:  [ 260/6631]  eta: 0:46:02  loss: 1.4640 (1.3983)  acc1: 75.0000 (71.8870)  acc5: 87.5000 (87.0690)  time: 0.4361  data: 0.0002  max mem: 2094
[08:47:59.283406] Test:  [ 270/6631]  eta: 0:45:58  loss: 1.5071 (1.3990)  acc1: 62.5000 (71.9096)  acc5: 87.5000 (87.0387)  time: 0.4350  data: 0.0002  max mem: 2094
[08:48:03.630701] Test:  [ 280/6631]  eta: 0:45:54  loss: 1.4369 (1.4024)  acc1: 75.0000 (71.9306)  acc5: 87.5000 (87.0552)  time: 0.4348  data: 0.0002  max mem: 2094
[08:48:07.978027] Test:  [ 290/6631]  eta: 0:45:50  loss: 1.2452 (1.3970)  acc1: 75.0000 (71.9931)  acc5: 87.5000 (87.1134)  time: 0.4346  data: 0.0002  max mem: 2094
[08:48:12.329032] Test:  [ 300/6631]  eta: 0:45:46  loss: 1.3080 (1.4036)  acc1: 75.0000 (71.8854)  acc5: 87.5000 (87.0017)  time: 0.4348  data: 0.0002  max mem: 2094
[08:48:16.685805] Test:  [ 310/6631]  eta: 0:45:42  loss: 1.3975 (1.4009)  acc1: 75.0000 (72.0257)  acc5: 87.5000 (87.0981)  time: 0.4353  data: 0.0002  max mem: 2094
[08:48:21.065415] Test:  [ 320/6631]  eta: 0:45:38  loss: 1.2917 (1.4054)  acc1: 75.0000 (71.8847)  acc5: 87.5000 (87.0327)  time: 0.4367  data: 0.0002  max mem: 2094
[08:48:25.466089] Test:  [ 330/6631]  eta: 0:45:35  loss: 1.5758 (1.4194)  acc1: 62.5000 (71.7145)  acc5: 87.5000 (86.8202)  time: 0.4389  data: 0.0002  max mem: 2094
[08:48:29.873004] Test:  [ 340/6631]  eta: 0:45:32  loss: 1.5758 (1.4180)  acc1: 62.5000 (71.7742)  acc5: 87.5000 (86.8402)  time: 0.4403  data: 0.0002  max mem: 2094
[08:48:34.302654] Test:  [ 350/6631]  eta: 0:45:29  loss: 1.0782 (1.4074)  acc1: 75.0000 (72.0085)  acc5: 87.5000 (87.0370)  time: 0.4417  data: 0.0002  max mem: 2094
[08:48:38.744184] Test:  [ 360/6631]  eta: 0:45:26  loss: 1.1247 (1.4013)  acc1: 75.0000 (72.1260)  acc5: 87.5000 (87.0845)  time: 0.4435  data: 0.0002  max mem: 2094
[08:48:43.188800] Test:  [ 370/6631]  eta: 0:45:23  loss: 1.2586 (1.4029)  acc1: 75.0000 (72.1024)  acc5: 87.5000 (87.0283)  time: 0.4442  data: 0.0002  max mem: 2094
[08:48:47.627341] Test:  [ 380/6631]  eta: 0:45:21  loss: 1.5387 (1.4098)  acc1: 75.0000 (72.0472)  acc5: 87.5000 (86.8438)  time: 0.4441  data: 0.0002  max mem: 2094
[08:48:52.063070] Test:  [ 390/6631]  eta: 0:45:17  loss: 1.3944 (1.4002)  acc1: 75.0000 (72.3146)  acc5: 87.5000 (86.9885)  time: 0.4436  data: 0.0002  max mem: 2094
[08:48:56.494829] Test:  [ 400/6631]  eta: 0:45:14  loss: 1.1500 (1.4020)  acc1: 75.0000 (72.2569)  acc5: 87.5000 (86.9701)  time: 0.4433  data: 0.0002  max mem: 2094
[08:49:00.914801] Test:  [ 410/6631]  eta: 0:45:11  loss: 1.1982 (1.4024)  acc1: 62.5000 (72.2019)  acc5: 87.5000 (87.0134)  time: 0.4425  data: 0.0002  max mem: 2094
[08:49:05.326469] Test:  [ 420/6631]  eta: 0:45:07  loss: 1.1638 (1.4011)  acc1: 62.5000 (72.1496)  acc5: 87.5000 (87.0546)  time: 0.4415  data: 0.0002  max mem: 2094
[08:49:09.731214] Test:  [ 430/6631]  eta: 0:45:04  loss: 1.1875 (1.4089)  acc1: 62.5000 (71.9838)  acc5: 87.5000 (87.0070)  time: 0.4407  data: 0.0002  max mem: 2094
[08:49:14.142240] Test:  [ 440/6631]  eta: 0:45:00  loss: 1.6154 (1.4158)  acc1: 62.5000 (71.8537)  acc5: 87.5000 (86.8197)  time: 0.4407  data: 0.0002  max mem: 2094
[08:49:18.547131] Test:  [ 450/6631]  eta: 0:44:56  loss: 1.3923 (1.4085)  acc1: 75.0000 (72.0621)  acc5: 87.5000 (86.9734)  time: 0.4407  data: 0.0002  max mem: 2094
[08:49:22.954664] Test:  [ 460/6631]  eta: 0:44:52  loss: 1.0340 (1.4012)  acc1: 75.0000 (72.2072)  acc5: 100.0000 (87.0662)  time: 0.4405  data: 0.0002  max mem: 2094
[08:49:27.375610] Test:  [ 470/6631]  eta: 0:44:49  loss: 1.2056 (1.4059)  acc1: 75.0000 (72.2134)  acc5: 87.5000 (86.9427)  time: 0.4413  data: 0.0002  max mem: 2094
[08:49:31.802538] Test:  [ 480/6631]  eta: 0:44:45  loss: 1.0900 (1.3989)  acc1: 75.0000 (72.3753)  acc5: 87.5000 (87.0322)  time: 0.4423  data: 0.0002  max mem: 2094
[08:49:36.244410] Test:  [ 490/6631]  eta: 0:44:42  loss: 1.1653 (1.3993)  acc1: 75.0000 (72.3269)  acc5: 87.5000 (87.0418)  time: 0.4433  data: 0.0002  max mem: 2094
[08:49:40.681651] Test:  [ 500/6631]  eta: 0:44:38  loss: 1.3044 (1.4005)  acc1: 75.0000 (72.2804)  acc5: 87.5000 (87.0010)  time: 0.4439  data: 0.0002  max mem: 2094
[08:49:45.117110] Test:  [ 510/6631]  eta: 0:44:35  loss: 1.1036 (1.3925)  acc1: 75.0000 (72.5294)  acc5: 87.5000 (87.0597)  time: 0.4435  data: 0.0002  max mem: 2094
[08:49:49.546873] Test:  [ 520/6631]  eta: 0:44:31  loss: 1.0655 (1.3892)  acc1: 87.5000 (72.6248)  acc5: 87.5000 (87.1161)  time: 0.4432  data: 0.0002  max mem: 2094
[08:49:53.971041] Test:  [ 530/6631]  eta: 0:44:27  loss: 1.4722 (1.3910)  acc1: 75.0000 (72.6224)  acc5: 87.5000 (87.0527)  time: 0.4426  data: 0.0002  max mem: 2094
[08:49:58.379846] Test:  [ 540/6631]  eta: 0:44:23  loss: 1.0883 (1.3884)  acc1: 75.0000 (72.7126)  acc5: 87.5000 (87.0841)  time: 0.4416  data: 0.0002  max mem: 2094
[08:50:02.784054] Test:  [ 550/6631]  eta: 0:44:19  loss: 1.4467 (1.3940)  acc1: 75.0000 (72.4819)  acc5: 87.5000 (87.0463)  time: 0.4406  data: 0.0002  max mem: 2094
[08:50:07.183994] Test:  [ 560/6631]  eta: 0:44:15  loss: 1.5855 (1.3934)  acc1: 62.5000 (72.4822)  acc5: 87.5000 (87.0544)  time: 0.4401  data: 0.0002  max mem: 2094
[08:50:11.590581] Test:  [ 570/6631]  eta: 0:44:11  loss: 1.0727 (1.3914)  acc1: 75.0000 (72.4825)  acc5: 87.5000 (87.1497)  time: 0.4402  data: 0.0002  max mem: 2094
[08:50:15.996301] Test:  [ 580/6631]  eta: 0:44:07  loss: 1.0296 (1.3938)  acc1: 75.0000 (72.3967)  acc5: 87.5000 (87.1558)  time: 0.4405  data: 0.0002  max mem: 2094
[08:50:20.392611] Test:  [ 590/6631]  eta: 0:44:03  loss: 1.1777 (1.3932)  acc1: 75.0000 (72.3985)  acc5: 87.5000 (87.1827)  time: 0.4400  data: 0.0002  max mem: 2094
[08:50:24.785804] Test:  [ 600/6631]  eta: 0:43:59  loss: 1.1777 (1.3931)  acc1: 75.0000 (72.3378)  acc5: 87.5000 (87.2296)  time: 0.4394  data: 0.0002  max mem: 2094
[08:50:29.174044] Test:  [ 610/6631]  eta: 0:43:54  loss: 1.6283 (1.3974)  acc1: 62.5000 (72.1768)  acc5: 87.5000 (87.2136)  time: 0.4390  data: 0.0002  max mem: 2094
[08:50:33.538796] Test:  [ 620/6631]  eta: 0:43:50  loss: 1.5817 (1.4000)  acc1: 62.5000 (72.0612)  acc5: 87.5000 (87.2585)  time: 0.4376  data: 0.0002  max mem: 2094
[08:50:37.899137] Test:  [ 630/6631]  eta: 0:43:45  loss: 1.5817 (1.4037)  acc1: 62.5000 (71.8700)  acc5: 87.5000 (87.1632)  time: 0.4362  data: 0.0002  max mem: 2094
[08:50:42.240737] Test:  [ 640/6631]  eta: 0:43:41  loss: 1.5019 (1.4041)  acc1: 62.5000 (71.8799)  acc5: 87.5000 (87.1685)  time: 0.4350  data: 0.0002  max mem: 2094
[08:50:46.591204] Test:  [ 650/6631]  eta: 0:43:36  loss: 1.3071 (1.4014)  acc1: 75.0000 (71.9662)  acc5: 87.5000 (87.2120)  time: 0.4345  data: 0.0002  max mem: 2094
[08:50:50.939275] Test:  [ 660/6631]  eta: 0:43:31  loss: 1.2546 (1.4010)  acc1: 75.0000 (71.9554)  acc5: 87.5000 (87.2352)  time: 0.4348  data: 0.0002  max mem: 2094
[08:50:55.300200] Test:  [ 670/6631]  eta: 0:43:27  loss: 1.2221 (1.3993)  acc1: 75.0000 (72.0007)  acc5: 87.5000 (87.3137)  time: 0.4354  data: 0.0002  max mem: 2094
[08:50:59.678217] Test:  [ 680/6631]  eta: 0:43:22  loss: 1.1516 (1.4021)  acc1: 75.0000 (71.9714)  acc5: 87.5000 (87.2614)  time: 0.4369  data: 0.0002  max mem: 2094
[08:51:04.081497] Test:  [ 690/6631]  eta: 0:43:18  loss: 1.1168 (1.3986)  acc1: 75.0000 (72.0695)  acc5: 87.5000 (87.3010)  time: 0.4390  data: 0.0002  max mem: 2094
[08:51:08.490997] Test:  [ 700/6631]  eta: 0:43:14  loss: 1.0224 (1.3975)  acc1: 75.0000 (72.0399)  acc5: 87.5000 (87.3217)  time: 0.4405  data: 0.0002  max mem: 2094
[08:51:12.920075] Test:  [ 710/6631]  eta: 0:43:10  loss: 1.0932 (1.3950)  acc1: 75.0000 (72.1167)  acc5: 87.5000 (87.3769)  time: 0.4418  data: 0.0002  max mem: 2094
[08:51:17.347097] Test:  [ 720/6631]  eta: 0:43:06  loss: 1.1499 (1.3939)  acc1: 75.0000 (72.1047)  acc5: 87.5000 (87.4133)  time: 0.4427  data: 0.0002  max mem: 2094
[08:51:21.794659] Test:  [ 730/6631]  eta: 0:43:03  loss: 1.2581 (1.3919)  acc1: 75.0000 (72.1272)  acc5: 87.5000 (87.4487)  time: 0.4436  data: 0.0002  max mem: 2094
[08:51:26.239886] Test:  [ 740/6631]  eta: 0:42:59  loss: 1.0357 (1.3863)  acc1: 75.0000 (72.2672)  acc5: 87.5000 (87.5000)  time: 0.4445  data: 0.0002  max mem: 2094
[08:51:30.675580] Test:  [ 750/6631]  eta: 0:42:55  loss: 1.0891 (1.3839)  acc1: 75.0000 (72.3036)  acc5: 87.5000 (87.5166)  time: 0.4440  data: 0.0002  max mem: 2094
[08:51:35.120076] Test:  [ 760/6631]  eta: 0:42:51  loss: 1.2178 (1.3834)  acc1: 75.0000 (72.2405)  acc5: 87.5000 (87.5657)  time: 0.4439  data: 0.0002  max mem: 2094
[08:51:39.553314] Test:  [ 770/6631]  eta: 0:42:47  loss: 1.2289 (1.3828)  acc1: 75.0000 (72.2763)  acc5: 87.5000 (87.5649)  time: 0.4438  data: 0.0002  max mem: 2094
[08:51:43.978718] Test:  [ 780/6631]  eta: 0:42:43  loss: 1.3412 (1.3831)  acc1: 75.0000 (72.2311)  acc5: 87.5000 (87.5480)  time: 0.4428  data: 0.0002  max mem: 2094
[08:51:48.394900] Test:  [ 790/6631]  eta: 0:42:39  loss: 1.4297 (1.3852)  acc1: 62.5000 (72.1713)  acc5: 87.5000 (87.5474)  time: 0.4420  data: 0.0002  max mem: 2094
[08:51:52.795483] Test:  [ 800/6631]  eta: 0:42:34  loss: 1.1585 (1.3826)  acc1: 62.5000 (72.2378)  acc5: 87.5000 (87.5780)  time: 0.4407  data: 0.0002  max mem: 2094
[08:51:57.192368] Test:  [ 810/6631]  eta: 0:42:30  loss: 1.1585 (1.3873)  acc1: 62.5000 (72.1178)  acc5: 87.5000 (87.5308)  time: 0.4398  data: 0.0002  max mem: 2094
[08:52:01.574009] Test:  [ 820/6631]  eta: 0:42:26  loss: 1.2321 (1.3878)  acc1: 62.5000 (72.1224)  acc5: 87.5000 (87.4695)  time: 0.4388  data: 0.0002  max mem: 2094
[08:52:05.946321] Test:  [ 830/6631]  eta: 0:42:21  loss: 1.3107 (1.3888)  acc1: 75.0000 (72.0668)  acc5: 87.5000 (87.4850)  time: 0.4376  data: 0.0002  max mem: 2094
[08:52:10.307905] Test:  [ 840/6631]  eta: 0:42:17  loss: 1.3862 (1.3894)  acc1: 75.0000 (72.0571)  acc5: 87.5000 (87.4851)  time: 0.4366  data: 0.0002  max mem: 2094
[08:52:14.674797] Test:  [ 850/6631]  eta: 0:42:12  loss: 1.3642 (1.3901)  acc1: 75.0000 (72.0329)  acc5: 87.5000 (87.4706)  time: 0.4363  data: 0.0002  max mem: 2094
[08:52:19.042961] Test:  [ 860/6631]  eta: 0:42:08  loss: 1.1694 (1.3862)  acc1: 75.0000 (72.1109)  acc5: 87.5000 (87.5290)  time: 0.4367  data: 0.0002  max mem: 2094
[08:52:23.423510] Test:  [ 870/6631]  eta: 0:42:03  loss: 1.2180 (1.3872)  acc1: 75.0000 (72.1010)  acc5: 87.5000 (87.5144)  time: 0.4373  data: 0.0002  max mem: 2094
[08:52:27.808366] Test:  [ 880/6631]  eta: 0:41:59  loss: 1.4417 (1.3902)  acc1: 75.0000 (72.0204)  acc5: 87.5000 (87.4858)  time: 0.4382  data: 0.0002  max mem: 2094
[08:52:32.198770] Test:  [ 890/6631]  eta: 0:41:55  loss: 1.8259 (1.3967)  acc1: 62.5000 (71.8715)  acc5: 75.0000 (87.3737)  time: 0.4387  data: 0.0002  max mem: 2094
[08:52:36.603577] Test:  [ 900/6631]  eta: 0:41:51  loss: 1.4254 (1.3958)  acc1: 62.5000 (71.8785)  acc5: 87.5000 (87.3751)  time: 0.4397  data: 0.0002  max mem: 2094
[08:52:41.002188] Test:  [ 910/6631]  eta: 0:41:46  loss: 1.3231 (1.3970)  acc1: 62.5000 (71.8578)  acc5: 87.5000 (87.3216)  time: 0.4401  data: 0.0002  max mem: 2094
[08:52:45.413927] Test:  [ 920/6631]  eta: 0:41:42  loss: 1.5712 (1.3960)  acc1: 62.5000 (71.8920)  acc5: 87.5000 (87.3100)  time: 0.4404  data: 0.0002  max mem: 2094
[08:52:49.838116] Test:  [ 930/6631]  eta: 0:41:38  loss: 1.4615 (1.3972)  acc1: 62.5000 (71.8582)  acc5: 87.5000 (87.3120)  time: 0.4417  data: 0.0002  max mem: 2094
[08:52:54.282763] Test:  [ 940/6631]  eta: 0:41:34  loss: 1.2573 (1.3935)  acc1: 75.0000 (71.9315)  acc5: 87.5000 (87.3804)  time: 0.4434  data: 0.0002  max mem: 2094
[08:52:58.718583] Test:  [ 950/6631]  eta: 0:41:30  loss: 1.0758 (1.3941)  acc1: 75.0000 (71.9111)  acc5: 87.5000 (87.3817)  time: 0.4439  data: 0.0002  max mem: 2094
[08:53:03.156390] Test:  [ 960/6631]  eta: 0:41:26  loss: 1.3188 (1.3960)  acc1: 75.0000 (71.8652)  acc5: 87.5000 (87.3309)  time: 0.4436  data: 0.0002  max mem: 2094
[08:53:07.581962] Test:  [ 970/6631]  eta: 0:41:22  loss: 1.4443 (1.3989)  acc1: 62.5000 (71.8332)  acc5: 87.5000 (87.2812)  time: 0.4431  data: 0.0002  max mem: 2094
[08:53:12.007363] Test:  [ 980/6631]  eta: 0:41:17  loss: 1.2400 (1.3984)  acc1: 75.0000 (71.8145)  acc5: 87.5000 (87.3216)  time: 0.4425  data: 0.0002  max mem: 2094
[08:53:16.416124] Test:  [ 990/6631]  eta: 0:41:13  loss: 1.2234 (1.3987)  acc1: 75.0000 (71.8088)  acc5: 87.5000 (87.2856)  time: 0.4416  data: 0.0002  max mem: 2094
[08:53:20.819529] Test:  [1000/6631]  eta: 0:41:09  loss: 1.2072 (1.3977)  acc1: 75.0000 (71.8407)  acc5: 87.5000 (87.2752)  time: 0.4405  data: 0.0002  max mem: 2094
[08:53:25.237006] Test:  [1010/6631]  eta: 0:41:05  loss: 1.5205 (1.4017)  acc1: 62.5000 (71.7606)  acc5: 87.5000 (87.2404)  time: 0.4410  data: 0.0002  max mem: 2094
[08:53:29.631850] Test:  [1020/6631]  eta: 0:41:00  loss: 1.6523 (1.4006)  acc1: 62.5000 (71.7801)  acc5: 87.5000 (87.2796)  time: 0.4405  data: 0.0002  max mem: 2094
[08:53:34.025931] Test:  [1030/6631]  eta: 0:40:56  loss: 1.2373 (1.4033)  acc1: 75.0000 (71.6901)  acc5: 87.5000 (87.2696)  time: 0.4394  data: 0.0002  max mem: 2094
[08:53:38.407149] Test:  [1040/6631]  eta: 0:40:52  loss: 1.5282 (1.4045)  acc1: 62.5000 (71.6619)  acc5: 87.5000 (87.2839)  time: 0.4387  data: 0.0002  max mem: 2094
[08:53:42.789792] Test:  [1050/6631]  eta: 0:40:47  loss: 1.3485 (1.4032)  acc1: 75.0000 (71.6817)  acc5: 87.5000 (87.3097)  time: 0.4381  data: 0.0002  max mem: 2094
[08:53:47.180041] Test:  [1060/6631]  eta: 0:40:43  loss: 1.2072 (1.4017)  acc1: 75.0000 (71.7248)  acc5: 87.5000 (87.3233)  time: 0.4386  data: 0.0002  max mem: 2094
[08:53:51.555677] Test:  [1070/6631]  eta: 0:40:38  loss: 1.0797 (1.3999)  acc1: 75.0000 (71.7670)  acc5: 87.5000 (87.3599)  time: 0.4382  data: 0.0002  max mem: 2094
[08:53:55.934790] Test:  [1080/6631]  eta: 0:40:34  loss: 1.4123 (1.4013)  acc1: 75.0000 (71.7623)  acc5: 87.5000 (87.3265)  time: 0.4376  data: 0.0002  max mem: 2094
[08:54:00.313988] Test:  [1090/6631]  eta: 0:40:30  loss: 1.5859 (1.4029)  acc1: 75.0000 (71.7576)  acc5: 87.5000 (87.2823)  time: 0.4378  data: 0.0002  max mem: 2094
[08:54:04.698735] Test:  [1100/6631]  eta: 0:40:25  loss: 1.6382 (1.4056)  acc1: 62.5000 (71.6962)  acc5: 87.5000 (87.2389)  time: 0.4381  data: 0.0002  max mem: 2094
[08:54:09.087831] Test:  [1110/6631]  eta: 0:40:21  loss: 1.3367 (1.4047)  acc1: 75.0000 (71.7034)  acc5: 87.5000 (87.2525)  time: 0.4386  data: 0.0002  max mem: 2094
[08:54:13.475605] Test:  [1120/6631]  eta: 0:40:16  loss: 1.2607 (1.4049)  acc1: 75.0000 (71.6771)  acc5: 87.5000 (87.2658)  time: 0.4388  data: 0.0002  max mem: 2094
[08:54:17.864948] Test:  [1130/6631]  eta: 0:40:12  loss: 1.2225 (1.4040)  acc1: 75.0000 (71.7286)  acc5: 87.5000 (87.2790)  time: 0.4388  data: 0.0002  max mem: 2094
[08:54:22.258619] Test:  [1140/6631]  eta: 0:40:08  loss: 1.1049 (1.4039)  acc1: 75.0000 (71.7134)  acc5: 87.5000 (87.2809)  time: 0.4391  data: 0.0002  max mem: 2094
[08:54:26.653146] Test:  [1150/6631]  eta: 0:40:03  loss: 1.2046 (1.4040)  acc1: 75.0000 (71.7420)  acc5: 87.5000 (87.2828)  time: 0.4393  data: 0.0002  max mem: 2094
[08:54:31.047810] Test:  [1160/6631]  eta: 0:39:59  loss: 1.5528 (1.4074)  acc1: 75.0000 (71.6516)  acc5: 87.5000 (87.2524)  time: 0.4394  data: 0.0002  max mem: 2094
[08:54:35.440026] Test:  [1170/6631]  eta: 0:39:55  loss: 1.3624 (1.4061)  acc1: 62.5000 (71.6802)  acc5: 87.5000 (87.2545)  time: 0.4393  data: 0.0002  max mem: 2094
[08:54:39.834819] Test:  [1180/6631]  eta: 0:39:50  loss: 1.1616 (1.4057)  acc1: 75.0000 (71.6871)  acc5: 87.5000 (87.2566)  time: 0.4393  data: 0.0002  max mem: 2094
[08:54:44.236497] Test:  [1190/6631]  eta: 0:39:46  loss: 1.1046 (1.4028)  acc1: 75.0000 (71.7569)  acc5: 87.5000 (87.3006)  time: 0.4397  data: 0.0002  max mem: 2094
[08:54:48.630715] Test:  [1200/6631]  eta: 0:39:42  loss: 1.1021 (1.4026)  acc1: 75.0000 (71.7319)  acc5: 87.5000 (87.2918)  time: 0.4397  data: 0.0002  max mem: 2094
[08:54:53.022464] Test:  [1210/6631]  eta: 0:39:37  loss: 1.4229 (1.4033)  acc1: 75.0000 (71.7073)  acc5: 87.5000 (87.2936)  time: 0.4392  data: 0.0002  max mem: 2094
[08:54:57.408878] Test:  [1220/6631]  eta: 0:39:33  loss: 1.3232 (1.4013)  acc1: 75.0000 (71.7547)  acc5: 87.5000 (87.3464)  time: 0.4388  data: 0.0002  max mem: 2094
[08:55:01.801720] Test:  [1230/6631]  eta: 0:39:28  loss: 1.1781 (1.4030)  acc1: 75.0000 (71.7405)  acc5: 87.5000 (87.3172)  time: 0.4389  data: 0.0002  max mem: 2094
[08:55:06.191286] Test:  [1240/6631]  eta: 0:39:24  loss: 1.5582 (1.4036)  acc1: 62.5000 (71.7063)  acc5: 87.5000 (87.3086)  time: 0.4390  data: 0.0002  max mem: 2094
[08:55:10.595104] Test:  [1250/6631]  eta: 0:39:20  loss: 1.3539 (1.4031)  acc1: 75.0000 (71.7426)  acc5: 87.5000 (87.3201)  time: 0.4396  data: 0.0002  max mem: 2094
[08:55:14.990965] Test:  [1260/6631]  eta: 0:39:15  loss: 1.2474 (1.4024)  acc1: 75.0000 (71.7784)  acc5: 87.5000 (87.3216)  time: 0.4399  data: 0.0002  max mem: 2094
[08:55:19.387620] Test:  [1270/6631]  eta: 0:39:11  loss: 1.2768 (1.4027)  acc1: 75.0000 (71.7742)  acc5: 87.5000 (87.3033)  time: 0.4395  data: 0.0002  max mem: 2094
[08:55:23.791901] Test:  [1280/6631]  eta: 0:39:07  loss: 1.0631 (1.4009)  acc1: 75.0000 (71.8286)  acc5: 87.5000 (87.3341)  time: 0.4400  data: 0.0002  max mem: 2094
[08:55:28.189987] Test:  [1290/6631]  eta: 0:39:02  loss: 1.2854 (1.4018)  acc1: 75.0000 (71.8145)  acc5: 87.5000 (87.3354)  time: 0.4400  data: 0.0002  max mem: 2094
[08:55:32.594390] Test:  [1300/6631]  eta: 0:38:58  loss: 1.2673 (1.4011)  acc1: 75.0000 (71.8486)  acc5: 87.5000 (87.3463)  time: 0.4400  data: 0.0002  max mem: 2094
[08:55:37.003629] Test:  [1310/6631]  eta: 0:38:54  loss: 1.2673 (1.4030)  acc1: 75.0000 (71.7773)  acc5: 87.5000 (87.3093)  time: 0.4406  data: 0.0002  max mem: 2094
[08:55:41.412962] Test:  [1320/6631]  eta: 0:38:50  loss: 1.5527 (1.4047)  acc1: 62.5000 (71.7449)  acc5: 87.5000 (87.2918)  time: 0.4408  data: 0.0002  max mem: 2094
[08:55:45.815904] Test:  [1330/6631]  eta: 0:38:45  loss: 1.2439 (1.4039)  acc1: 62.5000 (71.7693)  acc5: 87.5000 (87.2934)  time: 0.4405  data: 0.0002  max mem: 2094
[08:55:50.224620] Test:  [1340/6631]  eta: 0:38:41  loss: 1.3224 (1.4043)  acc1: 75.0000 (71.7468)  acc5: 87.5000 (87.3136)  time: 0.4405  data: 0.0002  max mem: 2094
[08:55:54.641617] Test:  [1350/6631]  eta: 0:38:37  loss: 1.3245 (1.4034)  acc1: 75.0000 (71.7987)  acc5: 87.5000 (87.3242)  time: 0.4412  data: 0.0002  max mem: 2094
[08:55:59.062906] Test:  [1360/6631]  eta: 0:38:32  loss: 1.8483 (1.4076)  acc1: 62.5000 (71.7120)  acc5: 87.5000 (87.2704)  time: 0.4418  data: 0.0002  max mem: 2094
[08:56:03.496095] Test:  [1370/6631]  eta: 0:38:28  loss: 1.8483 (1.4079)  acc1: 62.5000 (71.7177)  acc5: 87.5000 (87.2356)  time: 0.4426  data: 0.0002  max mem: 2094
[08:56:07.936216] Test:  [1380/6631]  eta: 0:38:24  loss: 1.3291 (1.4072)  acc1: 75.0000 (71.7415)  acc5: 87.5000 (87.2375)  time: 0.4436  data: 0.0002  max mem: 2094
[08:56:12.364983] Test:  [1390/6631]  eta: 0:38:20  loss: 1.3032 (1.4079)  acc1: 75.0000 (71.7290)  acc5: 87.5000 (87.2484)  time: 0.4434  data: 0.0002  max mem: 2094
[08:56:16.785944] Test:  [1400/6631]  eta: 0:38:15  loss: 1.5146 (1.4102)  acc1: 62.5000 (71.6631)  acc5: 87.5000 (87.2234)  time: 0.4424  data: 0.0002  max mem: 2094
[08:56:21.190352] Test:  [1410/6631]  eta: 0:38:11  loss: 1.5060 (1.4100)  acc1: 62.5000 (71.6513)  acc5: 87.5000 (87.2165)  time: 0.4412  data: 0.0002  max mem: 2094
[08:56:25.604383] Test:  [1420/6631]  eta: 0:38:07  loss: 1.4259 (1.4106)  acc1: 75.0000 (71.6397)  acc5: 87.5000 (87.2009)  time: 0.4408  data: 0.0002  max mem: 2094
[08:56:30.000339] Test:  [1430/6631]  eta: 0:38:02  loss: 1.4438 (1.4117)  acc1: 62.5000 (71.5846)  acc5: 87.5000 (87.2205)  time: 0.4404  data: 0.0002  max mem: 2094
[08:56:34.395935] Test:  [1440/6631]  eta: 0:37:58  loss: 1.4438 (1.4118)  acc1: 62.5000 (71.5822)  acc5: 87.5000 (87.2224)  time: 0.4395  data: 0.0002  max mem: 2094
[08:56:38.792681] Test:  [1450/6631]  eta: 0:37:54  loss: 1.4897 (1.4129)  acc1: 62.5000 (71.5627)  acc5: 87.5000 (87.1985)  time: 0.4395  data: 0.0002  max mem: 2094
[08:56:43.179998] Test:  [1460/6631]  eta: 0:37:49  loss: 1.4681 (1.4140)  acc1: 75.0000 (71.5435)  acc5: 87.5000 (87.1663)  time: 0.4391  data: 0.0002  max mem: 2094
[08:56:47.565819] Test:  [1470/6631]  eta: 0:37:45  loss: 1.3851 (1.4123)  acc1: 75.0000 (71.6010)  acc5: 87.5000 (87.1771)  time: 0.4386  data: 0.0002  max mem: 2094
[08:56:51.920423] Test:  [1480/6631]  eta: 0:37:40  loss: 1.3327 (1.4123)  acc1: 75.0000 (71.5817)  acc5: 87.5000 (87.1793)  time: 0.4369  data: 0.0002  max mem: 2094
[08:56:56.257984] Test:  [1490/6631]  eta: 0:37:36  loss: 1.4598 (1.4127)  acc1: 62.5000 (71.5962)  acc5: 87.5000 (87.1395)  time: 0.4345  data: 0.0002  max mem: 2094
[08:57:00.598286] Test:  [1500/6631]  eta: 0:37:31  loss: 1.4598 (1.4128)  acc1: 62.5000 (71.5856)  acc5: 87.5000 (87.1419)  time: 0.4338  data: 0.0002  max mem: 2094
[08:57:04.937336] Test:  [1510/6631]  eta: 0:37:27  loss: 1.3952 (1.4141)  acc1: 62.5000 (71.5420)  acc5: 87.5000 (87.1360)  time: 0.4339  data: 0.0002  max mem: 2094
[08:57:09.280786] Test:  [1520/6631]  eta: 0:37:22  loss: 1.4026 (1.4143)  acc1: 62.5000 (71.5155)  acc5: 87.5000 (87.1466)  time: 0.4340  data: 0.0002  max mem: 2094
[08:57:13.634488] Test:  [1530/6631]  eta: 0:37:18  loss: 1.4026 (1.4142)  acc1: 62.5000 (71.4974)  acc5: 87.5000 (87.1489)  time: 0.4348  data: 0.0002  max mem: 2094
[08:57:18.000037] Test:  [1540/6631]  eta: 0:37:13  loss: 1.1488 (1.4123)  acc1: 75.0000 (71.5363)  acc5: 87.5000 (87.1918)  time: 0.4359  data: 0.0002  max mem: 2094
[08:57:22.394961] Test:  [1550/6631]  eta: 0:37:09  loss: 0.9672 (1.4110)  acc1: 75.0000 (71.5909)  acc5: 87.5000 (87.1857)  time: 0.4379  data: 0.0002  max mem: 2094
[08:57:26.799568] Test:  [1560/6631]  eta: 0:37:04  loss: 1.4171 (1.4118)  acc1: 75.0000 (71.5407)  acc5: 87.5000 (87.1717)  time: 0.4399  data: 0.0002  max mem: 2094
[08:57:31.221776] Test:  [1570/6631]  eta: 0:37:00  loss: 1.5575 (1.4136)  acc1: 62.5000 (71.5070)  acc5: 87.5000 (87.1181)  time: 0.4412  data: 0.0002  max mem: 2094
[08:57:35.661876] Test:  [1580/6631]  eta: 0:36:56  loss: 1.5885 (1.4133)  acc1: 62.5000 (71.5054)  acc5: 87.5000 (87.1442)  time: 0.4430  data: 0.0002  max mem: 2094
[08:57:40.106730] Test:  [1590/6631]  eta: 0:36:52  loss: 1.5105 (1.4141)  acc1: 75.0000 (71.5195)  acc5: 87.5000 (87.1307)  time: 0.4442  data: 0.0002  max mem: 2094
[08:57:44.555073] Test:  [1600/6631]  eta: 0:36:48  loss: 1.5105 (1.4146)  acc1: 75.0000 (71.5100)  acc5: 87.5000 (87.1252)  time: 0.4446  data: 0.0002  max mem: 2094
[08:57:49.010596] Test:  [1610/6631]  eta: 0:36:43  loss: 1.3462 (1.4140)  acc1: 62.5000 (71.5084)  acc5: 87.5000 (87.1431)  time: 0.4451  data: 0.0002  max mem: 2094
[08:57:53.450297] Test:  [1620/6631]  eta: 0:36:39  loss: 1.1905 (1.4133)  acc1: 75.0000 (71.5299)  acc5: 87.5000 (87.1376)  time: 0.4447  data: 0.0002  max mem: 2094
[08:57:57.890589] Test:  [1630/6631]  eta: 0:36:35  loss: 1.4797 (1.4150)  acc1: 75.0000 (71.4899)  acc5: 87.5000 (87.1168)  time: 0.4439  data: 0.0002  max mem: 2094
[08:58:02.328694] Test:  [1640/6631]  eta: 0:36:31  loss: 1.5402 (1.4159)  acc1: 62.5000 (71.4884)  acc5: 87.5000 (87.1115)  time: 0.4438  data: 0.0002  max mem: 2094
[08:58:06.746216] Test:  [1650/6631]  eta: 0:36:26  loss: 1.3433 (1.4148)  acc1: 75.0000 (71.5097)  acc5: 87.5000 (87.1139)  time: 0.4427  data: 0.0002  max mem: 2094
[08:58:11.148910] Test:  [1660/6631]  eta: 0:36:22  loss: 1.2527 (1.4162)  acc1: 75.0000 (71.4705)  acc5: 87.5000 (87.1162)  time: 0.4409  data: 0.0002  max mem: 2094
[08:58:15.540234] Test:  [1670/6631]  eta: 0:36:18  loss: 1.3534 (1.4174)  acc1: 75.0000 (71.4542)  acc5: 87.5000 (87.0886)  time: 0.4396  data: 0.0002  max mem: 2094
[08:58:19.920455] Test:  [1680/6631]  eta: 0:36:13  loss: 1.4062 (1.4179)  acc1: 75.0000 (71.4381)  acc5: 87.5000 (87.0836)  time: 0.4385  data: 0.0002  max mem: 2094
[08:58:24.290008] Test:  [1690/6631]  eta: 0:36:09  loss: 1.2161 (1.4163)  acc1: 75.0000 (71.4666)  acc5: 87.5000 (87.1008)  time: 0.4374  data: 0.0002  max mem: 2094
[08:58:28.659038] Test:  [1700/6631]  eta: 0:36:04  loss: 1.2575 (1.4179)  acc1: 62.5000 (71.4139)  acc5: 87.5000 (87.1032)  time: 0.4368  data: 0.0002  max mem: 2094
[08:58:33.032123] Test:  [1710/6631]  eta: 0:36:00  loss: 1.6624 (1.4186)  acc1: 62.5000 (71.3837)  acc5: 87.5000 (87.1201)  time: 0.4370  data: 0.0002  max mem: 2094
[08:58:37.400928] Test:  [1720/6631]  eta: 0:35:55  loss: 1.5823 (1.4206)  acc1: 62.5000 (71.3321)  acc5: 87.5000 (87.0933)  time: 0.4370  data: 0.0002  max mem: 2094
[08:58:41.780853] Test:  [1730/6631]  eta: 0:35:51  loss: 1.5363 (1.4214)  acc1: 62.5000 (71.3027)  acc5: 87.5000 (87.0812)  time: 0.4373  data: 0.0002  max mem: 2094
[08:58:46.181354] Test:  [1740/6631]  eta: 0:35:47  loss: 1.0810 (1.4211)  acc1: 75.0000 (71.3096)  acc5: 87.5000 (87.0836)  time: 0.4389  data: 0.0002  max mem: 2094
[08:58:50.578915] Test:  [1750/6631]  eta: 0:35:42  loss: 1.1325 (1.4221)  acc1: 75.0000 (71.2807)  acc5: 87.5000 (87.0717)  time: 0.4398  data: 0.0002  max mem: 2094
[08:58:54.977944] Test:  [1760/6631]  eta: 0:35:38  loss: 1.2447 (1.4215)  acc1: 75.0000 (71.2876)  acc5: 87.5000 (87.0670)  time: 0.4397  data: 0.0002  max mem: 2094
[08:58:59.384239] Test:  [1770/6631]  eta: 0:35:33  loss: 1.2273 (1.4211)  acc1: 75.0000 (71.3156)  acc5: 87.5000 (87.0483)  time: 0.4402  data: 0.0002  max mem: 2094
[08:59:03.795861] Test:  [1780/6631]  eta: 0:35:29  loss: 1.4678 (1.4227)  acc1: 75.0000 (71.2942)  acc5: 75.0000 (87.0087)  time: 0.4408  data: 0.0002  max mem: 2094
[08:59:08.222510] Test:  [1790/6631]  eta: 0:35:25  loss: 1.5452 (1.4249)  acc1: 62.5000 (71.2521)  acc5: 75.0000 (86.9626)  time: 0.4418  data: 0.0002  max mem: 2094
[08:59:12.656674] Test:  [1800/6631]  eta: 0:35:21  loss: 1.4478 (1.4248)  acc1: 62.5000 (71.2451)  acc5: 87.5000 (86.9795)  time: 0.4429  data: 0.0002  max mem: 2094
[08:59:17.085035] Test:  [1810/6631]  eta: 0:35:16  loss: 1.1724 (1.4236)  acc1: 75.0000 (71.2866)  acc5: 87.5000 (86.9961)  time: 0.4430  data: 0.0002  max mem: 2094
[08:59:21.515420] Test:  [1820/6631]  eta: 0:35:12  loss: 1.1848 (1.4241)  acc1: 75.0000 (71.2727)  acc5: 87.5000 (86.9920)  time: 0.4428  data: 0.0002  max mem: 2094
[08:59:25.945678] Test:  [1830/6631]  eta: 0:35:08  loss: 1.3843 (1.4243)  acc1: 62.5000 (71.2520)  acc5: 87.5000 (87.0016)  time: 0.4429  data: 0.0002  max mem: 2094
[08:59:30.382138] Test:  [1840/6631]  eta: 0:35:03  loss: 1.3111 (1.4246)  acc1: 75.0000 (71.2588)  acc5: 87.5000 (87.0043)  time: 0.4432  data: 0.0002  max mem: 2094
[08:59:34.802860] Test:  [1850/6631]  eta: 0:34:59  loss: 1.2691 (1.4247)  acc1: 75.0000 (71.2520)  acc5: 87.5000 (86.9935)  time: 0.4428  data: 0.0002  max mem: 2094
[08:59:39.219703] Test:  [1860/6631]  eta: 0:34:55  loss: 1.4268 (1.4254)  acc1: 62.5000 (71.2319)  acc5: 87.5000 (86.9895)  time: 0.4418  data: 0.0002  max mem: 2094
[08:59:43.649862] Test:  [1870/6631]  eta: 0:34:50  loss: 1.6790 (1.4276)  acc1: 62.5000 (71.1652)  acc5: 87.5000 (86.9789)  time: 0.4423  data: 0.0002  max mem: 2094
[08:59:48.059922] Test:  [1880/6631]  eta: 0:34:46  loss: 1.4516 (1.4271)  acc1: 62.5000 (71.1722)  acc5: 87.5000 (86.9949)  time: 0.4419  data: 0.0002  max mem: 2094
[08:59:52.484653] Test:  [1890/6631]  eta: 0:34:42  loss: 1.3309 (1.4273)  acc1: 75.0000 (71.1528)  acc5: 87.5000 (87.0042)  time: 0.4416  data: 0.0002  max mem: 2094
[08:59:56.901480] Test:  [1900/6631]  eta: 0:34:38  loss: 1.2949 (1.4263)  acc1: 75.0000 (71.1796)  acc5: 87.5000 (87.0134)  time: 0.4420  data: 0.0002  max mem: 2094
[09:00:01.329290] Test:  [1910/6631]  eta: 0:34:33  loss: 1.2958 (1.4252)  acc1: 75.0000 (71.1931)  acc5: 87.5000 (87.0290)  time: 0.4421  data: 0.0002  max mem: 2094
[09:00:05.758925] Test:  [1920/6631]  eta: 0:34:29  loss: 1.2958 (1.4272)  acc1: 75.0000 (71.1804)  acc5: 87.5000 (86.9990)  time: 0.4428  data: 0.0002  max mem: 2094
[09:00:10.187565] Test:  [1930/6631]  eta: 0:34:25  loss: 1.4666 (1.4278)  acc1: 62.5000 (71.1484)  acc5: 87.5000 (87.0080)  time: 0.4428  data: 0.0002  max mem: 2094
[09:00:14.602811] Test:  [1940/6631]  eta: 0:34:20  loss: 1.4527 (1.4292)  acc1: 62.5000 (71.1167)  acc5: 87.5000 (86.9912)  time: 0.4421  data: 0.0002  max mem: 2094
[09:00:19.024796] Test:  [1950/6631]  eta: 0:34:16  loss: 1.4527 (1.4298)  acc1: 62.5000 (71.0917)  acc5: 87.5000 (86.9938)  time: 0.4418  data: 0.0002  max mem: 2094
[09:00:23.441337] Test:  [1960/6631]  eta: 0:34:12  loss: 1.2456 (1.4283)  acc1: 62.5000 (71.1181)  acc5: 87.5000 (87.0219)  time: 0.4418  data: 0.0002  max mem: 2094
[09:00:27.853244] Test:  [1970/6631]  eta: 0:34:07  loss: 0.9978 (1.4264)  acc1: 75.0000 (71.1631)  acc5: 87.5000 (87.0370)  time: 0.4413  data: 0.0002  max mem: 2094
[09:00:32.248114] Test:  [1980/6631]  eta: 0:34:03  loss: 1.0694 (1.4265)  acc1: 75.0000 (71.1762)  acc5: 87.5000 (87.0268)  time: 0.4402  data: 0.0002  max mem: 2094
[09:00:36.638263] Test:  [1990/6631]  eta: 0:33:58  loss: 1.2748 (1.4256)  acc1: 75.0000 (71.1828)  acc5: 87.5000 (87.0542)  time: 0.4392  data: 0.0002  max mem: 2094
[09:00:41.028879] Test:  [2000/6631]  eta: 0:33:54  loss: 1.2853 (1.4249)  acc1: 75.0000 (71.1957)  acc5: 87.5000 (87.0815)  time: 0.4389  data: 0.0002  max mem: 2094
[09:00:45.417283] Test:  [2010/6631]  eta: 0:33:50  loss: 1.1306 (1.4237)  acc1: 75.0000 (71.2332)  acc5: 87.5000 (87.1084)  time: 0.4389  data: 0.0002  max mem: 2094
[09:00:49.813307] Test:  [2020/6631]  eta: 0:33:45  loss: 1.2274 (1.4237)  acc1: 75.0000 (71.2395)  acc5: 87.5000 (87.1042)  time: 0.4391  data: 0.0002  max mem: 2094
[09:00:54.213120] Test:  [2030/6631]  eta: 0:33:41  loss: 1.1876 (1.4219)  acc1: 75.0000 (71.2826)  acc5: 87.5000 (87.1246)  time: 0.4397  data: 0.0002  max mem: 2094
[09:00:58.607040] Test:  [2040/6631]  eta: 0:33:36  loss: 1.1485 (1.4222)  acc1: 75.0000 (71.2702)  acc5: 87.5000 (87.1448)  time: 0.4396  data: 0.0002  max mem: 2094
[09:01:03.002324] Test:  [2050/6631]  eta: 0:33:32  loss: 1.3909 (1.4216)  acc1: 62.5000 (71.2640)  acc5: 87.5000 (87.1465)  time: 0.4394  data: 0.0002  max mem: 2094
[09:01:07.417427] Test:  [2060/6631]  eta: 0:33:28  loss: 1.4955 (1.4229)  acc1: 62.5000 (71.2397)  acc5: 75.0000 (87.1118)  time: 0.4404  data: 0.0002  max mem: 2094
[09:01:11.821181] Test:  [2070/6631]  eta: 0:33:23  loss: 1.1117 (1.4213)  acc1: 75.0000 (71.2820)  acc5: 87.5000 (87.1379)  time: 0.4408  data: 0.0002  max mem: 2094
[09:01:16.225060] Test:  [2080/6631]  eta: 0:33:19  loss: 1.1093 (1.4224)  acc1: 75.0000 (71.2578)  acc5: 87.5000 (87.1216)  time: 0.4403  data: 0.0002  max mem: 2094
[09:01:20.623149] Test:  [2090/6631]  eta: 0:33:15  loss: 1.0350 (1.4212)  acc1: 75.0000 (71.2757)  acc5: 100.0000 (87.1533)  time: 0.4400  data: 0.0002  max mem: 2094
[09:01:25.024689] Test:  [2100/6631]  eta: 0:33:10  loss: 1.0625 (1.4208)  acc1: 75.0000 (71.2815)  acc5: 87.5000 (87.1549)  time: 0.4399  data: 0.0002  max mem: 2094
[09:01:29.420931] Test:  [2110/6631]  eta: 0:33:06  loss: 1.1048 (1.4206)  acc1: 75.0000 (71.2695)  acc5: 87.5000 (87.1566)  time: 0.4398  data: 0.0002  max mem: 2094
[09:01:33.823329] Test:  [2120/6631]  eta: 0:33:01  loss: 1.3772 (1.4220)  acc1: 75.0000 (71.2518)  acc5: 87.5000 (87.1405)  time: 0.4398  data: 0.0002  max mem: 2094
[09:01:38.219839] Test:  [2130/6631]  eta: 0:32:57  loss: 1.5547 (1.4230)  acc1: 62.5000 (71.2107)  acc5: 87.5000 (87.1187)  time: 0.4399  data: 0.0002  max mem: 2094
[09:01:42.607689] Test:  [2140/6631]  eta: 0:32:53  loss: 1.5547 (1.4230)  acc1: 62.5000 (71.2050)  acc5: 87.5000 (87.1322)  time: 0.4391  data: 0.0002  max mem: 2094
[09:01:46.980404] Test:  [2150/6631]  eta: 0:32:48  loss: 1.4993 (1.4231)  acc1: 75.0000 (71.1994)  acc5: 87.5000 (87.1281)  time: 0.4379  data: 0.0002  max mem: 2094
[09:01:51.366576] Test:  [2160/6631]  eta: 0:32:44  loss: 1.4993 (1.4236)  acc1: 75.0000 (71.1997)  acc5: 87.5000 (87.1124)  time: 0.4378  data: 0.0002  max mem: 2094
[09:01:55.746546] Test:  [2170/6631]  eta: 0:32:39  loss: 1.3239 (1.4229)  acc1: 75.0000 (71.2345)  acc5: 87.5000 (87.1257)  time: 0.4382  data: 0.0002  max mem: 2094
[09:02:00.123799] Test:  [2180/6631]  eta: 0:32:35  loss: 1.4137 (1.4229)  acc1: 75.0000 (71.2345)  acc5: 87.5000 (87.1217)  time: 0.4378  data: 0.0002  max mem: 2094
[09:02:04.516213] Test:  [2190/6631]  eta: 0:32:31  loss: 1.2246 (1.4215)  acc1: 75.0000 (71.2574)  acc5: 87.5000 (87.1463)  time: 0.4384  data: 0.0002  max mem: 2094
[09:02:08.918129] Test:  [2200/6631]  eta: 0:32:26  loss: 0.9757 (1.4219)  acc1: 75.0000 (71.2631)  acc5: 87.5000 (87.1308)  time: 0.4396  data: 0.0002  max mem: 2094
[09:02:13.319418] Test:  [2210/6631]  eta: 0:32:22  loss: 1.4602 (1.4227)  acc1: 62.5000 (71.2573)  acc5: 87.5000 (87.1269)  time: 0.4401  data: 0.0002  max mem: 2094
[09:02:17.727211] Test:  [2220/6631]  eta: 0:32:17  loss: 1.2100 (1.4216)  acc1: 75.0000 (71.2517)  acc5: 87.5000 (87.1511)  time: 0.4404  data: 0.0002  max mem: 2094
