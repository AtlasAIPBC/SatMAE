/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
job dir: /home/ada/satmae/SatMAE
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=1,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAEjob dir: /home/ada/satmae/SatMAE

Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=7,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAENamespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=4,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=6,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)

Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=5,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=2,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=3,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/finetune',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
<util.datasets.CustomDatasetFromImages object at 0x7fac4fd5be10>
<util.datasets.CustomDatasetFromImages object at 0x7f0fd24b2bd0>
<util.datasets.CustomDatasetFromImages object at 0x7f72aa480c50>
<util.datasets.CustomDatasetFromImages object at 0x7f8a9aba1d10>
<util.datasets.CustomDatasetFromImages object at 0x7f1e4db08d50>
<util.datasets.CustomDatasetFromImages object at 0x7fa71bbd9850>
<util.datasets.CustomDatasetFromImages object at 0x7fcdcb02ef50>
<util.datasets.CustomDatasetFromImages object at 0x7fc2ecced550>
<util.datasets.CustomDatasetFromImages object at 0x7f0fd24acfd0>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f0fd24acd10>
<util.datasets.CustomDatasetFromImages object at 0x7fac4fd5b190>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fac58c0e390>
<util.datasets.CustomDatasetFromImages object at 0x7f8a9ab9bc90>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f8a9ab9bdd0>
<util.datasets.CustomDatasetFromImages object at 0x7f72aa47ac10>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f72aa47ad10>
<util.datasets.CustomDatasetFromImages object at 0x7f1e4db08250>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f1e4db08ed0>
<util.datasets.CustomDatasetFromImages object at 0x7fcdc3289c10>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fcdc3289850>
<util.datasets.CustomDatasetFromImages object at 0x7fa712e7bd90>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fa712e7bdd0>
<util.datasets.CustomDatasetFromImages object at 0x7fc2798ffcd0>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fc2798ffd90>
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
Test:  [   0/3316]  eta: 1 day, 5:19:47  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 31.8417  data: 1.3043
Test:  [   0/3316]  eta: 1 day, 5:07:53  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 31.6264  data: 1.7369
Test:  [   0/3316]  eta: 1 day, 5:39:43  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.2025  data: 1.5528
Test:  [   0/3316]  eta: 1 day, 5:49:53  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.3865  data: 1.1799
Test:  [   0/3316]  eta: 1 day, 5:51:19  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.4123  data: 1.6570
Test:  [   0/3316]  eta: 1 day, 5:58:53  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.5495  data: 1.5203
Test:  [   0/3316]  eta: 1 day, 6:01:56  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.6044  data: 1.1988
Test:  [   0/3316]  eta: 1 day, 6:04:52  loss: 1.1760 (1.1760)  acc1: 81.2500 (81.2500)  acc5: 87.5000 (87.5000)  time: 32.6577  data: 1.6810
Test:  [  10/3316]  eta: 1 day, 3:03:27  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 29.4639  data: 0.1206
Test:  [  10/3316]  eta: 1 day, 3:05:00  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 29.4921  data: 0.1603
Test:  [  10/3316]  eta: 1 day, 3:27:39  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 29.9031  data: 0.1545
Test:  [  10/3316]  eta: 1 day, 3:35:49  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 30.0513  data: 0.1531
Test:  [  10/3316]  eta: 1 day, 3:37:05  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 30.0742  data: 0.1437
Test:  [  10/3316]  eta: 1 day, 3:40:59  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 30.1449  data: 0.1102
Test:  [  10/3316]  eta: 1 day, 3:44:08  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 30.2023  data: 0.1411
Test:  [  10/3316]  eta: 1 day, 3:48:04  loss: 0.9194 (1.0413)  acc1: 81.2500 (77.2727)  acc5: 93.7500 (93.7500)  time: 30.2737  data: 0.1123
Test:  [  20/3316]  eta: 1 day, 2:42:36  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.0511  data: 0.0021
Test:  [  20/3316]  eta: 1 day, 2:48:27  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.1522  data: 0.0022
Test:  [  20/3316]  eta: 1 day, 3:13:23  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.5879  data: 0.0019
Test:  [  20/3316]  eta: 1 day, 3:17:11  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.6727  data: 0.0024
Test:  [  20/3316]  eta: 1 day, 3:19:50  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.7338  data: 0.0023
Test:  [  20/3316]  eta: 1 day, 3:23:00  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.7773  data: 0.0023
Test:  [  20/3316]  eta: 1 day, 3:23:47  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.8003  data: 0.0026
Test:  [  20/3316]  eta: 1 day, 3:27:08  loss: 0.9100 (1.0326)  acc1: 81.2500 (77.3810)  acc5: 93.7500 (92.8571)  time: 29.8534  data: 0.0028
Test:  [  30/3316]  eta: 1 day, 2:39:23  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.0452  data: 0.0023
Test:  [  30/3316]  eta: 1 day, 2:47:22  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.2864  data: 0.0027
Test:  [  30/3316]  eta: 1 day, 3:14:55  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.8245  data: 0.0025
Test:  [  30/3316]  eta: 1 day, 3:17:13  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.8084  data: 0.0025
Test:  [  30/3316]  eta: 1 day, 3:19:13  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.8522  data: 0.0023
Test:  [  30/3316]  eta: 1 day, 3:22:37  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.8779  data: 0.0021
Test:  [  30/3316]  eta: 1 day, 3:22:46  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.9138  data: 0.0024
Test:  [  30/3316]  eta: 1 day, 3:28:01  loss: 0.9100 (1.0508)  acc1: 75.0000 (76.6129)  acc5: 93.7500 (92.9435)  time: 29.9918  data: 0.0025
Test:  [  40/3316]  eta: 1 day, 2:34:43  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 29.2429  data: 0.0028
Test:  [  40/3316]  eta: 1 day, 2:41:48  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 29.3971  data: 0.0030
Test:  [  40/3316]  eta: 1 day, 3:09:51  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 29.9733  data: 0.0029
Test:  [  40/3316]  eta: 1 day, 3:11:04  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 29.9466  data: 0.0028
Test:  [  40/3316]  eta: 1 day, 3:13:14  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 29.9776  data: 0.0027
Test:  [  40/3316]  eta: 1 day, 3:17:21  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 30.0713  data: 0.0027
Test:  [  40/3316]  eta: 1 day, 3:18:10  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 30.0871  data: 0.0027
Test:  [  40/3316]  eta: 1 day, 3:21:11  loss: 1.1090 (1.0960)  acc1: 75.0000 (75.9146)  acc5: 93.7500 (92.8354)  time: 30.1359  data: 0.0030
Test:  [  50/3316]  eta: 1 day, 2:42:37  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 29.8112  data: 0.0032
Test:  [  50/3316]  eta: 1 day, 2:49:16  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 29.8968  data: 0.0031
Test:  [  50/3316]  eta: 1 day, 3:19:21  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.5263  data: 0.0030
Test:  [  50/3316]  eta: 1 day, 3:20:40  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.5229  data: 0.0027
Test:  [  50/3316]  eta: 1 day, 3:22:27  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.5502  data: 0.0028
Test:  [  50/3316]  eta: 1 day, 3:25:52  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.6142  data: 0.0031
Test:  [  50/3316]  eta: 1 day, 3:26:55  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.5099  data: 0.0034
Test:  [  50/3316]  eta: 1 day, 3:28:01  loss: 1.1731 (1.0980)  acc1: 75.0000 (75.9804)  acc5: 93.7500 (92.7696)  time: 30.7104  data: 0.0031
Test:  [  60/3316]  eta: 1 day, 2:33:02  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 29.6598  data: 0.0033
Test:  [  60/3316]  eta: 1 day, 2:38:17  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 29.6891  data: 0.0034
Test:  [  60/3316]  eta: 1 day, 3:10:56  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.4715  data: 0.0032
Test:  [  60/3316]  eta: 1 day, 3:11:58  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.4830  data: 0.0030
Test:  [  60/3316]  eta: 1 day, 3:12:34  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.4353  data: 0.0030
Test:  [  60/3316]  eta: 1 day, 3:17:12  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.3979  data: 0.0034
Test:  [  60/3316]  eta: 1 day, 3:17:28  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.5566  data: 0.0033
Test:  [  60/3316]  eta: 1 day, 3:19:39  loss: 1.1361 (1.1018)  acc1: 75.0000 (75.6148)  acc5: 93.7500 (92.9303)  time: 30.6490  data: 0.0033
Test:  [  70/3316]  eta: 1 day, 2:27:04  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.0653  data: 0.0029
Test:  [  70/3316]  eta: 1 day, 2:31:11  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.0243  data: 0.0033
Test:  [  70/3316]  eta: 1 day, 3:05:29  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.8662  data: 0.0033
Test:  [  70/3316]  eta: 1 day, 3:07:26  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.9325  data: 0.0032
Test:  [  70/3316]  eta: 1 day, 3:07:34  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.8570  data: 0.0030
Test:  [  70/3316]  eta: 1 day, 3:11:42  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 29.9191  data: 0.0030
Test:  [  70/3316]  eta: 1 day, 3:12:59  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 30.0527  data: 0.0033
Test:  [  70/3316]  eta: 1 day, 3:15:05  loss: 1.0721 (1.1209)  acc1: 75.0000 (75.1761)  acc5: 93.7500 (92.5176)  time: 30.0895  data: 0.0031
Test:  [  80/3316]  eta: 1 day, 2:20:30  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 29.1491  data: 0.0029
Test:  [  80/3316]  eta: 1 day, 2:24:45  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 29.1733  data: 0.0029
Test:  [  80/3316]  eta: 1 day, 2:58:00  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 29.8352  data: 0.0033
Test:  [  80/3316]  eta: 1 day, 3:00:25  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 29.9251  data: 0.0030
Test:  [  80/3316]  eta: 1 day, 3:00:29  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 29.9640  data: 0.0030
Test:  [  80/3316]  eta: 1 day, 3:05:38  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 30.0562  data: 0.0028
Test:  [  80/3316]  eta: 1 day, 3:06:36  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 30.1137  data: 0.0032
Test:  [  80/3316]  eta: 1 day, 3:07:00  loss: 1.0341 (1.0986)  acc1: 81.2500 (75.7716)  acc5: 93.7500 (92.9012)  time: 30.0211  data: 0.0031
Test:  [  90/3316]  eta: 1 day, 2:14:04  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 29.0639  data: 0.0029
Test:  [  90/3316]  eta: 1 day, 2:18:47  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 29.1926  data: 0.0026
Test:  [  90/3316]  eta: 1 day, 2:59:14  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.3646  data: 0.0030
Test:  [  90/3316]  eta: 1 day, 3:04:39  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.6952  data: 0.0030
Test:  [  90/3316]  eta: 1 day, 3:04:50  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.7016  data: 0.0031
Test:  [  90/3316]  eta: 1 day, 3:09:30  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.8254  data: 0.0028
Test:  [  90/3316]  eta: 1 day, 3:10:37  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.8347  data: 0.0030
Test:  [  90/3316]  eta: 1 day, 3:11:44  loss: 0.9786 (1.0986)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.6511)  time: 30.7927  data: 0.0032
Test:  [ 100/3316]  eta: 1 day, 2:16:54  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 29.8858  data: 0.0029
Test:  [ 100/3316]  eta: 1 day, 2:21:29  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 29.9988  data: 0.0030
Test:  [ 100/3316]  eta: 1 day, 2:51:38  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 30.3416  data: 0.0030
Test:  [ 100/3316]  eta: 1 day, 2:57:59  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 30.7539  data: 0.0032
Test:  [ 100/3316]  eta: 1 day, 2:58:51  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 30.8408  data: 0.0031
Test:  [ 100/3316]  eta: 1 day, 3:02:08  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 30.7593  data: 0.0030
Test:  [ 100/3316]  eta: 1 day, 3:04:20  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 30.8940  data: 0.0030
Test:  [ 100/3316]  eta: 1 day, 3:06:08  loss: 0.9786 (1.0927)  acc1: 81.2500 (76.4233)  acc5: 93.7500 (92.6980)  time: 31.0320  data: 0.0033
Test:  [ 110/3316]  eta: 1 day, 2:09:24  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.8043  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 2:13:34  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.8382  data: 0.0030
Test:  [ 110/3316]  eta: 1 day, 2:43:16  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.4996  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 2:51:22  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.8831  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 2:52:23  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.9744  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 2:54:17  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.7768  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 2:57:14  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 29.9881  data: 0.0029
Test:  [ 110/3316]  eta: 1 day, 3:00:20  loss: 1.1283 (1.1097)  acc1: 75.0000 (76.0135)  acc5: 93.7500 (92.4550)  time: 30.2156  data: 0.0028
Test:  [ 120/3316]  eta: 1 day, 2:03:51  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 29.0514  data: 0.0029
Test:  [ 120/3316]  eta: 1 day, 2:07:34  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 29.0420  data: 0.0029
Test:  [ 120/3316]  eta: 1 day, 2:36:55  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 29.5357  data: 0.0028
Test:  [ 120/3316]  eta: 1 day, 2:45:58  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 29.9651  data: 0.0028
Test:  [ 120/3316]  eta: 1 day, 2:47:29  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 30.0553  data: 0.0028
Test:  [ 120/3316]  eta: 1 day, 2:48:08  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 29.8180  data: 0.0030
Test:  [ 120/3316]  eta: 1 day, 2:51:56  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 30.0440  data: 0.0030
Test:  [ 120/3316]  eta: 1 day, 2:55:28  loss: 1.1359 (1.1119)  acc1: 75.0000 (75.9298)  acc5: 87.5000 (92.2521)  time: 30.2760  data: 0.0028
Test:  [ 130/3316]  eta: 1 day, 1:58:29  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 29.2333  data: 0.0031
Test:  [ 130/3316]  eta: 1 day, 2:02:08  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 29.2503  data: 0.0032
Test:  [ 130/3316]  eta: 1 day, 2:30:44  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 29.6929  data: 0.0029
Test:  [ 130/3316]  eta: 1 day, 2:41:06  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 30.1290  data: 0.0030
Test:  [ 130/3316]  eta: 1 day, 2:42:02  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 30.1397  data: 0.0030
Test:  [ 130/3316]  eta: 1 day, 2:42:18  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 29.9749  data: 0.0034
Test:  [ 130/3316]  eta: 1 day, 2:46:45  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 30.2174  data: 0.0032
Test:  [ 130/3316]  eta: 1 day, 2:50:10  loss: 1.1536 (1.1204)  acc1: 75.0000 (75.5725)  acc5: 93.7500 (92.4141)  time: 30.3168  data: 0.0030
Test:  [ 140/3316]  eta: 1 day, 1:53:21  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 29.2639  data: 0.0029
Test:  [ 140/3316]  eta: 1 day, 1:56:32  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 29.2660  data: 0.0032
Test:  [ 140/3316]  eta: 1 day, 2:24:48  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 29.6967  data: 0.0029
Test:  [ 140/3316]  eta: 1 day, 2:35:14  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 30.0592  data: 0.0030
Test:  [ 140/3316]  eta: 1 day, 2:36:12  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 30.0152  data: 0.0032
Test:  [ 140/3316]  eta: 1 day, 2:36:34  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 29.9916  data: 0.0032
Test:  [ 140/3316]  eta: 1 day, 2:41:11  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 30.1717  data: 0.0031
Test:  [ 140/3316]  eta: 1 day, 2:44:19  loss: 1.1536 (1.1259)  acc1: 75.0000 (75.4876)  acc5: 93.7500 (92.2872)  time: 30.1885  data: 0.0031
Test:  [ 150/3316]  eta: 1 day, 1:51:11  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 29.7049  data: 0.0028
Test:  [ 150/3316]  eta: 1 day, 1:55:53  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 29.9270  data: 0.0032
Test:  [ 150/3316]  eta: 1 day, 2:26:25  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 30.7686  data: 0.0029
Test:  [ 150/3316]  eta: 1 day, 2:38:01  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 31.1495  data: 0.0030
Test:  [ 150/3316]  eta: 1 day, 2:38:32  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 31.0744  data: 0.0029
Test:  [ 150/3316]  eta: 1 day, 2:38:37  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 31.1204  data: 0.0033
Test:  [ 150/3316]  eta: 1 day, 2:43:56  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 31.3007  data: 0.0032
Test:  [ 150/3316]  eta: 1 day, 2:46:27  loss: 1.1891 (1.1349)  acc1: 75.0000 (75.4553)  acc5: 93.7500 (92.2185)  time: 31.2372  data: 0.0032
Test:  [ 160/3316]  eta: 1 day, 1:49:36  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 30.2696  data: 0.0028
Test:  [ 160/3316]  eta: 1 day, 1:52:43  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 30.3217  data: 0.0032
Test:  [ 160/3316]  eta: 1 day, 2:19:41  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 30.6840  data: 0.0028
Test:  [ 160/3316]  eta: 1 day, 2:32:04  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 31.0110  data: 0.0030
Test:  [ 160/3316]  eta: 1 day, 2:32:12  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 31.2098  data: 0.0029
Test:  [ 160/3316]  eta: 1 day, 2:32:35  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 31.1389  data: 0.0032
Test:  [ 160/3316]  eta: 1 day, 2:38:12  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 31.3373  data: 0.0032
Test:  [ 160/3316]  eta: 1 day, 2:39:18  loss: 1.1964 (1.1401)  acc1: 75.0000 (75.5435)  acc5: 93.7500 (92.0807)  time: 31.0871  data: 0.0031
Test:  [ 170/3316]  eta: 1 day, 1:43:28  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.7378  data: 0.0028
Test:  [ 170/3316]  eta: 1 day, 1:46:41  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.5903  data: 0.0030
Test:  [ 170/3316]  eta: 1 day, 2:12:19  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.3995  data: 0.0028
Test:  [ 170/3316]  eta: 1 day, 2:25:17  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.7808  data: 0.0031
Test:  [ 170/3316]  eta: 1 day, 2:25:35  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.9049  data: 0.0029
Test:  [ 170/3316]  eta: 1 day, 2:26:01  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.8876  data: 0.0029
Test:  [ 170/3316]  eta: 1 day, 2:30:58  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.9346  data: 0.0028
Test:  [ 170/3316]  eta: 1 day, 2:32:09  loss: 1.1569 (1.1463)  acc1: 75.0000 (75.4386)  acc5: 93.7500 (92.1053)  time: 29.7677  data: 0.0029
Test:  [ 180/3316]  eta: 1 day, 1:37:11  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.0099  data: 0.0029
Test:  [ 180/3316]  eta: 1 day, 1:40:31  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.1117  data: 0.0028
Test:  [ 180/3316]  eta: 1 day, 2:05:57  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.3863  data: 0.0029
Test:  [ 180/3316]  eta: 1 day, 2:19:06  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.7712  data: 0.0031
Test:  [ 180/3316]  eta: 1 day, 2:19:30  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.8172  data: 0.0028
Test:  [ 180/3316]  eta: 1 day, 2:20:11  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.8788  data: 0.0028
Test:  [ 180/3316]  eta: 1 day, 2:25:01  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.8554  data: 0.0028
Test:  [ 180/3316]  eta: 1 day, 2:25:46  loss: 1.1107 (1.1350)  acc1: 75.0000 (75.7251)  acc5: 93.7500 (92.1961)  time: 29.8168  data: 0.0029
Test:  [ 190/3316]  eta: 1 day, 1:31:27  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.0314  data: 0.0031
Test:  [ 190/3316]  eta: 1 day, 1:34:46  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.1152  data: 0.0029
Test:  [ 190/3316]  eta: 1 day, 1:59:34  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.4840  data: 0.0031
Test:  [ 190/3316]  eta: 1 day, 2:12:42  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.7745  data: 0.0032
Test:  [ 190/3316]  eta: 1 day, 2:13:09  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.8091  data: 0.0028
Test:  [ 190/3316]  eta: 1 day, 2:13:11  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.7451  data: 0.0029
Test:  [ 190/3316]  eta: 1 day, 2:18:05  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.8335  data: 0.0030
Test:  [ 190/3316]  eta: 1 day, 2:18:47  loss: 0.9661 (1.1348)  acc1: 81.2500 (75.8181)  acc5: 93.7500 (92.0812)  time: 29.7709  data: 0.0033
Test:  [ 200/3316]  eta: 1 day, 1:25:09  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 28.9791  data: 0.0030
Test:  [ 200/3316]  eta: 1 day, 1:28:32  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.0553  data: 0.0030
Test:  [ 200/3316]  eta: 1 day, 1:53:24  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.4655  data: 0.0032
Test:  [ 200/3316]  eta: 1 day, 2:07:22  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.8898  data: 0.0030
Test:  [ 200/3316]  eta: 1 day, 2:08:03  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.8340  data: 0.0027
Test:  [ 200/3316]  eta: 1 day, 2:08:15  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.9918  data: 0.0028
Test:  [ 200/3316]  eta: 1 day, 2:12:34  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.8704  data: 0.0030
Test:  [ 200/3316]  eta: 1 day, 2:13:23  loss: 0.9901 (1.1329)  acc1: 75.0000 (75.7774)  acc5: 93.7500 (92.1331)  time: 29.8999  data: 0.0031
Test:  [ 210/3316]  eta: 1 day, 1:21:09  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 29.2913  data: 0.0029
Test:  [ 210/3316]  eta: 1 day, 1:24:41  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 29.4047  data: 0.0030
Test:  [ 210/3316]  eta: 1 day, 1:48:25  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 29.6935  data: 0.0033
Test:  [ 210/3316]  eta: 1 day, 2:02:31  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 30.1617  data: 0.0030
Test:  [ 210/3316]  eta: 1 day, 2:03:02  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 30.1790  data: 0.0028
Test:  [ 210/3316]  eta: 1 day, 2:03:20  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 30.2433  data: 0.0030
Test:  [ 210/3316]  eta: 1 day, 2:07:04  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 30.1019  data: 0.0031
Test:  [ 210/3316]  eta: 1 day, 2:08:08  loss: 0.9962 (1.1319)  acc1: 81.2500 (75.9479)  acc5: 93.7500 (92.0912)  time: 30.1925  data: 0.0030
Test:  [ 220/3316]  eta: 1 day, 1:17:15  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 29.7726  data: 0.0030
Test:  [ 220/3316]  eta: 1 day, 1:21:10  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 29.9557  data: 0.0030
Test:  [ 220/3316]  eta: 1 day, 1:44:05  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.0539  data: 0.0033
Test:  [ 220/3316]  eta: 1 day, 1:58:49  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.5047  data: 0.0033
Test:  [ 220/3316]  eta: 1 day, 1:58:54  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.3913  data: 0.0030
Test:  [ 220/3316]  eta: 1 day, 1:59:18  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.4371  data: 0.0032
Test:  [ 220/3316]  eta: 1 day, 2:02:48  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.3520  data: 0.0032
Test:  [ 220/3316]  eta: 1 day, 2:03:54  loss: 1.0684 (1.1460)  acc1: 75.0000 (75.6222)  acc5: 87.5000 (91.7986)  time: 30.4289  data: 0.0033
Test:  [ 230/3316]  eta: 1 day, 1:12:17  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 29.5940  data: 0.0030
Test:  [ 230/3316]  eta: 1 day, 1:16:28  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 29.8129  data: 0.0029
Test:  [ 230/3316]  eta: 1 day, 1:38:17  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 29.8776  data: 0.0032
Test:  [ 230/3316]  eta: 1 day, 1:53:03  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 30.2090  data: 0.0030
Test:  [ 230/3316]  eta: 1 day, 1:53:41  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 30.4619  data: 0.0033
Test:  [ 230/3316]  eta: 1 day, 1:53:49  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 30.3229  data: 0.0031
Test:  [ 230/3316]  eta: 1 day, 1:56:55  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 30.2596  data: 0.0032
Test:  [ 230/3316]  eta: 1 day, 1:58:02  loss: 1.0060 (1.1363)  acc1: 75.0000 (75.8658)  acc5: 93.7500 (91.9372)  time: 30.2921  data: 0.0030
Test:  [ 240/3316]  eta: 1 day, 1:06:43  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.2320  data: 0.0031
Test:  [ 240/3316]  eta: 1 day, 1:11:05  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.4191  data: 0.0030
Test:  [ 240/3316]  eta: 1 day, 1:32:03  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.4380  data: 0.0029
Test:  [ 240/3316]  eta: 1 day, 1:46:28  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.6554  data: 0.0027
Test:  [ 240/3316]  eta: 1 day, 1:47:19  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.8724  data: 0.0029
Test:  [ 240/3316]  eta: 1 day, 1:47:25  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.7924  data: 0.0029
Test:  [ 240/3316]  eta: 1 day, 1:50:16  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.7137  data: 0.0030
Test:  [ 240/3316]  eta: 1 day, 1:51:30  loss: 0.9485 (1.1358)  acc1: 81.2500 (75.8817)  acc5: 93.7500 (92.0384)  time: 29.7687  data: 0.0028
Test:  [ 250/3316]  eta: 1 day, 1:00:56  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.0222  data: 0.0029
Test:  [ 250/3316]  eta: 1 day, 1:05:49  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.2834  data: 0.0029
Test:  [ 250/3316]  eta: 1 day, 1:26:34  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.4800  data: 0.0027
Test:  [ 250/3316]  eta: 1 day, 1:40:58  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.7023  data: 0.0026
Test:  [ 250/3316]  eta: 1 day, 1:41:50  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.7707  data: 0.0029
Test:  [ 250/3316]  eta: 1 day, 1:42:15  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.8444  data: 0.0029
Test:  [ 250/3316]  eta: 1 day, 1:44:48  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.7730  data: 0.0028
Test:  [ 250/3316]  eta: 1 day, 1:46:17  loss: 0.9762 (1.1338)  acc1: 75.0000 (75.8715)  acc5: 93.7500 (92.0070)  time: 29.8855  data: 0.0029
Test:  [ 260/3316]  eta: 1 day, 0:55:57  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.1400  data: 0.0029
Test:  [ 260/3316]  eta: 1 day, 1:00:32  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.2923  data: 0.0031
Test:  [ 260/3316]  eta: 1 day, 1:20:44  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.5373  data: 0.0028
Test:  [ 260/3316]  eta: 1 day, 1:34:52  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.7714  data: 0.0027
Test:  [ 260/3316]  eta: 1 day, 1:35:44  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.7901  data: 0.0034
Test:  [ 260/3316]  eta: 1 day, 1:36:14  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.8991  data: 0.0029
Test:  [ 260/3316]  eta: 1 day, 1:38:25  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.7881  data: 0.0029
Test:  [ 260/3316]  eta: 1 day, 1:40:07  loss: 1.1206 (1.1331)  acc1: 75.0000 (76.0057)  acc5: 93.7500 (92.0019)  time: 29.9316  data: 0.0032
Test:  [ 270/3316]  eta: 1 day, 0:49:57  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.0532  data: 0.0030
Test:  [ 270/3316]  eta: 1 day, 0:54:37  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.0977  data: 0.0030
Test:  [ 270/3316]  eta: 1 day, 1:14:51  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.4037  data: 0.0028
Test:  [ 270/3316]  eta: 1 day, 1:29:29  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.7746  data: 0.0028
Test:  [ 270/3316]  eta: 1 day, 1:30:28  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.8210  data: 0.0033
Test:  [ 270/3316]  eta: 1 day, 1:31:07  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.8959  data: 0.0029
Test:  [ 270/3316]  eta: 1 day, 1:33:03  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.7849  data: 0.0030
Test:  [ 270/3316]  eta: 1 day, 1:34:53  loss: 1.1206 (1.1356)  acc1: 75.0000 (75.9225)  acc5: 93.7500 (91.9280)  time: 29.9086  data: 0.0032
Test:  [ 280/3316]  eta: 1 day, 0:45:06  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 29.0803  data: 0.0028
Test:  [ 280/3316]  eta: 1 day, 0:50:09  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 29.3023  data: 0.0031
Test:  [ 280/3316]  eta: 1 day, 1:10:01  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 29.6477  data: 0.0029
Test:  [ 280/3316]  eta: 1 day, 1:24:02  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 29.9135  data: 0.0028
Test:  [ 280/3316]  eta: 1 day, 1:25:16  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 30.0400  data: 0.0031
Test:  [ 280/3316]  eta: 1 day, 1:25:40  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 30.0163  data: 0.0030
Test:  [ 280/3316]  eta: 1 day, 1:27:30  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 29.9671  data: 0.0031
Test:  [ 280/3316]  eta: 1 day, 1:29:21  loss: 1.0592 (1.1345)  acc1: 75.0000 (75.9119)  acc5: 93.7500 (91.9484)  time: 30.0478  data: 0.0031
Test:  [ 290/3316]  eta: 1 day, 0:39:46  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 29.2336  data: 0.0028
Test:  [ 290/3316]  eta: 1 day, 0:44:52  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 29.4619  data: 0.0033
Test:  [ 290/3316]  eta: 1 day, 1:06:23  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.2662  data: 0.0032
Test:  [ 290/3316]  eta: 1 day, 1:20:37  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.4662  data: 0.0036
Test:  [ 290/3316]  eta: 1 day, 1:22:00  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.6061  data: 0.0037
Test:  [ 290/3316]  eta: 1 day, 1:22:29  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.5688  data: 0.0037
Test:  [ 290/3316]  eta: 1 day, 1:24:07  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.5264  data: 0.0040
Test:  [ 290/3316]  eta: 1 day, 1:25:59  loss: 0.8917 (1.1314)  acc1: 75.0000 (75.9880)  acc5: 93.7500 (91.9674)  time: 30.5741  data: 0.0037
Test:  [ 300/3316]  eta: 1 day, 0:36:50  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 29.8043  data: 0.0039
Test:  [ 300/3316]  eta: 1 day, 0:42:04  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 29.9682  data: 0.0043
Test:  [ 300/3316]  eta: 1 day, 1:01:42  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.3309  data: 0.0041
Test:  [ 300/3316]  eta: 1 day, 1:15:25  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.5466  data: 0.0042
Test:  [ 300/3316]  eta: 1 day, 1:16:59  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.6702  data: 0.0041
Test:  [ 300/3316]  eta: 1 day, 1:17:32  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.7302  data: 0.0039
Test:  [ 300/3316]  eta: 1 day, 1:19:00  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.6554  data: 0.0043
Test:  [ 300/3316]  eta: 1 day, 1:20:36  loss: 0.8524 (1.1261)  acc1: 75.0000 (76.0797)  acc5: 93.7500 (92.0681)  time: 30.6222  data: 0.0040
Test:  [ 310/3316]  eta: 1 day, 0:31:39  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 29.8573  data: 0.0040
Test:  [ 310/3316]  eta: 1 day, 0:37:02  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.0546  data: 0.0043
Test:  [ 310/3316]  eta: 1 day, 0:56:33  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 29.9045  data: 0.0039
Test:  [ 310/3316]  eta: 1 day, 1:10:07  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.0119  data: 0.0035
Test:  [ 310/3316]  eta: 1 day, 1:11:57  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.1831  data: 0.0036
Test:  [ 310/3316]  eta: 1 day, 1:12:40  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.2626  data: 0.0033
Test:  [ 310/3316]  eta: 1 day, 1:14:06  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.2381  data: 0.0033
Test:  [ 310/3316]  eta: 1 day, 1:15:32  loss: 0.9390 (1.1279)  acc1: 75.0000 (75.9445)  acc5: 93.7500 (92.0619)  time: 30.1454  data: 0.0032
Test:  [ 320/3316]  eta: 1 day, 0:26:57  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 29.3513  data: 0.0032
Test:  [ 320/3316]  eta: 1 day, 0:32:22  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 29.5280  data: 0.0030
Test:  [ 320/3316]  eta: 1 day, 0:51:52  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 29.9155  data: 0.0033
Test:  [ 320/3316]  eta: 1 day, 1:05:17  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 30.1205  data: 0.0030
Test:  [ 320/3316]  eta: 1 day, 1:07:15  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 30.2831  data: 0.0033
Test:  [ 320/3316]  eta: 1 day, 1:07:49  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 30.2994  data: 0.0031
Test:  [ 320/3316]  eta: 1 day, 1:09:05  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 30.2677  data: 0.0030
Test:  [ 320/3316]  eta: 1 day, 1:10:34  loss: 1.0992 (1.1292)  acc1: 68.7500 (75.9346)  acc5: 93.7500 (91.9977)  time: 30.2647  data: 0.0032
Test:  [ 330/3316]  eta: 1 day, 0:22:03  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 29.4395  data: 0.0031
Test:  [ 330/3316]  eta: 1 day, 0:27:29  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 29.5730  data: 0.0033
Test:  [ 330/3316]  eta: 1 day, 0:48:12  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.4053  data: 0.0037
Test:  [ 330/3316]  eta: 1 day, 1:01:20  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.5644  data: 0.0040
Test:  [ 330/3316]  eta: 1 day, 1:03:46  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.8034  data: 0.0038
Test:  [ 330/3316]  eta: 1 day, 1:04:24  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.7917  data: 0.0040
Test:  [ 330/3316]  eta: 1 day, 1:05:25  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.6854  data: 0.0039
Test:  [ 330/3316]  eta: 1 day, 1:07:00  loss: 1.0675 (1.1289)  acc1: 75.0000 (75.8875)  acc5: 93.7500 (92.0128)  time: 30.7643  data: 0.0046
Test:  [ 340/3316]  eta: 1 day, 0:18:44  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 29.9162  data: 0.0039
Test:  [ 340/3316]  eta: 1 day, 0:24:10  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.0469  data: 0.0041
Test:  [ 340/3316]  eta: 1 day, 0:43:10  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.3094  data: 0.0041
Test:  [ 340/3316]  eta: 1 day, 0:56:39  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.6325  data: 0.0044
Test:  [ 340/3316]  eta: 1 day, 0:59:09  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.8597  data: 0.0039
Test:  [ 340/3316]  eta: 1 day, 0:59:34  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.8182  data: 0.0043
Test:  [ 340/3316]  eta: 1 day, 1:00:30  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.7380  data: 0.0042
Test:  [ 350/3316]  eta: 1 day, 0:13:59  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 29.9938  data: 0.0041
Test:  [ 340/3316]  eta: 1 day, 1:02:07  loss: 1.2033 (1.1333)  acc1: 68.7500 (75.6782)  acc5: 87.5000 (91.9721)  time: 30.8121  data: 0.0047
Test:  [ 350/3316]  eta: 1 day, 0:19:18  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.0743  data: 0.0040
Test:  [ 350/3316]  eta: 1 day, 0:37:51  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 29.7707  data: 0.0038
Test:  [ 350/3316]  eta: 1 day, 0:50:55  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.0373  data: 0.0035
Test:  [ 350/3316]  eta: 1 day, 0:53:23  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.1046  data: 0.0033
Test:  [ 350/3316]  eta: 1 day, 0:53:51  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.0622  data: 0.0033
Test:  [ 360/3316]  eta: 1 day, 0:08:25  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.2234  data: 0.0030
Test:  [ 350/3316]  eta: 1 day, 0:54:48  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.0590  data: 0.0031
Test:  [ 350/3316]  eta: 1 day, 0:56:18  loss: 1.1283 (1.1279)  acc1: 75.0000 (75.8013)  acc5: 93.7500 (92.0762)  time: 30.0703  data: 0.0032
Test:  [ 360/3316]  eta: 1 day, 0:13:52  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.3503  data: 0.0030
Test:  [ 360/3316]  eta: 1 day, 0:32:07  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.5017  data: 0.0031
Test:  [ 360/3316]  eta: 1 day, 0:45:27  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.7562  data: 0.0029
Test:  [ 360/3316]  eta: 1 day, 0:48:13  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.9128  data: 0.0029
Test:  [ 370/3316]  eta: 1 day, 0:03:21  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 29.0974  data: 0.0029
Test:  [ 360/3316]  eta: 1 day, 0:48:37  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.9162  data: 0.0027
Test:  [ 360/3316]  eta: 1 day, 0:49:23  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.8746  data: 0.0029
Test:  [ 360/3316]  eta: 1 day, 0:50:48  loss: 0.9914 (1.1272)  acc1: 81.2500 (75.7964)  acc5: 93.7500 (92.0880)  time: 29.8378  data: 0.0028
Test:  [ 370/3316]  eta: 1 day, 0:08:39  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 29.2127  data: 0.0028
Test:  [ 370/3316]  eta: 1 day, 0:27:33  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 29.7675  data: 0.0031
Test:  [ 370/3316]  eta: 1 day, 0:40:24  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 29.9866  data: 0.0034
Test:  [ 380/3316]  eta: 23:58:39  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.4128  data: 0.0039
Test:  [ 370/3316]  eta: 1 day, 0:43:08  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 30.1388  data: 0.0037
Test:  [ 370/3316]  eta: 1 day, 0:43:33  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 30.1348  data: 0.0035
Test:  [ 370/3316]  eta: 1 day, 0:44:25  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 30.1259  data: 0.0034
Test:  [ 370/3316]  eta: 1 day, 0:45:46  loss: 0.9771 (1.1223)  acc1: 81.2500 (75.8929)  acc5: 93.7500 (92.1833)  time: 30.0967  data: 0.0035
Test:  [ 380/3316]  eta: 1 day, 0:03:45  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.3976  data: 0.0035
Test:  [ 380/3316]  eta: 1 day, 0:21:31  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.6334  data: 0.0038
Test:  [ 380/3316]  eta: 1 day, 0:34:37  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.8442  data: 0.0039
Test:  [ 390/3316]  eta: 23:53:07  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 29.2236  data: 0.0037
Test:  [ 380/3316]  eta: 1 day, 0:37:21  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.8975  data: 0.0039
Test:  [ 380/3316]  eta: 1 day, 0:37:55  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 29.9681  data: 0.0037
Test:  [ 380/3316]  eta: 1 day, 0:38:56  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 30.0859  data: 0.0034
Test:  [ 380/3316]  eta: 1 day, 0:40:07  loss: 0.8450 (1.1214)  acc1: 81.2500 (75.9350)  acc5: 93.7500 (92.1916)  time: 30.0292  data: 0.0036
Test:  [ 390/3316]  eta: 23:58:14  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 29.2725  data: 0.0034
Test:  [ 390/3316]  eta: 1 day, 0:16:26  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 29.4310  data: 0.0037
Test:  [ 390/3316]  eta: 1 day, 0:29:25  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 29.7738  data: 0.0035
Test:  [ 400/3316]  eta: 23:48:07  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 29.0970  data: 0.0028
Test:  [ 390/3316]  eta: 1 day, 0:33:29  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 30.3748  data: 0.0034
Test:  [ 390/3316]  eta: 1 day, 0:34:02  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 30.4386  data: 0.0030
Test:  [ 390/3316]  eta: 1 day, 0:35:00  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 30.4941  data: 0.0031
Test:  [ 390/3316]  eta: 1 day, 0:36:06  loss: 0.9978 (1.1204)  acc1: 81.2500 (76.0070)  acc5: 93.7500 (92.1515)  time: 30.4318  data: 0.0034
Test:  [ 400/3316]  eta: 23:54:24  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 29.7073  data: 0.0030
Test:  [ 400/3316]  eta: 1 day, 0:12:33  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.2752  data: 0.0041
Test:  [ 410/3316]  eta: 23:43:55  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.6408  data: 0.0040
Test:  [ 400/3316]  eta: 1 day, 0:25:07  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.3657  data: 0.0044
Test:  [ 400/3316]  eta: 1 day, 0:27:55  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.4464  data: 0.0047
Test:  [ 400/3316]  eta: 1 day, 0:28:34  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.5047  data: 0.0044
Test:  [ 400/3316]  eta: 1 day, 0:29:30  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.4837  data: 0.0039
Test:  [ 410/3316]  eta: 23:49:08  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.8045  data: 0.0041
Test:  [ 400/3316]  eta: 1 day, 0:30:23  loss: 0.9988 (1.1192)  acc1: 75.0000 (76.0131)  acc5: 93.7500 (92.1914)  time: 30.3894  data: 0.0043
Test:  [ 410/3316]  eta: 1 day, 0:07:09  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 30.1475  data: 0.0043
Test:  [ 420/3316]  eta: 23:38:42  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.5488  data: 0.0040
Test:  [ 410/3316]  eta: 1 day, 0:19:37  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 30.2342  data: 0.0042
Test:  [ 410/3316]  eta: 1 day, 0:22:45  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.9350  data: 0.0047
Test:  [ 410/3316]  eta: 1 day, 0:23:19  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.9501  data: 0.0046
Test:  [ 420/3316]  eta: 23:44:04  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.2977  data: 0.0040
Test:  [ 410/3316]  eta: 1 day, 0:24:10  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.9292  data: 0.0038
Test:  [ 410/3316]  eta: 1 day, 0:24:54  loss: 1.1879 (1.1220)  acc1: 75.0000 (76.0341)  acc5: 93.7500 (92.1533)  time: 29.7961  data: 0.0042
Test:  [ 420/3316]  eta: 1 day, 0:01:30  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.4142  data: 0.0031
Test:  [ 430/3316]  eta: 23:33:13  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 28.9938  data: 0.0027
Test:  [ 420/3316]  eta: 1 day, 0:13:47  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.5827  data: 0.0027
Test:  [ 420/3316]  eta: 1 day, 0:16:59  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.8288  data: 0.0031
Test:  [ 430/3316]  eta: 23:38:29  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.1395  data: 0.0030
Test:  [ 420/3316]  eta: 1 day, 0:17:35  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.8151  data: 0.0029
Test:  [ 420/3316]  eta: 1 day, 0:18:22  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.7828  data: 0.0027
Test:  [ 420/3316]  eta: 1 day, 0:19:02  loss: 1.1617 (1.1231)  acc1: 81.2500 (76.0095)  acc5: 93.7500 (92.1615)  time: 29.7041  data: 0.0029
Test:  [ 430/3316]  eta: 23:56:02  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.3650  data: 0.0026
Test:  [ 440/3316]  eta: 23:28:06  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.0296  data: 0.0027
Test:  [ 430/3316]  eta: 1 day, 0:08:21  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.5894  data: 0.0025
Test:  [ 440/3316]  eta: 23:33:16  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.0634  data: 0.0030
Test:  [ 430/3316]  eta: 1 day, 0:11:39  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.7424  data: 0.0030
Test:  [ 430/3316]  eta: 1 day, 0:12:21  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.8129  data: 0.0029
Test:  [ 430/3316]  eta: 1 day, 0:13:30  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 29.9653  data: 0.0027
Test:  [ 430/3316]  eta: 1 day, 0:14:17  loss: 1.0499 (1.1213)  acc1: 81.2500 (76.0586)  acc5: 93.7500 (92.1839)  time: 30.0053  data: 0.0027
Test:  [ 440/3316]  eta: 23:51:12  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.7087  data: 0.0029
Test:  [ 450/3316]  eta: 23:23:05  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.2210  data: 0.0032
Test:  [ 440/3316]  eta: 1 day, 0:03:09  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.8467  data: 0.0028
Test:  [ 450/3316]  eta: 23:28:10  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.2642  data: 0.0035
Test:  [ 440/3316]  eta: 1 day, 0:06:10  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.8424  data: 0.0034
Test:  [ 440/3316]  eta: 1 day, 0:07:07  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 30.0198  data: 0.0031
Test:  [ 440/3316]  eta: 1 day, 0:07:45  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 29.9605  data: 0.0033
Test:  [ 440/3316]  eta: 1 day, 0:08:33  loss: 1.0727 (1.1253)  acc1: 75.0000 (76.0062)  acc5: 87.5000 (92.1344)  time: 30.0402  data: 0.0032
Test:  [ 450/3316]  eta: 23:45:48  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.7185  data: 0.0032
Test:  [ 460/3316]  eta: 23:18:01  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 29.2350  data: 0.0033
Test:  [ 450/3316]  eta: 23:57:52  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.9001  data: 0.0030
Test:  [ 460/3316]  eta: 23:23:08  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 29.3351  data: 0.0034
Test:  [ 450/3316]  eta: 1 day, 0:01:09  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.9673  data: 0.0034
Test:  [ 450/3316]  eta: 1 day, 0:02:06  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 30.1098  data: 0.0031
Test:  [ 450/3316]  eta: 1 day, 0:02:29  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.7709  data: 0.0034
Test:  [ 450/3316]  eta: 1 day, 0:03:21  loss: 1.1400 (1.1258)  acc1: 75.0000 (76.0394)  acc5: 87.5000 (92.0870)  time: 29.8342  data: 0.0034
Test:  [ 460/3316]  eta: 23:41:19  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 29.8940  data: 0.0031
Test:  [ 470/3316]  eta: 23:13:54  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 29.6807  data: 0.0038
Test:  [ 460/3316]  eta: 23:53:15  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 30.1711  data: 0.0041
Test:  [ 470/3316]  eta: 23:18:50  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 29.7262  data: 0.0046
Test:  [ 460/3316]  eta: 23:56:39  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 30.4298  data: 0.0046
Test:  [ 460/3316]  eta: 23:57:44  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 30.5162  data: 0.0044
Test:  [ 460/3316]  eta: 23:57:57  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 30.3356  data: 0.0042
Test:  [ 460/3316]  eta: 23:58:54  loss: 0.9276 (1.1273)  acc1: 75.0000 (76.0168)  acc5: 93.7500 (92.0824)  time: 30.4263  data: 0.0045
Test:  [ 470/3316]  eta: 23:35:57  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 29.9005  data: 0.0049
Test:  [ 480/3316]  eta: 23:08:28  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.4939  data: 0.0046
Test:  [ 470/3316]  eta: 23:47:30  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 29.9400  data: 0.0045
Test:  [ 480/3316]  eta: 23:13:22  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.5110  data: 0.0053
Test:  [ 470/3316]  eta: 23:51:14  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 30.2400  data: 0.0051
Test:  [ 470/3316]  eta: 23:52:15  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 30.2969  data: 0.0050
Test:  [ 470/3316]  eta: 23:52:25  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 30.2014  data: 0.0046
Test:  [ 470/3316]  eta: 23:53:25  loss: 0.9352 (1.1247)  acc1: 75.0000 (76.1014)  acc5: 93.7500 (92.1046)  time: 30.2865  data: 0.0048
Test:  [ 480/3316]  eta: 23:30:49  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.5810  data: 0.0050
Test:  [ 490/3316]  eta: 23:03:13  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 28.9194  data: 0.0036
Test:  [ 490/3316]  eta: 23:08:04  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 28.9984  data: 0.0037
Test:  [ 480/3316]  eta: 23:42:04  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.5273  data: 0.0034
Test:  [ 480/3316]  eta: 23:45:44  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.7456  data: 0.0035
Test:  [ 480/3316]  eta: 23:46:48  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.7683  data: 0.0036
Test:  [ 480/3316]  eta: 23:46:55  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.7155  data: 0.0035
Test:  [ 480/3316]  eta: 23:47:49  loss: 1.1217 (1.1254)  acc1: 75.0000 (76.0395)  acc5: 93.7500 (92.0998)  time: 29.7174  data: 0.0036
Test:  [ 490/3316]  eta: 23:25:16  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.4722  data: 0.0031
Test:  [ 500/3316]  eta: 22:57:51  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 28.9282  data: 0.0028
Test:  [ 500/3316]  eta: 23:02:34  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 28.9575  data: 0.0028
Test:  [ 490/3316]  eta: 23:36:26  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.5546  data: 0.0030
Test:  [ 490/3316]  eta: 23:40:16  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.7070  data: 0.0029
Test:  [ 490/3316]  eta: 23:41:18  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.7347  data: 0.0030
Test:  [ 490/3316]  eta: 23:41:26  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.7177  data: 0.0028
Test:  [ 490/3316]  eta: 23:42:21  loss: 1.1806 (1.1298)  acc1: 68.7500 (75.9292)  acc5: 87.5000 (92.0188)  time: 29.7044  data: 0.0030
Test:  [ 500/3316]  eta: 23:20:13  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 29.4962  data: 0.0029
Test:  [ 510/3316]  eta: 22:53:23  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 29.3440  data: 0.0029
Test:  [ 510/3316]  eta: 22:57:57  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 29.3150  data: 0.0030
Test:  [ 500/3316]  eta: 23:31:48  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 29.9646  data: 0.0034
Test:  [ 500/3316]  eta: 23:35:33  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 30.0922  data: 0.0034
Test:  [ 500/3316]  eta: 23:36:36  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 30.1094  data: 0.0037
Test:  [ 500/3316]  eta: 23:36:50  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 30.1772  data: 0.0033
Test:  [ 500/3316]  eta: 23:37:38  loss: 1.0205 (1.1289)  acc1: 75.0000 (75.9481)  acc5: 93.7500 (92.0284)  time: 30.1508  data: 0.0039
Test:  [ 510/3316]  eta: 23:15:12  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 29.7628  data: 0.0033
Test:  [ 520/3316]  eta: 22:48:14  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 29.4537  data: 0.0037
Test:  [ 520/3316]  eta: 22:52:36  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 29.3748  data: 0.0032
Test:  [ 510/3316]  eta: 23:26:21  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 30.0434  data: 0.0036
Test:  [ 510/3316]  eta: 23:30:07  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 30.0999  data: 0.0035
Test:  [ 510/3316]  eta: 23:31:15  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 30.1777  data: 0.0041
Test:  [ 510/3316]  eta: 23:31:28  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 30.2293  data: 0.0038
Test:  [ 510/3316]  eta: 23:32:14  loss: 1.0532 (1.1314)  acc1: 75.0000 (75.8806)  acc5: 93.7500 (92.0132)  time: 30.1685  data: 0.0041
Test:  [ 520/3316]  eta: 23:10:04  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 29.7228  data: 0.0033
Test:  [ 530/3316]  eta: 22:43:23  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 29.2459  data: 0.0037
Test:  [ 530/3316]  eta: 22:47:44  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 29.2328  data: 0.0032
Test:  [ 520/3316]  eta: 23:21:25  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 29.8801  data: 0.0030
Test:  [ 520/3316]  eta: 23:25:54  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 30.3942  data: 0.0031
Test:  [ 520/3316]  eta: 23:27:01  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 30.4547  data: 0.0034
Test:  [ 520/3316]  eta: 23:27:14  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 30.4521  data: 0.0033
Test:  [ 520/3316]  eta: 23:27:54  loss: 1.1425 (1.1353)  acc1: 68.7500 (75.7678)  acc5: 93.7500 (91.9626)  time: 30.3925  data: 0.0033
Test:  [ 530/3316]  eta: 23:05:29  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 29.9654  data: 0.0039
Test:  [ 540/3316]  eta: 22:38:51  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 29.5978  data: 0.0046
Test:  [ 540/3316]  eta: 22:43:08  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 29.6600  data: 0.0047
Test:  [ 530/3316]  eta: 23:16:41  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 30.2746  data: 0.0047
Test:  [ 530/3316]  eta: 23:20:47  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 30.5592  data: 0.0049
Test:  [ 530/3316]  eta: 23:21:38  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 30.4333  data: 0.0047
Test:  [ 530/3316]  eta: 23:22:04  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 30.5583  data: 0.0047
Test:  [ 530/3316]  eta: 23:22:29  loss: 1.0033 (1.1351)  acc1: 75.0000 (75.7886)  acc5: 93.7500 (91.9492)  time: 30.3791  data: 0.0044
Test:  [ 540/3316]  eta: 23:00:18  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 29.9338  data: 0.0047
Test:  [ 550/3316]  eta: 22:33:54  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 29.5482  data: 0.0049
Test:  [ 550/3316]  eta: 22:38:06  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 29.5745  data: 0.0046
Test:  [ 540/3316]  eta: 23:11:39  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 30.2220  data: 0.0047
Test:  [ 540/3316]  eta: 23:15:54  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 30.1973  data: 0.0047
Test:  [ 540/3316]  eta: 23:16:37  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 29.9954  data: 0.0047
Test:  [ 540/3316]  eta: 23:17:15  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 30.2441  data: 0.0048
Test:  [ 540/3316]  eta: 23:17:28  loss: 1.0033 (1.1369)  acc1: 81.2500 (75.7856)  acc5: 93.7500 (91.8900)  time: 30.0027  data: 0.0044
Test:  [ 550/3316]  eta: 22:55:14  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 29.6563  data: 0.0037
Test:  [ 560/3316]  eta: 22:29:00  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 29.3389  data: 0.0035
Test:  [ 560/3316]  eta: 22:33:03  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 29.3067  data: 0.0034
Test:  [ 550/3316]  eta: 23:06:32  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 30.0104  data: 0.0032
Test:  [ 550/3316]  eta: 23:10:51  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 30.2371  data: 0.0029
Test:  [ 550/3316]  eta: 23:11:34  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 30.1844  data: 0.0032
Test:  [ 550/3316]  eta: 23:12:15  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 30.3513  data: 0.0032
Test:  [ 550/3316]  eta: 23:12:25  loss: 1.2863 (1.1399)  acc1: 75.0000 (75.6919)  acc5: 93.7500 (91.8671)  time: 30.2056  data: 0.0031
Test:  [ 560/3316]  eta: 22:50:13  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 29.7516  data: 0.0030
Test:  [ 570/3316]  eta: 22:24:06  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 29.3575  data: 0.0033
Test:  [ 570/3316]  eta: 22:28:08  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 29.3622  data: 0.0035
Test:  [ 560/3316]  eta: 23:01:28  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 29.9815  data: 0.0032
Test:  [ 560/3316]  eta: 23:05:44  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 30.1039  data: 0.0030
Test:  [ 560/3316]  eta: 23:06:53  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 30.3870  data: 0.0034
Test:  [ 560/3316]  eta: 23:07:57  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 30.6640  data: 0.0032
Test:  [ 560/3316]  eta: 23:07:57  loss: 1.3393 (1.1404)  acc1: 75.0000 (75.7130)  acc5: 93.7500 (91.8561)  time: 30.5398  data: 0.0031
Test:  [ 570/3316]  eta: 22:45:48  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.1564  data: 0.0041
Test:  [ 580/3316]  eta: 22:19:42  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 29.6773  data: 0.0045
Test:  [ 580/3316]  eta: 22:23:40  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 29.7265  data: 0.0049
Test:  [ 570/3316]  eta: 22:56:49  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.2603  data: 0.0056
Test:  [ 570/3316]  eta: 23:00:52  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.2194  data: 0.0051
Test:  [ 570/3316]  eta: 23:01:56  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.4459  data: 0.0058
Test:  [ 570/3316]  eta: 23:02:36  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.3563  data: 0.0053
Test:  [ 570/3316]  eta: 23:02:46  loss: 1.1721 (1.1407)  acc1: 75.0000 (75.6677)  acc5: 93.7500 (91.8564)  time: 30.5590  data: 0.0054
Test:  [ 580/3316]  eta: 22:40:30  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 29.9686  data: 0.0047
Test:  [ 590/3316]  eta: 22:14:29  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.4841  data: 0.0047
Test:  [ 590/3316]  eta: 22:18:23  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.5072  data: 0.0050
Test:  [ 580/3316]  eta: 22:51:27  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 30.0806  data: 0.0054
Test:  [ 580/3316]  eta: 22:55:40  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 30.1650  data: 0.0050
Test:  [ 580/3316]  eta: 22:56:46  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 30.1563  data: 0.0056
Test:  [ 580/3316]  eta: 22:57:24  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 29.9150  data: 0.0052
Test:  [ 580/3316]  eta: 22:57:33  loss: 1.1918 (1.1435)  acc1: 68.7500 (75.5379)  acc5: 93.7500 (91.8567)  time: 30.0127  data: 0.0054
Test:  [ 590/3316]  eta: 22:35:29  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.5987  data: 0.0038
Test:  [ 600/3316]  eta: 22:09:39  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.2056  data: 0.0034
Test:  [ 600/3316]  eta: 22:13:26  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.1975  data: 0.0033
Test:  [ 590/3316]  eta: 22:46:27  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.8673  data: 0.0031
Test:  [ 590/3316]  eta: 22:50:23  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.8928  data: 0.0029
Test:  [ 590/3316]  eta: 22:51:32  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.9857  data: 0.0030
Test:  [ 590/3316]  eta: 22:52:05  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.9245  data: 0.0030
Test:  [ 590/3316]  eta: 22:52:21  loss: 1.2307 (1.1441)  acc1: 75.0000 (75.5499)  acc5: 93.7500 (91.8570)  time: 29.9895  data: 0.0032
Test:  [ 600/3316]  eta: 22:30:14  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.6203  data: 0.0031
Test:  [ 610/3316]  eta: 22:04:28  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 29.2233  data: 0.0031
Test:  [ 610/3316]  eta: 22:08:13  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 29.2231  data: 0.0030
Test:  [ 600/3316]  eta: 22:41:14  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.9526  data: 0.0032
Test:  [ 600/3316]  eta: 22:45:13  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.9030  data: 0.0028
Test:  [ 600/3316]  eta: 22:46:24  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.9923  data: 0.0029
Test:  [ 600/3316]  eta: 22:46:47  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 29.8364  data: 0.0028
Test:  [ 600/3316]  eta: 22:47:15  loss: 1.1216 (1.1432)  acc1: 75.0000 (75.5616)  acc5: 93.7500 (91.8677)  time: 30.0565  data: 0.0032
Test:  [ 610/3316]  eta: 22:25:44  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 29.9722  data: 0.0041
Test:  [ 620/3316]  eta: 22:00:07  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 29.5548  data: 0.0040
Test:  [ 620/3316]  eta: 22:03:48  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 29.5880  data: 0.0042
Test:  [ 610/3316]  eta: 22:36:53  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 30.3906  data: 0.0051
Test:  [ 610/3316]  eta: 22:40:41  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 30.4083  data: 0.0057
Test:  [ 610/3316]  eta: 22:41:51  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 30.4412  data: 0.0049
Test:  [ 610/3316]  eta: 22:42:07  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 30.2692  data: 0.0049
Test:  [ 610/3316]  eta: 22:42:36  loss: 1.1216 (1.1461)  acc1: 75.0000 (75.4910)  acc5: 93.7500 (91.8372)  time: 30.4269  data: 0.0049
Test:  [ 620/3316]  eta: 22:20:40  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.0960  data: 0.0054
Test:  [ 630/3316]  eta: 21:55:02  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 29.6110  data: 0.0052
Test:  [ 630/3316]  eta: 21:58:33  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 29.5477  data: 0.0050
Test:  [ 620/3316]  eta: 22:31:33  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.3122  data: 0.0053
Test:  [ 620/3316]  eta: 22:35:17  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.2483  data: 0.0059
Test:  [ 620/3316]  eta: 22:36:27  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.2631  data: 0.0049
Test:  [ 620/3316]  eta: 22:36:43  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.1901  data: 0.0050
Test:  [ 620/3316]  eta: 22:37:14  loss: 1.2788 (1.1479)  acc1: 75.0000 (75.4831)  acc5: 87.5000 (91.7774)  time: 30.2355  data: 0.0048
Test:  [ 630/3316]  eta: 22:15:34  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 29.6756  data: 0.0043
Test:  [ 640/3316]  eta: 21:49:58  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 29.1240  data: 0.0042
Test:  [ 640/3316]  eta: 21:53:36  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 29.1874  data: 0.0038
Test:  [ 630/3316]  eta: 22:26:45  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 30.0046  data: 0.0034
Test:  [ 630/3316]  eta: 22:30:23  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 29.9952  data: 0.0033
Test:  [ 630/3316]  eta: 22:31:30  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 29.9974  data: 0.0033
Test:  [ 630/3316]  eta: 22:31:48  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 30.0238  data: 0.0032
Test:  [ 630/3316]  eta: 22:32:15  loss: 1.2340 (1.1480)  acc1: 75.0000 (75.4754)  acc5: 93.7500 (91.7987)  time: 30.0181  data: 0.0032
Test:  [ 650/3316]  eta: 21:44:58  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.1707  data: 0.0031
Test:  [ 640/3316]  eta: 22:10:27  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 29.6344  data: 0.0031
Test:  [ 650/3316]  eta: 21:48:42  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.4334  data: 0.0035
Test:  [ 640/3316]  eta: 22:21:39  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 30.1635  data: 0.0033
Test:  [ 640/3316]  eta: 22:25:10  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 30.1178  data: 0.0035
Test:  [ 640/3316]  eta: 22:26:18  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 30.1255  data: 0.0042
Test:  [ 640/3316]  eta: 22:26:35  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 30.1551  data: 0.0038
Test:  [ 640/3316]  eta: 22:27:00  loss: 0.9878 (1.1443)  acc1: 81.2500 (75.5948)  acc5: 93.7500 (91.8389)  time: 30.0962  data: 0.0038
Test:  [ 660/3316]  eta: 21:40:04  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 29.2840  data: 0.0036
Test:  [ 650/3316]  eta: 22:05:24  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.6700  data: 0.0030
Test:  [ 660/3316]  eta: 21:43:31  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 29.2558  data: 0.0036
Test:  [ 650/3316]  eta: 22:16:29  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.9125  data: 0.0032
Test:  [ 650/3316]  eta: 22:20:04  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.9739  data: 0.0031
Test:  [ 650/3316]  eta: 22:21:12  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 30.0121  data: 0.0038
Test:  [ 650/3316]  eta: 22:21:30  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 30.0291  data: 0.0039
Test:  [ 650/3316]  eta: 22:21:47  loss: 0.9878 (1.1441)  acc1: 81.2500 (75.6240)  acc5: 93.7500 (91.8299)  time: 29.9111  data: 0.0037
Test:  [ 670/3316]  eta: 21:35:11  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 29.3787  data: 0.0034
Test:  [ 660/3316]  eta: 22:00:19  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 29.6987  data: 0.0028
Test:  [ 670/3316]  eta: 21:38:25  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 29.1074  data: 0.0033
Test:  [ 660/3316]  eta: 22:11:41  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 30.1268  data: 0.0031
Test:  [ 660/3316]  eta: 22:15:19  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 30.3125  data: 0.0033
Test:  [ 660/3316]  eta: 22:16:26  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 30.3378  data: 0.0036
Test:  [ 660/3316]  eta: 22:16:46  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 30.3700  data: 0.0039
Test:  [ 660/3316]  eta: 22:16:58  loss: 1.0811 (1.1471)  acc1: 75.0000 (75.5768)  acc5: 87.5000 (91.7455)  time: 30.2187  data: 0.0039
Test:  [ 680/3316]  eta: 21:30:41  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 29.6858  data: 0.0041
Test:  [ 670/3316]  eta: 21:55:37  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 29.9551  data: 0.0041
Test:  [ 680/3316]  eta: 21:33:49  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 29.5332  data: 0.0047
Test:  [ 670/3316]  eta: 22:06:46  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 30.3152  data: 0.0053
Test:  [ 670/3316]  eta: 22:10:17  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 30.3616  data: 0.0047
Test:  [ 670/3316]  eta: 22:11:32  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 30.4904  data: 0.0048
Test:  [ 670/3316]  eta: 22:11:42  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 30.3841  data: 0.0047
Test:  [ 670/3316]  eta: 22:11:56  loss: 1.1064 (1.1471)  acc1: 75.0000 (75.5496)  acc5: 93.7500 (91.7381)  time: 30.3563  data: 0.0050
Test:  [ 690/3316]  eta: 21:25:50  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 29.7216  data: 0.0050
Test:  [ 680/3316]  eta: 21:50:41  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.0636  data: 0.0047
Test:  [ 690/3316]  eta: 21:28:51  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 29.6382  data: 0.0053
Test:  [ 680/3316]  eta: 22:01:35  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.0315  data: 0.0053
Test:  [ 680/3316]  eta: 22:05:07  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.0453  data: 0.0041
Test:  [ 680/3316]  eta: 22:06:23  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.1998  data: 0.0043
Test:  [ 680/3316]  eta: 22:06:27  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.0027  data: 0.0042
Test:  [ 680/3316]  eta: 22:06:41  loss: 1.2343 (1.1489)  acc1: 75.0000 (75.5507)  acc5: 93.7500 (91.7034)  time: 30.0448  data: 0.0043
Test:  [ 700/3316]  eta: 21:20:45  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 29.2755  data: 0.0039
Test:  [ 690/3316]  eta: 21:45:36  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 29.7759  data: 0.0036
Test:  [ 700/3316]  eta: 21:23:47  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 29.2754  data: 0.0038
Test:  [ 690/3316]  eta: 21:56:34  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 29.9454  data: 0.0031
Test:  [ 690/3316]  eta: 22:00:05  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 30.0501  data: 0.0030
Test:  [ 690/3316]  eta: 22:01:26  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 30.1649  data: 0.0031
Test:  [ 690/3316]  eta: 22:01:28  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 30.0733  data: 0.0033
Test:  [ 690/3316]  eta: 22:01:42  loss: 1.2343 (1.1501)  acc1: 75.0000 (75.5427)  acc5: 87.5000 (91.6697)  time: 30.0765  data: 0.0031
Test:  [ 710/3316]  eta: 21:15:55  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 29.2762  data: 0.0031
Test:  [ 700/3316]  eta: 21:40:41  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 29.7915  data: 0.0032
Test:  [ 710/3316]  eta: 21:19:01  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 29.4348  data: 0.0030
Test:  [ 700/3316]  eta: 21:51:45  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 30.2290  data: 0.0031
Test:  [ 700/3316]  eta: 21:55:27  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 30.4746  data: 0.0037
Test:  [ 700/3316]  eta: 21:56:46  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 30.5029  data: 0.0040
Test:  [ 700/3316]  eta: 21:56:47  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 30.5677  data: 0.0038
Test:  [ 700/3316]  eta: 21:56:56  loss: 1.1279 (1.1518)  acc1: 75.0000 (75.5171)  acc5: 87.5000 (91.6459)  time: 30.4506  data: 0.0039
Test:  [ 720/3316]  eta: 21:11:18  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6524  data: 0.0042
Test:  [ 710/3316]  eta: 21:35:56  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.0629  data: 0.0050
Test:  [ 720/3316]  eta: 21:14:15  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6840  data: 0.0040
Test:  [ 710/3316]  eta: 21:46:41  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.1981  data: 0.0050
Test:  [ 710/3316]  eta: 21:50:01  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.1557  data: 0.0048
Test:  [ 710/3316]  eta: 21:51:15  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.0594  data: 0.0047
Test:  [ 710/3316]  eta: 21:51:18  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.1331  data: 0.0048
Test:  [ 710/3316]  eta: 21:51:26  loss: 1.2221 (1.1530)  acc1: 75.0000 (75.5362)  acc5: 87.5000 (91.6227)  time: 30.0385  data: 0.0048
Test:  [ 730/3316]  eta: 21:06:04  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.3304  data: 0.0049
Test:  [ 720/3316]  eta: 21:30:34  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6881  data: 0.0052
Test:  [ 730/3316]  eta: 21:09:04  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.3213  data: 0.0047
Test:  [ 720/3316]  eta: 21:41:35  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.9735  data: 0.0046
Test:  [ 720/3316]  eta: 21:44:55  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.7736  data: 0.0040
Test:  [ 720/3316]  eta: 21:46:01  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6261  data: 0.0040
Test:  [ 720/3316]  eta: 21:46:07  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6921  data: 0.0038
Test:  [ 740/3316]  eta: 21:01:04  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 28.9983  data: 0.0037
Test:  [ 720/3316]  eta: 21:46:13  loss: 1.1617 (1.1521)  acc1: 75.0000 (75.5548)  acc5: 93.7500 (91.6262)  time: 29.6637  data: 0.0038
Test:  [ 730/3316]  eta: 21:25:16  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.2185  data: 0.0034
Test:  [ 740/3316]  eta: 21:03:55  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 29.0103  data: 0.0037
Test:  [ 730/3316]  eta: 21:36:13  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.7143  data: 0.0029
Test:  [ 730/3316]  eta: 21:39:35  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.8350  data: 0.0028
Test:  [ 750/3316]  eta: 20:55:53  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 29.0323  data: 0.0029
Test:  [ 730/3316]  eta: 21:40:35  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.6889  data: 0.0030
Test:  [ 730/3316]  eta: 21:40:44  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.7710  data: 0.0029
Test:  [ 730/3316]  eta: 21:40:48  loss: 1.1080 (1.1524)  acc1: 75.0000 (75.5643)  acc5: 93.7500 (91.6467)  time: 29.7093  data: 0.0027
Test:  [ 740/3316]  eta: 21:20:07  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 29.3935  data: 0.0030
Test:  [ 750/3316]  eta: 20:58:51  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 29.0959  data: 0.0030
Test:  [ 740/3316]  eta: 21:31:14  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 29.8104  data: 0.0029
Test:  [ 740/3316]  eta: 21:34:56  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 30.2250  data: 0.0030
Test:  [ 760/3316]  eta: 20:51:17  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 29.3906  data: 0.0034
Test:  [ 740/3316]  eta: 21:35:55  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 30.1644  data: 0.0036
Test:  [ 740/3316]  eta: 21:36:02  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 30.0884  data: 0.0034
Test:  [ 740/3316]  eta: 21:36:07  loss: 0.9681 (1.1499)  acc1: 81.2500 (75.6495)  acc5: 93.7500 (91.6582)  time: 30.2435  data: 0.0035
Test:  [ 750/3316]  eta: 21:15:27  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 29.9260  data: 0.0040
Test:  [ 760/3316]  eta: 20:54:08  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 29.4633  data: 0.0036
Test:  [ 750/3316]  eta: 21:26:18  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 30.1746  data: 0.0043
Test:  [ 770/3316]  eta: 20:46:12  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.4499  data: 0.0042
Test:  [ 750/3316]  eta: 21:29:44  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 30.3302  data: 0.0039
Test:  [ 750/3316]  eta: 21:30:40  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 30.2878  data: 0.0043
Test:  [ 750/3316]  eta: 21:30:43  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 30.1611  data: 0.0040
Test:  [ 750/3316]  eta: 21:30:53  loss: 0.9402 (1.1485)  acc1: 81.2500 (75.6575)  acc5: 93.7500 (91.6528)  time: 30.3619  data: 0.0043
Test:  [ 770/3316]  eta: 20:48:57  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.3534  data: 0.0040
Test:  [ 760/3316]  eta: 21:10:22  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 29.9732  data: 0.0046
Test:  [ 760/3316]  eta: 21:21:16  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 30.1268  data: 0.0045
Test:  [ 780/3316]  eta: 20:41:17  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.1700  data: 0.0037
Test:  [ 760/3316]  eta: 21:24:45  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 30.0395  data: 0.0039
Test:  [ 760/3316]  eta: 21:25:41  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 30.0276  data: 0.0037
Test:  [ 760/3316]  eta: 21:25:42  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 29.9335  data: 0.0036
Test:  [ 760/3316]  eta: 21:25:55  loss: 1.0479 (1.1490)  acc1: 75.0000 (75.6406)  acc5: 93.7500 (91.6393)  time: 30.0622  data: 0.0038
Test:  [ 780/3316]  eta: 20:44:00  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.1366  data: 0.0034
Test:  [ 770/3316]  eta: 21:05:27  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.7689  data: 0.0035
Test:  [ 770/3316]  eta: 21:16:04  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.8877  data: 0.0030
Test:  [ 790/3316]  eta: 20:36:14  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.2141  data: 0.0028
Test:  [ 770/3316]  eta: 21:19:31  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 30.0166  data: 0.0032
Test:  [ 770/3316]  eta: 21:20:23  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.9263  data: 0.0031
Test:  [ 770/3316]  eta: 21:20:24  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 30.0031  data: 0.0031
Test:  [ 770/3316]  eta: 21:20:36  loss: 1.1161 (1.1494)  acc1: 75.0000 (75.6080)  acc5: 93.7500 (91.6342)  time: 29.9821  data: 0.0030
Test:  [ 790/3316]  eta: 20:38:52  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.1865  data: 0.0029
Test:  [ 780/3316]  eta: 21:00:21  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.7545  data: 0.0030
Test:  [ 780/3316]  eta: 21:11:05  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.9260  data: 0.0041
Test:  [ 800/3316]  eta: 20:31:25  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.3061  data: 0.0043
Test:  [ 780/3316]  eta: 21:14:35  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 30.0677  data: 0.0047
Test:  [ 780/3316]  eta: 21:15:21  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.9160  data: 0.0049
Test:  [ 780/3316]  eta: 21:15:24  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.9786  data: 0.0048
Test:  [ 780/3316]  eta: 21:15:38  loss: 1.1085 (1.1500)  acc1: 75.0000 (75.6002)  acc5: 93.7500 (91.5973)  time: 29.9892  data: 0.0047
Test:  [ 800/3316]  eta: 20:34:04  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.3261  data: 0.0047
Test:  [ 790/3316]  eta: 20:55:25  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.7478  data: 0.0053
Test:  [ 790/3316]  eta: 21:05:51  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.8944  data: 0.0045
Test:  [ 810/3316]  eta: 20:26:15  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.1717  data: 0.0047
Test:  [ 790/3316]  eta: 21:09:15  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.9449  data: 0.0046
Test:  [ 810/3316]  eta: 20:28:50  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.1989  data: 0.0048
Test:  [ 790/3316]  eta: 21:09:58  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.8403  data: 0.0049
Test:  [ 790/3316]  eta: 21:10:01  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.8612  data: 0.0047
Test:  [ 790/3316]  eta: 21:10:11  loss: 1.2781 (1.1531)  acc1: 75.0000 (75.5215)  acc5: 87.5000 (91.5455)  time: 29.8418  data: 0.0048
Test:  [ 800/3316]  eta: 20:50:07  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.5327  data: 0.0054
Test:  [ 800/3316]  eta: 21:00:20  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.3901  data: 0.0031
Test:  [ 820/3316]  eta: 20:20:58  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 28.7086  data: 0.0032
Test:  [ 800/3316]  eta: 21:03:51  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.4963  data: 0.0027
Test:  [ 820/3316]  eta: 20:23:35  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 28.7501  data: 0.0028
Test:  [ 800/3316]  eta: 21:04:36  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.5185  data: 0.0027
Test:  [ 800/3316]  eta: 21:04:38  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.4936  data: 0.0028
Test:  [ 800/3316]  eta: 21:04:47  loss: 1.4082 (1.1554)  acc1: 68.7500 (75.4448)  acc5: 87.5000 (91.5106)  time: 29.4311  data: 0.0029
Test:  [ 810/3316]  eta: 20:44:53  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.2335  data: 0.0029
Test:  [ 810/3316]  eta: 20:55:10  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.4215  data: 0.0028
Test:  [ 830/3316]  eta: 20:15:56  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 28.8321  data: 0.0031
Test:  [ 830/3316]  eta: 20:18:24  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 28.8013  data: 0.0027
Test:  [ 810/3316]  eta: 20:58:34  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.5363  data: 0.0028
Test:  [ 810/3316]  eta: 20:59:21  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.6222  data: 0.0026
Test:  [ 810/3316]  eta: 20:59:23  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.6086  data: 0.0028
Test:  [ 810/3316]  eta: 20:59:31  loss: 1.1810 (1.1555)  acc1: 68.7500 (75.4239)  acc5: 87.5000 (91.4997)  time: 29.5745  data: 0.0027
Test:  [ 820/3316]  eta: 20:39:39  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.2937  data: 0.0029
Test:  [ 820/3316]  eta: 20:50:07  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.8561  data: 0.0041
Test:  [ 840/3316]  eta: 20:11:01  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.1743  data: 0.0042
Test:  [ 840/3316]  eta: 20:13:27  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.0901  data: 0.0043
Test:  [ 820/3316]  eta: 20:53:26  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.7808  data: 0.0043
Test:  [ 820/3316]  eta: 20:54:14  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.8443  data: 0.0045
Test:  [ 820/3316]  eta: 20:54:17  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.8653  data: 0.0046
Test:  [ 820/3316]  eta: 20:54:28  loss: 1.1810 (1.1571)  acc1: 68.7500 (75.3806)  acc5: 93.7500 (91.4662)  time: 29.8978  data: 0.0047
Test:  [ 830/3316]  eta: 20:34:41  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 29.5379  data: 0.0044
Test:  [ 830/3316]  eta: 20:45:00  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 29.9022  data: 0.0046
Test:  [ 850/3316]  eta: 20:06:00  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.1879  data: 0.0043
Test:  [ 850/3316]  eta: 20:08:28  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.2717  data: 0.0048
Test:  [ 830/3316]  eta: 20:48:23  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 30.0003  data: 0.0044
Test:  [ 830/3316]  eta: 20:49:06  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 29.9590  data: 0.0047
Test:  [ 830/3316]  eta: 20:49:08  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 29.9605  data: 0.0049
Test:  [ 830/3316]  eta: 20:49:19  loss: 1.1956 (1.1564)  acc1: 75.0000 (75.4137)  acc5: 93.7500 (91.4786)  time: 30.0051  data: 0.0049
Test:  [ 840/3316]  eta: 20:29:39  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.7375  data: 0.0044
Test:  [ 860/3316]  eta: 20:00:58  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 29.0676  data: 0.0031
Test:  [ 840/3316]  eta: 20:39:47  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.7319  data: 0.0034
Test:  [ 860/3316]  eta: 20:03:23  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 29.1180  data: 0.0032
Test:  [ 840/3316]  eta: 20:43:08  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.8585  data: 0.0030
Test:  [ 840/3316]  eta: 20:43:47  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.7475  data: 0.0030
Test:  [ 840/3316]  eta: 20:43:53  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.8009  data: 0.0033
Test:  [ 840/3316]  eta: 20:44:05  loss: 1.1103 (1.1569)  acc1: 75.0000 (75.4162)  acc5: 93.7500 (91.4611)  time: 29.8119  data: 0.0030
Test:  [ 850/3316]  eta: 20:24:30  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.5548  data: 0.0029
Test:  [ 870/3316]  eta: 19:56:11  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 29.3034  data: 0.0032
Test:  [ 850/3316]  eta: 20:34:50  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.8957  data: 0.0028
Test:  [ 870/3316]  eta: 19:58:34  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 29.2896  data: 0.0028
Test:  [ 850/3316]  eta: 20:38:05  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.8607  data: 0.0031
Test:  [ 850/3316]  eta: 20:38:48  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.8896  data: 0.0035
Test:  [ 850/3316]  eta: 20:38:56  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 29.9815  data: 0.0030
Test:  [ 860/3316]  eta: 20:19:39  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 29.7311  data: 0.0028
Test:  [ 850/3316]  eta: 20:39:08  loss: 1.1103 (1.1570)  acc1: 81.2500 (75.4333)  acc5: 87.5000 (91.4586)  time: 30.0217  data: 0.0030
Test:  [ 880/3316]  eta: 19:51:19  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.4882  data: 0.0048
Test:  [ 860/3316]  eta: 20:29:48  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 30.0687  data: 0.0040
Test:  [ 880/3316]  eta: 19:53:43  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.5400  data: 0.0036
Test:  [ 860/3316]  eta: 20:32:55  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 29.9513  data: 0.0044
Test:  [ 860/3316]  eta: 20:33:47  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 30.1843  data: 0.0051
Test:  [ 860/3316]  eta: 20:33:50  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 30.1382  data: 0.0042
Test:  [ 870/3316]  eta: 20:14:44  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 29.9713  data: 0.0040
Test:  [ 860/3316]  eta: 20:34:05  loss: 1.2109 (1.1588)  acc1: 75.0000 (75.3775)  acc5: 93.7500 (91.4562)  time: 30.2025  data: 0.0044
Test:  [ 890/3316]  eta: 19:46:18  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.2265  data: 0.0051
Test:  [ 870/3316]  eta: 20:24:39  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 29.8675  data: 0.0044
Test:  [ 890/3316]  eta: 19:48:44  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.3740  data: 0.0038
Test:  [ 870/3316]  eta: 20:27:45  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 29.8067  data: 0.0044
Test:  [ 870/3316]  eta: 20:28:43  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 30.0962  data: 0.0048
Test:  [ 880/3316]  eta: 20:09:50  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.9132  data: 0.0042
Test:  [ 870/3316]  eta: 20:28:45  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 30.0082  data: 0.0045
Test:  [ 870/3316]  eta: 20:29:02  loss: 1.2945 (1.1597)  acc1: 75.0000 (75.3803)  acc5: 93.7500 (91.4036)  time: 30.0905  data: 0.0045
Test:  [ 900/3316]  eta: 19:41:14  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.0025  data: 0.0035
Test:  [ 880/3316]  eta: 20:19:23  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.6164  data: 0.0032
Test:  [ 900/3316]  eta: 19:43:37  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.0826  data: 0.0033
Test:  [ 880/3316]  eta: 20:22:22  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.5552  data: 0.0031
Test:  [ 890/3316]  eta: 20:04:34  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.5250  data: 0.0031
Test:  [ 880/3316]  eta: 20:23:26  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.7508  data: 0.0032
Test:  [ 880/3316]  eta: 20:23:27  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.8255  data: 0.0031
Test:  [ 880/3316]  eta: 20:23:40  loss: 1.1831 (1.1605)  acc1: 75.0000 (75.3831)  acc5: 87.5000 (91.3805)  time: 29.7555  data: 0.0031
Test:  [ 910/3316]  eta: 19:36:14  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.0221  data: 0.0028
Test:  [ 890/3316]  eta: 20:14:12  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.5636  data: 0.0032
Test:  [ 910/3316]  eta: 19:38:41  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.1073  data: 0.0033
Test:  [ 890/3316]  eta: 20:17:12  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.5620  data: 0.0037
Test:  [ 900/3316]  eta: 19:59:40  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.5383  data: 0.0037
Test:  [ 890/3316]  eta: 20:18:23  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.7933  data: 0.0039
Test:  [ 890/3316]  eta: 20:18:26  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.8823  data: 0.0038
Test:  [ 890/3316]  eta: 20:18:38  loss: 1.0779 (1.1600)  acc1: 75.0000 (75.3928)  acc5: 93.7500 (91.3861)  time: 29.7759  data: 0.0039
Test:  [ 920/3316]  eta: 19:31:14  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 29.0814  data: 0.0043
Test:  [ 900/3316]  eta: 20:08:55  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.5167  data: 0.0045
Test:  [ 920/3316]  eta: 19:33:38  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 29.1708  data: 0.0043
Test:  [ 900/3316]  eta: 20:11:51  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.5843  data: 0.0044
Test:  [ 910/3316]  eta: 19:54:27  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.5788  data: 0.0044
Test:  [ 900/3316]  eta: 20:13:04  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.7773  data: 0.0045
Test:  [ 900/3316]  eta: 20:13:08  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.8433  data: 0.0045
Test:  [ 900/3316]  eta: 20:13:17  loss: 1.1170 (1.1612)  acc1: 68.7500 (75.3330)  acc5: 87.5000 (91.3430)  time: 29.7532  data: 0.0046
Test:  [ 930/3316]  eta: 19:26:02  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 28.8268  data: 0.0043
Test:  [ 910/3316]  eta: 20:03:33  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.2974  data: 0.0041
Test:  [ 930/3316]  eta: 19:28:32  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 28.9835  data: 0.0037
Test:  [ 910/3316]  eta: 20:06:32  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.3905  data: 0.0032
Test:  [ 920/3316]  eta: 19:49:16  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 29.2486  data: 0.0034
Test:  [ 910/3316]  eta: 20:07:51  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.5708  data: 0.0033
Test:  [ 910/3316]  eta: 20:07:56  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.6142  data: 0.0035
Test:  [ 910/3316]  eta: 20:08:06  loss: 1.2520 (1.1614)  acc1: 68.7500 (75.3293)  acc5: 87.5000 (91.3488)  time: 29.5785  data: 0.0034
Test:  [ 940/3316]  eta: 19:21:10  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 28.9816  data: 0.0027
Test:  [ 920/3316]  eta: 19:58:33  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 29.6136  data: 0.0030
Test:  [ 940/3316]  eta: 19:23:42  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.2210  data: 0.0030
Test:  [ 920/3316]  eta: 20:01:38  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 29.8760  data: 0.0028
Test:  [ 930/3316]  eta: 19:44:23  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.6329  data: 0.0032
Test:  [ 920/3316]  eta: 20:02:56  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 30.0248  data: 0.0035
Test:  [ 920/3316]  eta: 20:03:00  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 30.0230  data: 0.0038
Test:  [ 920/3316]  eta: 20:03:10  loss: 1.0222 (1.1595)  acc1: 81.2500 (75.4072)  acc5: 93.7500 (91.3681)  time: 30.0528  data: 0.0030
Test:  [ 950/3316]  eta: 19:16:14  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.3059  data: 0.0041
Test:  [ 930/3316]  eta: 19:53:25  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.8601  data: 0.0044
Test:  [ 950/3316]  eta: 19:18:45  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.3941  data: 0.0044
Test:  [ 930/3316]  eta: 19:56:17  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.8124  data: 0.0038
Test:  [ 940/3316]  eta: 19:39:10  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.5911  data: 0.0043
Test:  [ 930/3316]  eta: 19:57:36  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.8825  data: 0.0045
Test:  [ 930/3316]  eta: 19:57:44  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.9355  data: 0.0048
Test:  [ 930/3316]  eta: 19:57:53  loss: 1.0536 (1.1606)  acc1: 81.2500 (75.3894)  acc5: 87.5000 (91.3332)  time: 29.9279  data: 0.0039
Test:  [ 960/3316]  eta: 19:11:16  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.1710  data: 0.0042
Test:  [ 940/3316]  eta: 19:48:20  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.7636  data: 0.0043
Test:  [ 960/3316]  eta: 19:13:51  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.3218  data: 0.0042
Test:  [ 940/3316]  eta: 19:51:16  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.6813  data: 0.0037
Test:  [ 950/3316]  eta: 19:34:10  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.4371  data: 0.0043
Test:  [ 940/3316]  eta: 19:52:31  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.6814  data: 0.0037
Test:  [ 940/3316]  eta: 19:52:42  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.8054  data: 0.0039
Test:  [ 940/3316]  eta: 19:52:46  loss: 1.2431 (1.1610)  acc1: 75.0000 (75.3653)  acc5: 87.5000 (91.3390)  time: 29.7104  data: 0.0037
Test:  [ 970/3316]  eta: 19:06:11  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 28.9761  data: 0.0028
Test:  [ 970/3316]  eta: 19:08:45  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.1445  data: 0.0029
Test:  [ 950/3316]  eta: 19:43:02  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.5495  data: 0.0030
Test:  [ 950/3316]  eta: 19:46:00  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.7844  data: 0.0029
Test:  [ 960/3316]  eta: 19:28:55  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.3668  data: 0.0032
Test:  [ 950/3316]  eta: 19:47:11  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.6625  data: 0.0028
Test:  [ 950/3316]  eta: 19:47:23  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.7384  data: 0.0029
Test:  [ 950/3316]  eta: 19:47:26  loss: 1.0209 (1.1595)  acc1: 75.0000 (75.3352)  acc5: 93.7500 (91.3906)  time: 29.6347  data: 0.0029
Test:  [ 980/3316]  eta: 19:01:17  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.0621  data: 0.0033
Test:  [ 980/3316]  eta: 19:03:55  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.2264  data: 0.0040
Test:  [ 960/3316]  eta: 19:38:07  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.7392  data: 0.0041
Test:  [ 960/3316]  eta: 19:41:07  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.9449  data: 0.0045
Test:  [ 970/3316]  eta: 19:24:07  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.6154  data: 0.0048
Test:  [ 960/3316]  eta: 19:42:16  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.8444  data: 0.0045
Test:  [ 960/3316]  eta: 19:42:27  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.8646  data: 0.0047
Test:  [ 960/3316]  eta: 19:42:35  loss: 0.9728 (1.1592)  acc1: 75.0000 (75.3642)  acc5: 93.7500 (91.4087)  time: 29.9348  data: 0.0046
Test:  [ 990/3316]  eta: 18:56:20  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 29.2089  data: 0.0042
Test:  [ 990/3316]  eta: 18:58:55  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 29.3162  data: 0.0045
Test:  [ 970/3316]  eta: 19:32:56  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.8855  data: 0.0041
Test:  [ 970/3316]  eta: 19:35:53  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.9561  data: 0.0045
Test:  [ 980/3316]  eta: 19:18:57  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.7199  data: 0.0049
Test:  [ 970/3316]  eta: 19:37:02  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.9648  data: 0.0046
Test:  [ 970/3316]  eta: 19:37:12  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 29.9379  data: 0.0047
Test:  [ 970/3316]  eta: 19:37:23  loss: 0.9912 (1.1594)  acc1: 81.2500 (75.3540)  acc5: 93.7500 (91.4135)  time: 30.0983  data: 0.0047
Test:  [1000/3316]  eta: 18:51:22  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.1065  data: 0.0036
Test:  [1000/3316]  eta: 18:54:00  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.2050  data: 0.0033
Test:  [ 980/3316]  eta: 19:27:53  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.7244  data: 0.0029
Test:  [ 990/3316]  eta: 19:14:02  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 29.5776  data: 0.0030
Test:  [ 980/3316]  eta: 19:30:59  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.9347  data: 0.0030
Test:  [ 980/3316]  eta: 19:32:04  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.9020  data: 0.0029
Test:  [ 980/3316]  eta: 19:32:11  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.8238  data: 0.0029
Test:  [ 980/3316]  eta: 19:32:25  loss: 1.1605 (1.1597)  acc1: 75.0000 (75.3377)  acc5: 93.7500 (91.4437)  time: 29.9552  data: 0.0029
Test:  [1010/3316]  eta: 18:46:46  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.5875  data: 0.0029
Test:  [1010/3316]  eta: 18:49:16  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.5819  data: 0.0033
Test:  [ 990/3316]  eta: 19:22:59  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 30.0663  data: 0.0035
Test:  [1000/3316]  eta: 19:09:09  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.9279  data: 0.0050
Test:  [ 990/3316]  eta: 19:25:56  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 30.1815  data: 0.0041
Test:  [ 990/3316]  eta: 19:27:04  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 30.1747  data: 0.0048
Test:  [ 990/3316]  eta: 19:27:09  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 30.1004  data: 0.0050
Test:  [ 990/3316]  eta: 19:27:25  loss: 1.1699 (1.1594)  acc1: 75.0000 (75.3469)  acc5: 93.7500 (91.4543)  time: 30.1941  data: 0.0040
Test:  [1020/3316]  eta: 18:41:42  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.4471  data: 0.0041
Test:  [1020/3316]  eta: 18:44:11  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.3493  data: 0.0041
Test:  [1000/3316]  eta: 19:17:44  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.7886  data: 0.0038
Test:  [1010/3316]  eta: 19:04:05  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.7383  data: 0.0052
Test:  [1000/3316]  eta: 19:20:48  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.8902  data: 0.0042
Test:  [1000/3316]  eta: 19:21:58  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 30.0139  data: 0.0049
Test:  [1000/3316]  eta: 19:22:00  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 29.9299  data: 0.0052
Test:  [1000/3316]  eta: 19:22:19  loss: 1.1531 (1.1590)  acc1: 81.2500 (75.3684)  acc5: 93.7500 (91.4585)  time: 30.0475  data: 0.0043
Test:  [1030/3316]  eta: 18:36:49  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.0589  data: 0.0040
Test:  [1030/3316]  eta: 18:39:12  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.0170  data: 0.0036
Test:  [1010/3316]  eta: 19:12:35  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.4685  data: 0.0035
Test:  [1020/3316]  eta: 18:58:57  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.3883  data: 0.0033
Test:  [1010/3316]  eta: 19:15:36  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.6742  data: 0.0032
Test:  [1010/3316]  eta: 19:16:46  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.6311  data: 0.0033
Test:  [1010/3316]  eta: 19:16:46  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.7705  data: 0.0031
Test:  [1010/3316]  eta: 19:17:06  loss: 1.1531 (1.1584)  acc1: 75.0000 (75.3709)  acc5: 93.7500 (91.4812)  time: 29.7509  data: 0.0034
Test:  [1040/3316]  eta: 18:31:42  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 28.9977  data: 0.0027
Test:  [1040/3316]  eta: 18:34:05  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 28.9685  data: 0.0029
Test:  [1020/3316]  eta: 19:07:25  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.5727  data: 0.0030
Test:  [1030/3316]  eta: 18:53:56  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.4461  data: 0.0035
Test:  [1020/3316]  eta: 19:10:35  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.8086  data: 0.0033
Test:  [1020/3316]  eta: 19:11:44  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.7929  data: 0.0043
Test:  [1020/3316]  eta: 19:11:45  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.8769  data: 0.0036
Test:  [1020/3316]  eta: 19:12:05  loss: 0.9783 (1.1567)  acc1: 81.2500 (75.4224)  acc5: 93.7500 (91.4912)  time: 29.8426  data: 0.0039
Test:  [1050/3316]  eta: 18:26:55  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.1354  data: 0.0040
Test:  [1050/3316]  eta: 18:29:17  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.2021  data: 0.0045
Test:  [1030/3316]  eta: 19:02:23  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.7218  data: 0.0041
Test:  [1040/3316]  eta: 18:48:51  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.5382  data: 0.0044
Test:  [1030/3316]  eta: 19:05:29  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.9256  data: 0.0042
Test:  [1030/3316]  eta: 19:06:35  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.9098  data: 0.0050
Test:  [1030/3316]  eta: 19:06:37  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.9306  data: 0.0039
Test:  [1030/3316]  eta: 19:06:54  loss: 0.9633 (1.1563)  acc1: 81.2500 (75.4365)  acc5: 93.7500 (91.4949)  time: 29.8910  data: 0.0042
Test:  [1060/3316]  eta: 18:21:52  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 29.2224  data: 0.0044
Test:  [1060/3316]  eta: 18:24:10  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 29.1894  data: 0.0047
Test:  [1040/3316]  eta: 18:57:07  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.5621  data: 0.0041
Test:  [1050/3316]  eta: 18:43:39  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.2561  data: 0.0038
Test:  [1040/3316]  eta: 19:00:09  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.4909  data: 0.0037
Test:  [1040/3316]  eta: 19:01:23  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.6552  data: 0.0036
Test:  [1040/3316]  eta: 19:01:25  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.6793  data: 0.0032
Test:  [1040/3316]  eta: 19:01:41  loss: 1.0922 (1.1564)  acc1: 75.0000 (75.4203)  acc5: 93.7500 (91.4745)  time: 29.6162  data: 0.0034
Test:  [1070/3316]  eta: 18:16:51  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 28.8848  data: 0.0032
Test:  [1070/3316]  eta: 18:19:12  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 28.9331  data: 0.0030
Test:  [1050/3316]  eta: 18:52:08  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.6155  data: 0.0028
Test:  [1060/3316]  eta: 18:38:40  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 29.3694  data: 0.0030
Test:  [1050/3316]  eta: 18:55:10  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.6688  data: 0.0029
Test:  [1050/3316]  eta: 18:56:29  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 29.9989  data: 0.0035
Test:  [1050/3316]  eta: 18:56:34  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 30.0632  data: 0.0035
Test:  [1050/3316]  eta: 18:56:49  loss: 1.1465 (1.1563)  acc1: 75.0000 (75.4103)  acc5: 93.7500 (91.4962)  time: 30.0185  data: 0.0038
Test:  [1080/3316]  eta: 18:12:03  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.2506  data: 0.0039
Test:  [1080/3316]  eta: 18:14:23  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.3595  data: 0.0045
Test:  [1060/3316]  eta: 18:47:15  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 30.1604  data: 0.0044
Test:  [1070/3316]  eta: 18:33:46  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 29.8134  data: 0.0049
Test:  [1060/3316]  eta: 18:49:58  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 29.8443  data: 0.0048
Test:  [1060/3316]  eta: 18:51:22  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 30.1091  data: 0.0050
Test:  [1060/3316]  eta: 18:51:25  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 30.1318  data: 0.0049
Test:  [1060/3316]  eta: 18:51:43  loss: 1.1637 (1.1567)  acc1: 68.7500 (75.3888)  acc5: 93.7500 (91.4939)  time: 30.1823  data: 0.0050
Test:  [1090/3316]  eta: 18:07:06  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.3259  data: 0.0047
Test:  [1090/3316]  eta: 18:09:26  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.3978  data: 0.0051
Test:  [1070/3316]  eta: 18:42:13  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 30.0957  data: 0.0046
Test:  [1080/3316]  eta: 18:28:42  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.6954  data: 0.0048
Test:  [1070/3316]  eta: 18:44:51  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 29.6119  data: 0.0048
Test:  [1070/3316]  eta: 18:46:17  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 29.8456  data: 0.0043
Test:  [1070/3316]  eta: 18:46:20  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 29.8188  data: 0.0043
Test:  [1070/3316]  eta: 18:46:39  loss: 1.1322 (1.1559)  acc1: 68.7500 (75.3676)  acc5: 93.7500 (91.5324)  time: 29.9216  data: 0.0042
Test:  [1100/3316]  eta: 18:02:08  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.0669  data: 0.0038
Test:  [1100/3316]  eta: 18:04:23  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.0518  data: 0.0037
Test:  [1080/3316]  eta: 18:37:04  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.6988  data: 0.0032
Test:  [1090/3316]  eta: 18:23:35  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.3657  data: 0.0029
Test:  [1080/3316]  eta: 18:39:41  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.6495  data: 0.0031
Test:  [1080/3316]  eta: 18:41:01  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.6160  data: 0.0030
Test:  [1080/3316]  eta: 18:41:05  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.6528  data: 0.0030
Test:  [1080/3316]  eta: 18:41:27  loss: 1.1322 (1.1581)  acc1: 68.7500 (75.3122)  acc5: 93.7500 (91.5067)  time: 29.7556  data: 0.0031
Test:  [1110/3316]  eta: 17:57:08  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 29.0081  data: 0.0029
Test:  [1110/3316]  eta: 17:59:23  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 28.9733  data: 0.0034
Test:  [1090/3316]  eta: 18:32:01  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.6596  data: 0.0033
Test:  [1100/3316]  eta: 18:18:39  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.5451  data: 0.0029
Test:  [1090/3316]  eta: 18:34:39  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.7970  data: 0.0035
Test:  [1090/3316]  eta: 18:35:59  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.6981  data: 0.0035
Test:  [1090/3316]  eta: 18:36:03  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.7045  data: 0.0036
Test:  [1090/3316]  eta: 18:36:28  loss: 1.1641 (1.1573)  acc1: 75.0000 (75.3552)  acc5: 93.7500 (91.5158)  time: 29.8758  data: 0.0034
Test:  [1120/3316]  eta: 17:52:12  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 29.0464  data: 0.0033
Test:  [1120/3316]  eta: 17:54:20  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 28.9633  data: 0.0035
Test:  [1100/3316]  eta: 18:26:48  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.5606  data: 0.0031
Test:  [1110/3316]  eta: 18:13:30  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 29.4896  data: 0.0030
Test:  [1100/3316]  eta: 18:29:21  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.5712  data: 0.0034
Test:  [1100/3316]  eta: 18:30:47  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.7829  data: 0.0033
Test:  [1100/3316]  eta: 18:30:48  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.6910  data: 0.0035
Test:  [1130/3316]  eta: 17:47:08  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 28.9387  data: 0.0032
Test:  [1100/3316]  eta: 18:31:16  loss: 1.1560 (1.1567)  acc1: 81.2500 (75.3803)  acc5: 93.7500 (91.5191)  time: 29.8692  data: 0.0032
Test:  [1130/3316]  eta: 17:49:14  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 28.7907  data: 0.0030
Test:  [1110/3316]  eta: 18:21:52  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 29.7316  data: 0.0027
Test:  [1120/3316]  eta: 18:08:43  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 29.7123  data: 0.0034
Test:  [1110/3316]  eta: 18:24:22  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 29.6352  data: 0.0044
Test:  [1110/3316]  eta: 18:25:55  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 30.0180  data: 0.0037
Test:  [1110/3316]  eta: 18:25:55  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 29.9389  data: 0.0040
Test:  [1140/3316]  eta: 17:42:22  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 29.1963  data: 0.0044
Test:  [1110/3316]  eta: 18:26:23  loss: 1.1527 (1.1554)  acc1: 81.2500 (75.4219)  acc5: 93.7500 (91.5279)  time: 30.0218  data: 0.0043
Test:  [1140/3316]  eta: 17:44:25  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 29.1560  data: 0.0047
Test:  [1120/3316]  eta: 18:16:45  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 29.8750  data: 0.0047
Test:  [1130/3316]  eta: 18:03:37  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.7983  data: 0.0047
Test:  [1120/3316]  eta: 18:19:08  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 29.7391  data: 0.0052
Test:  [1150/3316]  eta: 17:37:22  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 29.2813  data: 0.0049
Test:  [1120/3316]  eta: 18:20:45  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 30.0540  data: 0.0048
Test:  [1120/3316]  eta: 18:20:47  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 30.1077  data: 0.0046
Test:  [1120/3316]  eta: 18:21:13  loss: 1.1336 (1.1557)  acc1: 75.0000 (75.4014)  acc5: 93.7500 (91.5366)  time: 30.0601  data: 0.0050
Test:  [1150/3316]  eta: 17:39:21  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 29.1984  data: 0.0051
Test:  [1130/3316]  eta: 18:11:41  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.6758  data: 0.0048
Test:  [1140/3316]  eta: 17:58:36  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 29.4381  data: 0.0043
Test:  [1130/3316]  eta: 18:13:58  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.4211  data: 0.0038
Test:  [1160/3316]  eta: 17:32:28  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 29.0625  data: 0.0031
Test:  [1130/3316]  eta: 18:15:43  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.7907  data: 0.0037
Test:  [1130/3316]  eta: 18:15:45  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.8558  data: 0.0035
Test:  [1130/3316]  eta: 18:16:10  loss: 1.1155 (1.1550)  acc1: 68.7500 (75.4034)  acc5: 93.7500 (91.5506)  time: 29.7913  data: 0.0035
Test:  [1160/3316]  eta: 17:34:24  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 28.9707  data: 0.0032
Test:  [1140/3316]  eta: 18:06:37  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 29.7340  data: 0.0030
Test:  [1150/3316]  eta: 17:53:28  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 29.3597  data: 0.0031
Test:  [1140/3316]  eta: 18:08:53  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 29.6387  data: 0.0030
Test:  [1170/3316]  eta: 17:27:37  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 29.2986  data: 0.0029
Test:  [1140/3316]  eta: 18:10:47  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 30.1538  data: 0.0042
Test:  [1140/3316]  eta: 18:10:48  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 30.1485  data: 0.0042
Test:  [1140/3316]  eta: 18:11:15  loss: 1.0009 (1.1544)  acc1: 75.0000 (75.4218)  acc5: 93.7500 (91.5535)  time: 30.1742  data: 0.0038
Test:  [1170/3316]  eta: 17:29:38  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 29.4400  data: 0.0040
Test:  [1150/3316]  eta: 18:01:47  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 30.1045  data: 0.0043
Test:  [1160/3316]  eta: 17:48:50  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 29.9832  data: 0.0039
Test:  [1150/3316]  eta: 18:04:13  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 30.4414  data: 0.0041
Test:  [1180/3316]  eta: 17:23:10  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 30.0606  data: 0.0039
Test:  [1150/3316]  eta: 18:06:22  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 31.1481  data: 0.0051
Test:  [1150/3316]  eta: 18:06:24  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 31.1627  data: 0.0053
Test:  [1150/3316]  eta: 18:06:51  loss: 1.1409 (1.1550)  acc1: 75.0000 (75.3910)  acc5: 87.5000 (91.5454)  time: 31.2265  data: 0.0048
Test:  [1180/3316]  eta: 17:25:20  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 30.5312  data: 0.0049
Test:  [1160/3316]  eta: 17:57:25  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 31.2450  data: 0.0047
Test:  [1170/3316]  eta: 17:44:27  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 31.1845  data: 0.0045
Test:  [1160/3316]  eta: 17:59:32  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 31.1027  data: 0.0044
Test:  [1190/3316]  eta: 17:18:30  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 30.3922  data: 0.0042
Test:  [1160/3316]  eta: 18:01:24  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 31.1146  data: 0.0043
Test:  [1160/3316]  eta: 18:01:28  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 31.1911  data: 0.0041
Test:  [1160/3316]  eta: 18:01:49  loss: 1.2494 (1.1559)  acc1: 68.7500 (75.3661)  acc5: 87.5000 (91.5375)  time: 31.0560  data: 0.0041
Test:  [1190/3316]  eta: 17:20:20  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 30.1555  data: 0.0039
Test:  [1170/3316]  eta: 17:52:12  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 30.6445  data: 0.0033
Test:  [1180/3316]  eta: 17:39:17  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 30.3543  data: 0.0034
Test:  [1170/3316]  eta: 17:54:15  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 30.1166  data: 0.0030
Test:  [1200/3316]  eta: 17:13:27  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.4073  data: 0.0033
Test:  [1170/3316]  eta: 17:56:26  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 30.2651  data: 0.0032
Test:  [1170/3316]  eta: 17:56:30  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 30.3112  data: 0.0032
Test:  [1170/3316]  eta: 17:56:50  loss: 1.0665 (1.1544)  acc1: 75.0000 (75.3896)  acc5: 93.7500 (91.5617)  time: 30.1330  data: 0.0030
Test:  [1200/3316]  eta: 17:15:29  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.2614  data: 0.0036
Test:  [1180/3316]  eta: 17:47:21  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 29.8673  data: 0.0043
Test:  [1190/3316]  eta: 17:34:29  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.6947  data: 0.0045
Test:  [1180/3316]  eta: 17:49:16  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 29.6249  data: 0.0050
Test:  [1210/3316]  eta: 17:08:43  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.2903  data: 0.0046
Test:  [1180/3316]  eta: 17:51:19  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 30.0090  data: 0.0046
Test:  [1180/3316]  eta: 17:51:21  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 29.9651  data: 0.0049
Test:  [1180/3316]  eta: 17:51:40  loss: 0.9335 (1.1549)  acc1: 81.2500 (75.3969)  acc5: 93.7500 (91.5485)  time: 29.9267  data: 0.0048
Test:  [1210/3316]  eta: 17:10:27  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.2059  data: 0.0050
Test:  [1190/3316]  eta: 17:42:08  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.8317  data: 0.0048
Test:  [1200/3316]  eta: 17:29:18  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.6490  data: 0.0048
Test:  [1190/3316]  eta: 17:43:57  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.5635  data: 0.0051
Test:  [1220/3316]  eta: 17:03:37  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.1850  data: 0.0046
Test:  [1190/3316]  eta: 17:46:07  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.6077  data: 0.0047
Test:  [1190/3316]  eta: 17:46:10  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.5744  data: 0.0047
Test:  [1220/3316]  eta: 17:05:25  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 28.8748  data: 0.0041
Test:  [1190/3316]  eta: 17:46:27  loss: 0.9153 (1.1530)  acc1: 75.0000 (75.4513)  acc5: 93.7500 (91.5565)  time: 29.5222  data: 0.0048
Test:  [1200/3316]  eta: 17:37:03  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.4546  data: 0.0035
Test:  [1210/3316]  eta: 17:24:14  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.2012  data: 0.0030
Test:  [1200/3316]  eta: 17:38:51  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.3393  data: 0.0030
Test:  [1230/3316]  eta: 16:58:40  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 28.8144  data: 0.0027
Test:  [1230/3316]  eta: 17:00:25  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 28.9132  data: 0.0027
Test:  [1200/3316]  eta: 17:41:00  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.6280  data: 0.0029
Test:  [1200/3316]  eta: 17:41:03  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.6458  data: 0.0028
Test:  [1200/3316]  eta: 17:41:18  loss: 0.9073 (1.1538)  acc1: 75.0000 (75.4371)  acc5: 93.7500 (91.5539)  time: 29.5420  data: 0.0031
Test:  [1210/3316]  eta: 17:31:55  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.6128  data: 0.0033
Test:  [1220/3316]  eta: 17:19:10  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.3595  data: 0.0030
Test:  [1210/3316]  eta: 17:33:41  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.5650  data: 0.0033
Test:  [1240/3316]  eta: 16:53:41  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 28.9944  data: 0.0029
Test:  [1240/3316]  eta: 16:55:23  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 28.8945  data: 0.0033
Test:  [1210/3316]  eta: 17:35:51  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.6972  data: 0.0035
Test:  [1210/3316]  eta: 17:35:53  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.6611  data: 0.0037
Test:  [1210/3316]  eta: 17:36:10  loss: 1.1117 (1.1531)  acc1: 75.0000 (75.4697)  acc5: 93.7500 (91.5669)  time: 29.6599  data: 0.0034
Test:  [1220/3316]  eta: 17:26:47  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.5095  data: 0.0030
Test:  [1230/3316]  eta: 17:14:06  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.3630  data: 0.0029
Test:  [1250/3316]  eta: 16:48:45  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.0233  data: 0.0033
Test:  [1220/3316]  eta: 17:28:35  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.5605  data: 0.0030
Test:  [1250/3316]  eta: 16:50:25  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 28.9695  data: 0.0036
Test:  [1220/3316]  eta: 17:30:46  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.7128  data: 0.0035
Test:  [1220/3316]  eta: 17:30:47  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.6693  data: 0.0037
Test:  [1220/3316]  eta: 17:31:04  loss: 1.1117 (1.1528)  acc1: 75.0000 (75.4812)  acc5: 93.7500 (91.5592)  time: 29.7242  data: 0.0032
Test:  [1230/3316]  eta: 17:21:36  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.3647  data: 0.0027
Test:  [1240/3316]  eta: 17:08:57  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.2244  data: 0.0027
Test:  [1260/3316]  eta: 16:43:48  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.0713  data: 0.0029
Test:  [1230/3316]  eta: 17:23:28  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.6580  data: 0.0027
Test:  [1260/3316]  eta: 16:45:25  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.0154  data: 0.0036
Test:  [1230/3316]  eta: 17:25:39  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.7079  data: 0.0038
Test:  [1230/3316]  eta: 17:25:39  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.7684  data: 0.0040
Test:  [1230/3316]  eta: 17:25:57  loss: 1.0959 (1.1522)  acc1: 75.0000 (75.4976)  acc5: 93.7500 (91.5871)  time: 29.7711  data: 0.0034
Test:  [1240/3316]  eta: 17:16:37  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.6448  data: 0.0040
Test:  [1250/3316]  eta: 17:04:03  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.4786  data: 0.0040
Test:  [1270/3316]  eta: 16:38:51  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.0505  data: 0.0044
Test:  [1240/3316]  eta: 17:18:22  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.6517  data: 0.0041
Test:  [1270/3316]  eta: 16:40:27  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.0002  data: 0.0046
Test:  [1240/3316]  eta: 17:20:36  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.7955  data: 0.0046
Test:  [1240/3316]  eta: 17:20:37  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.8451  data: 0.0049
Test:  [1240/3316]  eta: 17:20:53  loss: 1.1052 (1.1529)  acc1: 75.0000 (75.4583)  acc5: 93.7500 (91.5794)  time: 29.8091  data: 0.0043
Test:  [1250/3316]  eta: 17:11:28  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.7170  data: 0.0040
Test:  [1260/3316]  eta: 16:58:55  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.5178  data: 0.0040
Test:  [1280/3316]  eta: 16:33:49  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 28.8908  data: 0.0047
Test:  [1250/3316]  eta: 17:13:09  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.4559  data: 0.0042
Test:  [1280/3316]  eta: 16:35:21  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 28.8093  data: 0.0041
Test:  [1250/3316]  eta: 17:15:23  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.6358  data: 0.0039
Test:  [1250/3316]  eta: 17:15:23  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.6430  data: 0.0036
Test:  [1250/3316]  eta: 17:15:38  loss: 1.2907 (1.1545)  acc1: 68.7500 (75.4197)  acc5: 87.5000 (91.5468)  time: 29.5473  data: 0.0037
Test:  [1260/3316]  eta: 17:06:22  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.4706  data: 0.0026
Test:  [1270/3316]  eta: 16:53:49  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.1679  data: 0.0027
Test:  [1290/3316]  eta: 16:28:51  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 28.8440  data: 0.0028
Test:  [1260/3316]  eta: 17:08:01  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.3628  data: 0.0029
Test:  [1290/3316]  eta: 16:30:24  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 28.7992  data: 0.0027
Test:  [1260/3316]  eta: 17:10:25  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.7802  data: 0.0028
Test:  [1260/3316]  eta: 17:10:26  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.8170  data: 0.0026
Test:  [1260/3316]  eta: 17:10:39  loss: 1.2597 (1.1555)  acc1: 68.7500 (75.3916)  acc5: 87.5000 (91.5444)  time: 29.7339  data: 0.0028
Test:  [1270/3316]  eta: 17:01:23  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.7726  data: 0.0037
Test:  [1280/3316]  eta: 16:48:49  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.3654  data: 0.0040
Test:  [1300/3316]  eta: 16:23:57  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.0907  data: 0.0039
Test:  [1270/3316]  eta: 17:02:59  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.6824  data: 0.0043
Test:  [1300/3316]  eta: 16:25:26  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.0410  data: 0.0045
Test:  [1270/3316]  eta: 17:05:13  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.8100  data: 0.0041
Test:  [1270/3316]  eta: 17:05:14  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.8475  data: 0.0040
Test:  [1270/3316]  eta: 17:05:27  loss: 1.1180 (1.1547)  acc1: 75.0000 (75.3983)  acc5: 93.7500 (91.5519)  time: 29.7802  data: 0.0042
Test:  [1280/3316]  eta: 16:56:15  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.7104  data: 0.0041
Test:  [1290/3316]  eta: 16:43:45  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.4082  data: 0.0039
Test:  [1310/3316]  eta: 16:18:57  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.0014  data: 0.0041
Test:  [1280/3316]  eta: 16:57:53  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.7703  data: 0.0042
Test:  [1310/3316]  eta: 16:20:27  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.0013  data: 0.0045
Test:  [1280/3316]  eta: 17:00:09  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.6110  data: 0.0042
Test:  [1280/3316]  eta: 17:00:10  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.5983  data: 0.0042
Test:  [1280/3316]  eta: 17:00:22  loss: 1.0803 (1.1551)  acc1: 75.0000 (75.4050)  acc5: 93.7500 (91.5203)  time: 29.5740  data: 0.0043
Test:  [1290/3316]  eta: 16:51:10  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.4930  data: 0.0030
Test:  [1300/3316]  eta: 16:38:41  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.3079  data: 0.0029
Test:  [1320/3316]  eta: 16:13:58  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 28.8271  data: 0.0030
Test:  [1290/3316]  eta: 16:52:45  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.5678  data: 0.0028
Test:  [1320/3316]  eta: 16:15:26  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 28.8884  data: 0.0025
Test:  [1290/3316]  eta: 16:54:57  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.5852  data: 0.0031
Test:  [1290/3316]  eta: 16:54:59  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.6025  data: 0.0029
Test:  [1290/3316]  eta: 16:55:11  loss: 1.0597 (1.1555)  acc1: 75.0000 (75.4018)  acc5: 93.7500 (91.5230)  time: 29.6068  data: 0.0029
Test:  [1300/3316]  eta: 16:46:10  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.7526  data: 0.0033
Test:  [1310/3316]  eta: 16:33:44  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.5185  data: 0.0034
Test:  [1330/3316]  eta: 16:09:04  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 29.0220  data: 0.0036
Test:  [1300/3316]  eta: 16:47:47  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.7958  data: 0.0038
Test:  [1330/3316]  eta: 16:10:37  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 29.2281  data: 0.0039
Test:  [1300/3316]  eta: 16:50:01  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.8480  data: 0.0046
Test:  [1300/3316]  eta: 16:50:06  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.9815  data: 0.0043
Test:  [1300/3316]  eta: 16:50:16  loss: 1.0524 (1.1549)  acc1: 75.0000 (75.4131)  acc5: 93.7500 (91.5402)  time: 29.9366  data: 0.0047
Test:  [1310/3316]  eta: 16:41:07  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.8139  data: 0.0045
Test:  [1340/3316]  eta: 16:04:08  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 29.1433  data: 0.0046
Test:  [1320/3316]  eta: 16:28:45  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 29.6544  data: 0.0044
Test:  [1310/3316]  eta: 16:42:44  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.9286  data: 0.0047
Test:  [1340/3316]  eta: 16:05:39  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 29.3092  data: 0.0044
Test:  [1310/3316]  eta: 16:44:53  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 29.9677  data: 0.0048
Test:  [1310/3316]  eta: 16:44:59  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 30.1180  data: 0.0045
Test:  [1310/3316]  eta: 16:45:08  loss: 1.0361 (1.1545)  acc1: 75.0000 (75.4338)  acc5: 93.7500 (91.5332)  time: 30.0114  data: 0.0051
Test:  [1350/3316]  eta: 15:59:11  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 29.0268  data: 0.0039
Test:  [1320/3316]  eta: 16:36:02  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 29.6223  data: 0.0041
Test:  [1330/3316]  eta: 16:23:46  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 29.6229  data: 0.0039
Test:  [1350/3316]  eta: 16:00:43  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 29.0617  data: 0.0032
Test:  [1320/3316]  eta: 16:37:42  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 29.8296  data: 0.0036
Test:  [1320/3316]  eta: 16:40:03  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 30.1522  data: 0.0033
Test:  [1320/3316]  eta: 16:40:13  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 30.3463  data: 0.0031
Test:  [1320/3316]  eta: 16:40:18  loss: 1.0976 (1.1546)  acc1: 81.2500 (75.4495)  acc5: 93.7500 (91.5452)  time: 30.1734  data: 0.0034
Test:  [1360/3316]  eta: 15:54:34  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 29.6628  data: 0.0032
Test:  [1330/3316]  eta: 16:31:16  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 30.2118  data: 0.0032
Test:  [1340/3316]  eta: 16:19:08  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.3166  data: 0.0029
Test:  [1360/3316]  eta: 15:56:02  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 29.6437  data: 0.0032
Test:  [1330/3316]  eta: 16:32:55  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 30.3796  data: 0.0032
Test:  [1330/3316]  eta: 16:35:08  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 30.6094  data: 0.0034
Test:  [1330/3316]  eta: 16:35:19  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 30.7524  data: 0.0033
Test:  [1330/3316]  eta: 16:35:20  loss: 1.0976 (1.1537)  acc1: 75.0000 (75.4790)  acc5: 93.7500 (91.5665)  time: 30.5381  data: 0.0035
Test:  [1370/3316]  eta: 15:49:48  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.0603  data: 0.0038
Test:  [1340/3316]  eta: 16:26:19  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.4870  data: 0.0043
Test:  [1350/3316]  eta: 16:14:18  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.6208  data: 0.0035
Test:  [1370/3316]  eta: 15:51:13  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 29.8775  data: 0.0042
Test:  [1340/3316]  eta: 16:27:59  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.5487  data: 0.0045
Test:  [1340/3316]  eta: 16:30:21  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.7037  data: 0.0050
Test:  [1340/3316]  eta: 16:30:31  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.5389  data: 0.0051
Test:  [1340/3316]  eta: 16:30:31  loss: 1.1553 (1.1541)  acc1: 75.0000 (75.4428)  acc5: 93.7500 (91.5548)  time: 30.7070  data: 0.0050
Test:  [1380/3316]  eta: 15:45:02  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 29.7655  data: 0.0050
Test:  [1350/3316]  eta: 16:21:23  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.1579  data: 0.0054
Test:  [1360/3316]  eta: 16:09:27  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 30.2194  data: 0.0048
Test:  [1380/3316]  eta: 15:46:25  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 29.6846  data: 0.0051
Test:  [1350/3316]  eta: 16:23:06  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.3630  data: 0.0050
Test:  [1350/3316]  eta: 16:25:24  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.6597  data: 0.0051
Test:  [1350/3316]  eta: 16:25:34  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.5943  data: 0.0053
Test:  [1350/3316]  eta: 16:25:37  loss: 1.1553 (1.1555)  acc1: 75.0000 (75.4256)  acc5: 87.5000 (91.5155)  time: 30.7435  data: 0.0053
Test:  [1390/3316]  eta: 15:40:08  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 29.5119  data: 0.0047
Test:  [1360/3316]  eta: 16:16:20  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 29.9442  data: 0.0044
Test:  [1370/3316]  eta: 16:04:30  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 29.9886  data: 0.0044
Test:  [1390/3316]  eta: 15:41:30  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 29.4640  data: 0.0043
Test:  [1360/3316]  eta: 16:18:05  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 30.2123  data: 0.0039
Test:  [1360/3316]  eta: 16:20:24  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 30.2225  data: 0.0034
Test:  [1360/3316]  eta: 16:20:35  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 30.2890  data: 0.0036
Test:  [1360/3316]  eta: 16:20:41  loss: 1.0478 (1.1536)  acc1: 75.0000 (75.4776)  acc5: 93.7500 (91.5274)  time: 30.4768  data: 0.0034
Test:  [1400/3316]  eta: 15:35:25  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 29.6155  data: 0.0033
Test:  [1370/3316]  eta: 16:11:29  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.1222  data: 0.0032
Test:  [1380/3316]  eta: 15:59:47  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.2643  data: 0.0032
Test:  [1400/3316]  eta: 15:36:47  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 29.6434  data: 0.0032
Test:  [1370/3316]  eta: 16:13:18  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.4341  data: 0.0033
Test:  [1370/3316]  eta: 16:15:36  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.5315  data: 0.0032
Test:  [1370/3316]  eta: 16:15:45  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.5155  data: 0.0033
Test:  [1370/3316]  eta: 16:15:51  loss: 0.9205 (1.1535)  acc1: 75.0000 (75.4923)  acc5: 93.7500 (91.5299)  time: 30.6397  data: 0.0032
Test:  [1410/3316]  eta: 15:30:42  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 30.0233  data: 0.0032
Test:  [1380/3316]  eta: 16:06:39  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.6004  data: 0.0041
Test:  [1390/3316]  eta: 15:55:02  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.6794  data: 0.0042
Test:  [1410/3316]  eta: 15:32:01  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 29.9831  data: 0.0038
Test:  [1380/3316]  eta: 16:08:22  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.6231  data: 0.0051
Test:  [1380/3316]  eta: 16:10:45  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.8832  data: 0.0047
Test:  [1380/3316]  eta: 16:10:54  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.7923  data: 0.0048
Test:  [1380/3316]  eta: 16:10:59  loss: 1.0541 (1.1531)  acc1: 75.0000 (75.4978)  acc5: 93.7500 (91.5369)  time: 30.7918  data: 0.0046
Test:  [1420/3316]  eta: 15:25:52  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 29.7882  data: 0.0051
Test:  [1390/3316]  eta: 16:01:41  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.3493  data: 0.0055
Test:  [1400/3316]  eta: 15:50:08  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.3130  data: 0.0051
Test:  [1420/3316]  eta: 15:27:13  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 29.7862  data: 0.0048
Test:  [1390/3316]  eta: 16:03:27  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.3532  data: 0.0056
Test:  [1390/3316]  eta: 16:05:51  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.6936  data: 0.0050
Test:  [1390/3316]  eta: 16:05:59  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.6307  data: 0.0050
Test:  [1390/3316]  eta: 16:06:02  loss: 1.1361 (1.1539)  acc1: 75.0000 (75.4808)  acc5: 93.7500 (91.5439)  time: 30.5556  data: 0.0049
Test:  [1430/3316]  eta: 15:21:03  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 29.5375  data: 0.0052
Test:  [1400/3316]  eta: 15:56:42  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.0367  data: 0.0048
Test:  [1410/3316]  eta: 15:45:13  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 29.9658  data: 0.0042
Test:  [1430/3316]  eta: 15:22:20  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 29.5523  data: 0.0042
Test:  [1400/3316]  eta: 15:58:28  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.2281  data: 0.0038
Test:  [1400/3316]  eta: 16:00:47  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.2466  data: 0.0035
Test:  [1400/3316]  eta: 16:00:56  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.2607  data: 0.0034
Test:  [1400/3316]  eta: 16:00:57  loss: 1.1091 (1.1533)  acc1: 75.0000 (75.4996)  acc5: 93.7500 (91.5462)  time: 30.1001  data: 0.0035
Test:  [1440/3316]  eta: 15:16:10  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.4403  data: 0.0031
Test:  [1410/3316]  eta: 15:51:40  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 29.9077  data: 0.0032
Test:  [1420/3316]  eta: 15:40:15  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 29.8415  data: 0.0030
Test:  [1440/3316]  eta: 15:17:26  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.3695  data: 0.0031
Test:  [1410/3316]  eta: 15:53:31  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 30.1508  data: 0.0029
Test:  [1410/3316]  eta: 15:55:52  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 30.1993  data: 0.0031
Test:  [1410/3316]  eta: 15:55:58  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 29.9981  data: 0.0031
Test:  [1410/3316]  eta: 15:55:58  loss: 1.0826 (1.1539)  acc1: 75.0000 (75.4740)  acc5: 87.5000 (91.5397)  time: 30.1598  data: 0.0031
Test:  [1450/3316]  eta: 15:11:18  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.3680  data: 0.0030
Test:  [1420/3316]  eta: 15:46:51  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 30.2736  data: 0.0037
Test:  [1450/3316]  eta: 15:12:42  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.6999  data: 0.0039
Test:  [1430/3316]  eta: 15:35:25  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.0591  data: 0.0037
Test:  [1420/3316]  eta: 15:48:36  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 30.3216  data: 0.0035
Test:  [1420/3316]  eta: 15:50:56  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 30.4558  data: 0.0048
Test:  [1420/3316]  eta: 15:51:01  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 30.3026  data: 0.0048
Test:  [1420/3316]  eta: 15:51:02  loss: 1.1304 (1.1539)  acc1: 75.0000 (75.4750)  acc5: 93.7500 (91.5464)  time: 30.3550  data: 0.0047
Test:  [1460/3316]  eta: 15:06:29  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 29.5110  data: 0.0049
Test:  [1460/3316]  eta: 15:07:45  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 29.5364  data: 0.0049
Test:  [1430/3316]  eta: 15:41:45  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.1194  data: 0.0050
Test:  [1440/3316]  eta: 15:30:22  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.8468  data: 0.0051
Test:  [1430/3316]  eta: 15:43:34  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.1465  data: 0.0042
Test:  [1430/3316]  eta: 15:45:55  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.2558  data: 0.0049
Test:  [1430/3316]  eta: 15:46:00  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.2469  data: 0.0048
Test:  [1430/3316]  eta: 15:46:02  loss: 1.1267 (1.1538)  acc1: 75.0000 (75.4892)  acc5: 93.7500 (91.5531)  time: 30.3078  data: 0.0049
Test:  [1470/3316]  eta: 15:01:38  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 29.5114  data: 0.0049
Test:  [1470/3316]  eta: 15:02:52  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 29.2202  data: 0.0041
Test:  [1440/3316]  eta: 15:36:46  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.7311  data: 0.0043
Test:  [1450/3316]  eta: 15:25:24  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.5358  data: 0.0044
Test:  [1440/3316]  eta: 15:38:32  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.8675  data: 0.0038
Test:  [1440/3316]  eta: 15:40:46  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.7996  data: 0.0031
Test:  [1440/3316]  eta: 15:40:50  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.7484  data: 0.0032
Test:  [1440/3316]  eta: 15:40:53  loss: 1.1888 (1.1534)  acc1: 81.2500 (75.5161)  acc5: 93.7500 (91.5510)  time: 29.8357  data: 0.0031
Test:  [1480/3316]  eta: 14:56:36  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 29.0078  data: 0.0030
Test:  [1480/3316]  eta: 14:57:51  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 29.0700  data: 0.0029
Test:  [1450/3316]  eta: 15:31:36  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.5671  data: 0.0028
Test:  [1460/3316]  eta: 15:20:18  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 29.3931  data: 0.0029
Test:  [1450/3316]  eta: 15:33:27  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.7487  data: 0.0030
Test:  [1450/3316]  eta: 15:35:45  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.7907  data: 0.0032
Test:  [1450/3316]  eta: 15:35:47  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.6570  data: 0.0029
Test:  [1450/3316]  eta: 15:35:52  loss: 1.2216 (1.1532)  acc1: 75.0000 (75.5126)  acc5: 93.7500 (91.5532)  time: 29.7551  data: 0.0033
Test:  [1490/3316]  eta: 14:51:41  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 28.8791  data: 0.0031
Test:  [1490/3316]  eta: 14:53:00  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 29.1454  data: 0.0030
Test:  [1470/3316]  eta: 15:15:26  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 29.6367  data: 0.0027
Test:  [1460/3316]  eta: 15:26:42  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 29.7802  data: 0.0027
Test:  [1460/3316]  eta: 15:28:32  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 30.0364  data: 0.0032
Test:  [1460/3316]  eta: 15:30:43  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 30.0516  data: 0.0041
Test:  [1460/3316]  eta: 15:30:44  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 29.9078  data: 0.0034
Test:  [1460/3316]  eta: 15:30:50  loss: 1.0290 (1.1528)  acc1: 75.0000 (75.5219)  acc5: 93.7500 (91.5597)  time: 30.0515  data: 0.0040
Test:  [1500/3316]  eta: 14:46:47  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 29.1790  data: 0.0042
Test:  [1500/3316]  eta: 14:48:06  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 29.4167  data: 0.0039
Test:  [1480/3316]  eta: 15:10:22  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 29.7032  data: 0.0038
Test:  [1470/3316]  eta: 15:21:35  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 29.8753  data: 0.0033
Test:  [1470/3316]  eta: 15:23:28  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 30.0725  data: 0.0034
Test:  [1470/3316]  eta: 15:25:42  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 29.9622  data: 0.0036
Test:  [1470/3316]  eta: 15:25:43  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 30.0729  data: 0.0040
Test:  [1470/3316]  eta: 15:25:50  loss: 0.9669 (1.1520)  acc1: 75.0000 (75.5523)  acc5: 93.7500 (91.5619)  time: 30.1131  data: 0.0038
Test:  [1510/3316]  eta: 14:41:55  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 29.2841  data: 0.0042
Test:  [1510/3316]  eta: 14:43:17  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 29.5006  data: 0.0037
Test:  [1490/3316]  eta: 15:05:29  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 29.6480  data: 0.0039
Test:  [1480/3316]  eta: 15:16:37  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 29.7306  data: 0.0034
Test:  [1480/3316]  eta: 15:18:30  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 29.9675  data: 0.0031
Test:  [1480/3316]  eta: 15:20:40  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 30.0001  data: 0.0037
Test:  [1480/3316]  eta: 15:20:43  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 30.1645  data: 0.0039
Test:  [1520/3316]  eta: 14:37:01  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 29.2946  data: 0.0037
Test:  [1480/3316]  eta: 15:20:49  loss: 0.9833 (1.1522)  acc1: 75.0000 (75.5402)  acc5: 93.7500 (91.5555)  time: 30.1241  data: 0.0041
Test:  [1520/3316]  eta: 14:38:24  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 29.5674  data: 0.0031
Test:  [1500/3316]  eta: 15:00:32  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 29.9474  data: 0.0037
Test:  [1490/3316]  eta: 15:11:37  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 29.9869  data: 0.0037
Test:  [1490/3316]  eta: 15:13:32  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 30.2067  data: 0.0035
Test:  [1530/3316]  eta: 14:32:08  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 29.2644  data: 0.0039
Test:  [1490/3316]  eta: 15:15:39  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 30.0361  data: 0.0035
Test:  [1490/3316]  eta: 15:15:45  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 30.2439  data: 0.0036
Test:  [1490/3316]  eta: 15:15:50  loss: 1.1483 (1.1528)  acc1: 75.0000 (75.5198)  acc5: 87.5000 (91.5409)  time: 30.1547  data: 0.0038
Test:  [1530/3316]  eta: 14:33:34  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 29.5296  data: 0.0033
Test:  [1510/3316]  eta: 14:55:39  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 29.9656  data: 0.0036
Test:  [1500/3316]  eta: 15:06:40  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 30.0481  data: 0.0039
Test:  [1500/3316]  eta: 15:08:36  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 30.2809  data: 0.0034
Test:  [1540/3316]  eta: 14:27:15  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 29.2947  data: 0.0035
Test:  [1500/3316]  eta: 15:10:39  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 30.1112  data: 0.0030
Test:  [1500/3316]  eta: 15:10:45  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 30.2083  data: 0.0029
Test:  [1500/3316]  eta: 15:10:49  loss: 0.9591 (1.1518)  acc1: 75.0000 (75.5455)  acc5: 93.7500 (91.5473)  time: 30.1737  data: 0.0030
Test:  [1540/3316]  eta: 14:28:43  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 29.5777  data: 0.0030
Test:  [1520/3316]  eta: 14:50:41  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 29.9534  data: 0.0037
Test:  [1510/3316]  eta: 15:01:41  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 30.0964  data: 0.0035
Test:  [1510/3316]  eta: 15:03:37  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 30.2412  data: 0.0037
Test:  [1550/3316]  eta: 14:22:24  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 29.4096  data: 0.0044
Test:  [1510/3316]  eta: 15:05:40  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 30.2158  data: 0.0047
Test:  [1510/3316]  eta: 15:05:47  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 30.2247  data: 0.0045
Test:  [1510/3316]  eta: 15:05:52  loss: 0.9742 (1.1521)  acc1: 81.2500 (75.5253)  acc5: 93.7500 (91.5329)  time: 30.2579  data: 0.0047
Test:  [1550/3316]  eta: 14:23:51  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 29.5114  data: 0.0042
Test:  [1530/3316]  eta: 14:45:47  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 29.8968  data: 0.0049
Test:  [1520/3316]  eta: 14:56:45  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 30.1162  data: 0.0045
Test:  [1520/3316]  eta: 14:58:47  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 30.5100  data: 0.0043
Test:  [1560/3316]  eta: 14:17:40  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.7900  data: 0.0047
Test:  [1520/3316]  eta: 15:00:47  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 30.5398  data: 0.0050
Test:  [1520/3316]  eta: 15:00:55  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 30.6196  data: 0.0049
Test:  [1520/3316]  eta: 15:01:00  loss: 1.2381 (1.1529)  acc1: 68.7500 (75.5178)  acc5: 87.5000 (91.5228)  time: 30.6529  data: 0.0051
Test:  [1560/3316]  eta: 14:19:05  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.7648  data: 0.0046
Test:  [1540/3316]  eta: 14:40:54  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.1107  data: 0.0046
Test:  [1530/3316]  eta: 14:51:49  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 30.2544  data: 0.0043
Test:  [1530/3316]  eta: 14:53:49  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 30.5584  data: 0.0036
Test:  [1570/3316]  eta: 14:12:47  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 29.7036  data: 0.0035
Test:  [1530/3316]  eta: 14:55:47  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 30.4527  data: 0.0032
Test:  [1530/3316]  eta: 14:55:55  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 30.5338  data: 0.0034
Test:  [1530/3316]  eta: 14:55:59  loss: 1.0513 (1.1525)  acc1: 75.0000 (75.5266)  acc5: 93.7500 (91.5251)  time: 30.4988  data: 0.0033
Test:  [1570/3316]  eta: 14:14:11  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 29.6649  data: 0.0035
Test:  [1550/3316]  eta: 14:35:57  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 29.9817  data: 0.0034
Test:  [1540/3316]  eta: 14:46:49  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.0760  data: 0.0030
Test:  [1540/3316]  eta: 14:48:54  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.3539  data: 0.0035
Test:  [1580/3316]  eta: 14:08:00  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 29.5895  data: 0.0043
Test:  [1540/3316]  eta: 14:50:52  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.3550  data: 0.0043
Test:  [1540/3316]  eta: 14:51:00  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.3957  data: 0.0046
Test:  [1540/3316]  eta: 14:51:04  loss: 1.0402 (1.1519)  acc1: 75.0000 (75.5313)  acc5: 93.7500 (91.5315)  time: 30.3453  data: 0.0045
Test:  [1580/3316]  eta: 14:09:24  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 29.6080  data: 0.0044
Test:  [1560/3316]  eta: 14:31:03  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.9373  data: 0.0051
Test:  [1550/3316]  eta: 14:41:50  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 29.9741  data: 0.0042
Test:  [1550/3316]  eta: 14:43:46  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 29.8931  data: 0.0038
Test:  [1590/3316]  eta: 14:03:00  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 29.2474  data: 0.0043
Test:  [1550/3316]  eta: 14:45:43  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 30.0029  data: 0.0044
Test:  [1550/3316]  eta: 14:45:51  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 30.0145  data: 0.0045
Test:  [1550/3316]  eta: 14:45:56  loss: 1.0402 (1.1522)  acc1: 75.0000 (75.5239)  acc5: 93.7500 (91.5176)  time: 30.0177  data: 0.0045
Test:  [1590/3316]  eta: 14:04:23  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 29.3202  data: 0.0042
Test:  [1570/3316]  eta: 14:25:59  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 29.6327  data: 0.0048
Test:  [1560/3316]  eta: 14:36:42  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.6230  data: 0.0040
Test:  [1560/3316]  eta: 14:38:41  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.4811  data: 0.0031
Test:  [1600/3316]  eta: 13:58:08  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 29.0380  data: 0.0031
Test:  [1560/3316]  eta: 14:40:44  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.8526  data: 0.0029
Test:  [1560/3316]  eta: 14:40:51  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.7677  data: 0.0028
Test:  [1560/3316]  eta: 14:40:57  loss: 1.0119 (1.1520)  acc1: 81.2500 (75.5445)  acc5: 87.5000 (91.5078)  time: 29.8644  data: 0.0028
Test:  [1600/3316]  eta: 13:59:34  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 29.2252  data: 0.0030
Test:  [1580/3316]  eta: 14:21:03  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 29.5624  data: 0.0032
Test:  [1570/3316]  eta: 14:31:44  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 29.6509  data: 0.0029
Test:  [1570/3316]  eta: 14:33:40  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 29.7976  data: 0.0027
Test:  [1610/3316]  eta: 13:53:16  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 29.3984  data: 0.0032
Test:  [1570/3316]  eta: 14:35:46  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 30.3140  data: 0.0038
Test:  [1570/3316]  eta: 14:35:50  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 30.1344  data: 0.0041
Test:  [1570/3316]  eta: 14:35:57  loss: 1.0709 (1.1518)  acc1: 75.0000 (75.5729)  acc5: 87.5000 (91.5062)  time: 30.2364  data: 0.0042
Test:  [1610/3316]  eta: 13:54:42  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 29.6379  data: 0.0039
Test:  [1590/3316]  eta: 14:16:07  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 29.9275  data: 0.0046
Test:  [1580/3316]  eta: 14:26:46  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 30.0948  data: 0.0046
Test:  [1580/3316]  eta: 14:28:40  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 29.9650  data: 0.0046
Test:  [1620/3316]  eta: 13:48:24  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 29.4155  data: 0.0041
Test:  [1580/3316]  eta: 14:30:50  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 30.4562  data: 0.0050
Test:  [1580/3316]  eta: 14:30:54  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 30.3613  data: 0.0051
Test:  [1620/3316]  eta: 13:49:55  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 29.7776  data: 0.0049
Test:  [1580/3316]  eta: 14:31:02  loss: 1.1290 (1.1515)  acc1: 75.0000 (75.5653)  acc5: 93.7500 (91.5283)  time: 30.4360  data: 0.0050
Test:  [1600/3316]  eta: 14:11:16  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 30.1055  data: 0.0048
Test:  [1590/3316]  eta: 14:21:51  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 30.2312  data: 0.0051
Test:  [1590/3316]  eta: 14:23:43  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 30.1653  data: 0.0048
Test:  [1630/3316]  eta: 13:43:31  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 29.3778  data: 0.0042
Test:  [1590/3316]  eta: 14:25:45  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 30.1363  data: 0.0042
Test:  [1630/3316]  eta: 13:45:00  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 29.5765  data: 0.0040
Test:  [1590/3316]  eta: 14:25:50  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 30.1895  data: 0.0039
Test:  [1590/3316]  eta: 14:25:57  loss: 1.1182 (1.1520)  acc1: 75.0000 (75.5696)  acc5: 93.7500 (91.5069)  time: 30.1996  data: 0.0036
Test:  [1610/3316]  eta: 14:06:15  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 29.8808  data: 0.0036
Test:  [1600/3316]  eta: 14:16:46  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 29.9484  data: 0.0035
Test:  [1600/3316]  eta: 14:18:42  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 30.1660  data: 0.0031
Test:  [1640/3316]  eta: 13:38:38  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.3147  data: 0.0032
Test:  [1640/3316]  eta: 13:40:13  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.5817  data: 0.0035
Test:  [1600/3316]  eta: 14:20:50  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 30.1890  data: 0.0030
Test:  [1600/3316]  eta: 14:20:54  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 30.1781  data: 0.0030
Test:  [1600/3316]  eta: 14:21:02  loss: 1.1182 (1.1518)  acc1: 75.0000 (75.5856)  acc5: 87.5000 (91.4897)  time: 30.2065  data: 0.0030
Test:  [1620/3316]  eta: 14:01:25  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 29.9962  data: 0.0031
Test:  [1610/3316]  eta: 14:11:53  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 30.0535  data: 0.0034
Test:  [1610/3316]  eta: 14:13:48  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 30.2923  data: 0.0033
Test:  [1650/3316]  eta: 13:33:49  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.5231  data: 0.0033
Test:  [1650/3316]  eta: 13:35:16  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.5263  data: 0.0038
Test:  [1610/3316]  eta: 14:15:46  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 30.2395  data: 0.0029
Test:  [1610/3316]  eta: 14:15:51  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 30.2415  data: 0.0032
Test:  [1610/3316]  eta: 14:15:57  loss: 1.0694 (1.1515)  acc1: 75.0000 (75.5936)  acc5: 87.5000 (91.4960)  time: 30.2200  data: 0.0030
Test:  [1630/3316]  eta: 13:56:24  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 29.9807  data: 0.0032
Test:  [1620/3316]  eta: 14:06:49  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 30.0319  data: 0.0034
Test:  [1620/3316]  eta: 14:08:51  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 30.4712  data: 0.0039
Test:  [1660/3316]  eta: 13:29:00  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 29.7161  data: 0.0034
Test:  [1660/3316]  eta: 13:30:29  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 29.5544  data: 0.0042
Test:  [1620/3316]  eta: 14:10:51  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 30.2297  data: 0.0043
Test:  [1620/3316]  eta: 14:10:58  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 30.3875  data: 0.0046
Test:  [1620/3316]  eta: 14:11:04  loss: 1.0595 (1.1512)  acc1: 75.0000 (75.5976)  acc5: 93.7500 (91.5060)  time: 30.2781  data: 0.0043
Test:  [1640/3316]  eta: 13:51:34  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.9619  data: 0.0047
Test:  [1630/3316]  eta: 14:01:56  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 30.0597  data: 0.0045
Test:  [1670/3316]  eta: 13:24:07  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 29.4955  data: 0.0041
Test:  [1630/3316]  eta: 14:03:53  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 30.3100  data: 0.0051
Test:  [1670/3316]  eta: 13:25:32  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 29.5481  data: 0.0045
Test:  [1630/3316]  eta: 14:05:46  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 30.1739  data: 0.0051
Test:  [1630/3316]  eta: 14:05:52  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 30.2655  data: 0.0049
Test:  [1630/3316]  eta: 14:05:58  loss: 0.9922 (1.1498)  acc1: 75.0000 (75.6323)  acc5: 93.7500 (91.5274)  time: 30.2581  data: 0.0049
Test:  [1650/3316]  eta: 13:46:29  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.8126  data: 0.0048
Test:  [1640/3316]  eta: 13:56:50  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.9543  data: 0.0044
Test:  [1680/3316]  eta: 13:19:07  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 28.9408  data: 0.0038
Test:  [1640/3316]  eta: 13:58:46  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.8022  data: 0.0045
Test:  [1680/3316]  eta: 13:20:33  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 28.9023  data: 0.0034
Test:  [1640/3316]  eta: 14:00:40  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.6656  data: 0.0036
Test:  [1640/3316]  eta: 14:00:49  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.7615  data: 0.0034
Test:  [1640/3316]  eta: 14:00:54  loss: 0.9922 (1.1499)  acc1: 75.0000 (75.6284)  acc5: 93.7500 (91.5296)  time: 29.7664  data: 0.0035
Test:  [1660/3316]  eta: 13:41:30  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 29.3614  data: 0.0029
Test:  [1650/3316]  eta: 13:51:49  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.5925  data: 0.0027
Test:  [1690/3316]  eta: 13:14:14  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 28.9580  data: 0.0028
Test:  [1650/3316]  eta: 13:53:46  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.7277  data: 0.0028
Test:  [1690/3316]  eta: 13:15:38  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 28.9802  data: 0.0029
Test:  [1650/3316]  eta: 13:55:35  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.6647  data: 0.0031
Test:  [1650/3316]  eta: 13:55:45  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.8305  data: 0.0029
Test:  [1650/3316]  eta: 13:55:50  loss: 1.1127 (1.1502)  acc1: 75.0000 (75.6322)  acc5: 87.5000 (91.5165)  time: 29.8033  data: 0.0029
Test:  [1670/3316]  eta: 13:36:28  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 29.4599  data: 0.0027
Test:  [1660/3316]  eta: 13:46:44  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 29.6490  data: 0.0027
Test:  [1700/3316]  eta: 13:09:21  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 29.3204  data: 0.0027
Test:  [1660/3316]  eta: 13:48:49  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 30.2280  data: 0.0035
Test:  [1700/3316]  eta: 13:10:46  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 29.3883  data: 0.0038
Test:  [1660/3316]  eta: 13:50:38  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 30.1030  data: 0.0042
Test:  [1660/3316]  eta: 13:50:51  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 30.2851  data: 0.0049
Test:  [1660/3316]  eta: 13:50:55  loss: 1.0906 (1.1509)  acc1: 75.0000 (75.6284)  acc5: 87.5000 (91.5074)  time: 30.2185  data: 0.0047
Test:  [1680/3316]  eta: 13:31:39  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 29.9896  data: 0.0050
Test:  [1670/3316]  eta: 13:41:55  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 30.1733  data: 0.0044
Test:  [1710/3316]  eta: 13:04:33  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 29.5903  data: 0.0048
Test:  [1670/3316]  eta: 13:43:53  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 30.4070  data: 0.0051
Test:  [1710/3316]  eta: 13:05:55  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 29.6034  data: 0.0050
Test:  [1670/3316]  eta: 13:45:39  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 30.3711  data: 0.0051
Test:  [1670/3316]  eta: 13:45:51  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 30.5292  data: 0.0059
Test:  [1670/3316]  eta: 13:45:55  loss: 1.0947 (1.1507)  acc1: 75.0000 (75.6545)  acc5: 93.7500 (91.5096)  time: 30.4428  data: 0.0054
Test:  [1690/3316]  eta: 13:26:41  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 30.1726  data: 0.0054
Test:  [1680/3316]  eta: 13:36:52  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 30.3124  data: 0.0049
Test:  [1720/3316]  eta: 12:59:37  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 29.4286  data: 0.0049
Test:  [1680/3316]  eta: 13:38:48  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 30.0183  data: 0.0045
Test:  [1720/3316]  eta: 13:00:58  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 29.2831  data: 0.0042
Test:  [1680/3316]  eta: 13:40:36  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 30.0794  data: 0.0040
Test:  [1680/3316]  eta: 13:40:46  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 30.0013  data: 0.0040
Test:  [1680/3316]  eta: 13:40:51  loss: 1.0582 (1.1498)  acc1: 81.2500 (75.6692)  acc5: 93.7500 (91.5155)  time: 30.0276  data: 0.0037
Test:  [1700/3316]  eta: 13:21:42  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 29.6739  data: 0.0034
Test:  [1690/3316]  eta: 13:31:52  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 29.7823  data: 0.0034
Test:  [1730/3316]  eta: 12:54:47  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.3048  data: 0.0029
Test:  [1690/3316]  eta: 13:33:52  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 30.0232  data: 0.0031
Test:  [1730/3316]  eta: 12:56:08  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.3851  data: 0.0030
Test:  [1690/3316]  eta: 13:35:41  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 30.3437  data: 0.0032
Test:  [1690/3316]  eta: 13:35:51  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 30.2004  data: 0.0031
Test:  [1690/3316]  eta: 13:35:54  loss: 1.0179 (1.1507)  acc1: 75.0000 (75.6431)  acc5: 93.7500 (91.5065)  time: 30.1908  data: 0.0034
Test:  [1710/3316]  eta: 13:16:49  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 29.9472  data: 0.0032
Test:  [1700/3316]  eta: 13:26:54  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 30.0291  data: 0.0032
Test:  [1740/3316]  eta: 12:49:55  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.5356  data: 0.0031
Test:  [1700/3316]  eta: 13:28:56  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 30.4693  data: 0.0032
Test:  [1740/3316]  eta: 12:51:17  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.7509  data: 0.0035
Test:  [1700/3316]  eta: 13:30:46  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 30.7029  data: 0.0046
Test:  [1700/3316]  eta: 13:30:53  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 30.5560  data: 0.0040
Test:  [1700/3316]  eta: 13:30:59  loss: 1.2216 (1.1518)  acc1: 75.0000 (75.6210)  acc5: 87.5000 (91.4903)  time: 30.6208  data: 0.0047
Test:  [1720/3316]  eta: 13:11:55  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.2323  data: 0.0042
Test:  [1710/3316]  eta: 13:22:00  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 30.3589  data: 0.0046
Test:  [1750/3316]  eta: 12:45:16  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.1896  data: 0.0049
Test:  [1710/3316]  eta: 13:24:12  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 31.0881  data: 0.0057
Test:  [1750/3316]  eta: 12:46:38  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.3171  data: 0.0052
Test:  [1710/3316]  eta: 13:26:00  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 31.1971  data: 0.0055
Test:  [1710/3316]  eta: 13:26:07  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 31.0556  data: 0.0053
Test:  [1710/3316]  eta: 13:26:13  loss: 1.3050 (1.1525)  acc1: 75.0000 (75.5954)  acc5: 87.5000 (91.4779)  time: 31.2357  data: 0.0057
Test:  [1730/3316]  eta: 13:07:12  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 30.7908  data: 0.0048
Test:  [1720/3316]  eta: 13:17:13  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.9307  data: 0.0053
Test:  [1760/3316]  eta: 12:40:27  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 30.3588  data: 0.0050
Test:  [1720/3316]  eta: 13:19:08  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.7034  data: 0.0055
Test:  [1760/3316]  eta: 12:41:39  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 29.8950  data: 0.0047
Test:  [1720/3316]  eta: 13:20:52  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.5523  data: 0.0041
Test:  [1720/3316]  eta: 13:20:58  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.5104  data: 0.0043
Test:  [1720/3316]  eta: 13:21:05  loss: 1.0456 (1.1521)  acc1: 75.0000 (75.6137)  acc5: 87.5000 (91.4730)  time: 30.5535  data: 0.0043
Test:  [1740/3316]  eta: 13:02:08  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 30.2226  data: 0.0036
Test:  [1730/3316]  eta: 13:12:06  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 30.2289  data: 0.0037
Test:  [1770/3316]  eta: 12:35:30  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.3368  data: 0.0031
Test:  [1730/3316]  eta: 13:14:06  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.7924  data: 0.0029
Test:  [1770/3316]  eta: 12:36:46  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.1160  data: 0.0029
Test:  [1730/3316]  eta: 13:15:53  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.8713  data: 0.0034
Test:  [1730/3316]  eta: 13:15:59  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.8154  data: 0.0031
Test:  [1730/3316]  eta: 13:16:06  loss: 1.0450 (1.1523)  acc1: 75.0000 (75.6030)  acc5: 93.7500 (91.4717)  time: 29.8611  data: 0.0029
Test:  [1750/3316]  eta: 12:57:14  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 29.6550  data: 0.0029
Test:  [1740/3316]  eta: 13:07:08  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.6703  data: 0.0034
Test:  [1780/3316]  eta: 12:30:39  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 29.2550  data: 0.0033
Test:  [1740/3316]  eta: 13:09:03  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.8122  data: 0.0030
Test:  [1780/3316]  eta: 12:31:50  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 29.2638  data: 0.0032
Test:  [1740/3316]  eta: 13:10:46  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.9053  data: 0.0033
Test:  [1740/3316]  eta: 13:10:51  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.8315  data: 0.0030
Test:  [1740/3316]  eta: 13:10:59  loss: 1.0338 (1.1519)  acc1: 75.0000 (75.6103)  acc5: 93.7500 (91.4812)  time: 29.9493  data: 0.0029
Test:  [1760/3316]  eta: 12:52:11  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 29.6859  data: 0.0029
Test:  [1750/3316]  eta: 13:02:02  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 29.7052  data: 0.0033
Test:  [1790/3316]  eta: 12:25:42  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 29.2142  data: 0.0033
Test:  [1750/3316]  eta: 13:04:05  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.0115  data: 0.0039
Test:  [1790/3316]  eta: 12:27:00  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 29.4302  data: 0.0042
Test:  [1750/3316]  eta: 13:05:51  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.1048  data: 0.0048
Test:  [1750/3316]  eta: 13:05:56  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.0862  data: 0.0047
Test:  [1750/3316]  eta: 13:06:04  loss: 1.0338 (1.1520)  acc1: 75.0000 (75.6032)  acc5: 93.7500 (91.4799)  time: 30.1407  data: 0.0046
Test:  [1770/3316]  eta: 12:47:19  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.8078  data: 0.0047
Test:  [1760/3316]  eta: 12:57:07  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 29.8446  data: 0.0044
Test:  [1800/3316]  eta: 12:20:56  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 29.5188  data: 0.0049
Test:  [1800/3316]  eta: 12:22:05  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 29.4778  data: 0.0051
Test:  [1760/3316]  eta: 12:59:01  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 29.9729  data: 0.0051
Test:  [1760/3316]  eta: 13:00:44  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 30.1124  data: 0.0053
Test:  [1760/3316]  eta: 13:00:49  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 30.1085  data: 0.0050
Test:  [1780/3316]  eta: 12:42:15  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 29.7518  data: 0.0049
Test:  [1760/3316]  eta: 13:00:57  loss: 1.0455 (1.1515)  acc1: 75.0000 (75.6140)  acc5: 93.7500 (91.4857)  time: 30.1146  data: 0.0050
Test:  [1770/3316]  eta: 12:52:01  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.8563  data: 0.0048
Test:  [1810/3316]  eta: 12:15:58  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 29.4560  data: 0.0048
Test:  [1810/3316]  eta: 12:17:08  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 29.1143  data: 0.0041
Test:  [1770/3316]  eta: 12:53:57  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.6256  data: 0.0041
Test:  [1770/3316]  eta: 12:55:46  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.9647  data: 0.0035
Test:  [1770/3316]  eta: 12:55:51  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 29.9227  data: 0.0033
Test:  [1790/3316]  eta: 12:37:21  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 29.6108  data: 0.0031
Test:  [1770/3316]  eta: 12:56:00  loss: 0.9833 (1.1509)  acc1: 81.2500 (75.6282)  acc5: 93.7500 (91.4879)  time: 30.0456  data: 0.0034
Test:  [1780/3316]  eta: 12:47:06  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 29.8809  data: 0.0032
Test:  [1820/3316]  eta: 12:11:08  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 29.2344  data: 0.0031
Test:  [1820/3316]  eta: 12:12:16  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 29.2757  data: 0.0030
Test:  [1780/3316]  eta: 12:48:57  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 29.8863  data: 0.0030
Test:  [1780/3316]  eta: 12:50:43  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 30.1895  data: 0.0030
Test:  [1780/3316]  eta: 12:50:47  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 30.1214  data: 0.0029
Test:  [1800/3316]  eta: 12:32:21  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 29.8862  data: 0.0029
Test:  [1780/3316]  eta: 12:50:58  loss: 1.0166 (1.1507)  acc1: 81.2500 (75.6562)  acc5: 93.7500 (91.4900)  time: 30.3126  data: 0.0035
Test:  [1790/3316]  eta: 12:42:03  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 30.0709  data: 0.0029
Test:  [1830/3316]  eta: 12:06:12  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 29.3776  data: 0.0037
Test:  [1830/3316]  eta: 12:07:19  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 29.2795  data: 0.0031
Test:  [1790/3316]  eta: 12:43:55  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 29.9804  data: 0.0030
Test:  [1790/3316]  eta: 12:45:43  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 30.0609  data: 0.0029
Test:  [1790/3316]  eta: 12:45:47  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 30.0266  data: 0.0028
Test:  [1810/3316]  eta: 12:27:25  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 29.7881  data: 0.0028
Test:  [1790/3316]  eta: 12:46:00  loss: 1.0506 (1.1502)  acc1: 81.2500 (75.6700)  acc5: 93.7500 (91.5061)  time: 30.2097  data: 0.0034
Test:  [1800/3316]  eta: 12:37:06  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 29.9071  data: 0.0029
Test:  [1840/3316]  eta: 12:01:22  loss: 1.0703 (1.1493)  acc1: 75.0000 (75.7299)  acc5: 87.5000 (91.4754)  time: 29.3528  data: 0.0036
Test:  [1840/3316]  eta: 12:02:33  loss: 1.0703 (1.1493)  acc1: 75.0000 (75.7299)  acc5: 87.5000 (91.4754)  time: 29.6397  data: 0.0035
Test:  [1800/3316]  eta: 12:39:01  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 30.3172  data: 0.0037
Test:  [1800/3316]  eta: 12:40:46  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 30.3623  data: 0.0043
Test:  [1820/3316]  eta: 12:22:31  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 30.0654  data: 0.0048
Test:  [1800/3316]  eta: 12:40:49  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 30.3668  data: 0.0043
Test:  [1800/3316]  eta: 12:41:02  loss: 1.0862 (1.1507)  acc1: 75.0000 (75.6732)  acc5: 93.7500 (91.4874)  time: 30.5051  data: 0.0053
Test:  [1810/3316]  eta: 12:32:09  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 30.2583  data: 0.0052
Test:  [1850/3316]  eta: 11:56:32  loss: 1.0927 (1.1495)  acc1: 75.0000 (75.7226)  acc5: 87.5000 (91.4708)  time: 29.7393  data: 0.0052
Test:  [1850/3316]  eta: 11:57:37  loss: 1.0927 (1.1495)  acc1: 75.0000 (75.7226)  acc5: 87.5000 (91.4708)  time: 29.6927  data: 0.0047
Test:  [1810/3316]  eta: 12:33:58  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 30.2733  data: 0.0047
Test:  [1830/3316]  eta: 12:17:37  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 30.2158  data: 0.0052
Test:  [1810/3316]  eta: 12:35:49  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 30.5545  data: 0.0045
Test:  [1810/3316]  eta: 12:35:52  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 30.5444  data: 0.0044
Test:  [1810/3316]  eta: 12:36:05  loss: 1.1986 (1.1506)  acc1: 75.0000 (75.6937)  acc5: 87.5000 (91.4723)  time: 30.5703  data: 0.0052
Test:  [1820/3316]  eta: 12:27:13  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 30.3447  data: 0.0052
Test:  [1860/3316]  eta: 11:51:43  loss: 1.2040 (1.1499)  acc1: 75.0000 (75.7019)  acc5: 93.7500 (91.4764)  time: 29.8228  data: 0.0049
Test:  [1860/3316]  eta: 11:52:47  loss: 1.2040 (1.1499)  acc1: 75.0000 (75.7019)  acc5: 93.7500 (91.4764)  time: 29.5008  data: 0.0042
Test:  [1820/3316]  eta: 12:29:02  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 30.1538  data: 0.0039
Test:  [1840/3316]  eta: 12:12:36  loss: 1.0703 (1.1493)  acc1: 75.0000 (75.7299)  acc5: 87.5000 (91.4754)  time: 29.8649  data: 0.0032
Test:  [1820/3316]  eta: 12:30:45  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 30.1986  data: 0.0031
Test:  [1820/3316]  eta: 12:30:48  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 30.2078  data: 0.0030
Test:  [1820/3316]  eta: 12:31:00  loss: 1.0380 (1.1499)  acc1: 81.2500 (75.7070)  acc5: 87.5000 (91.4813)  time: 30.1326  data: 0.0029
Test:  [1830/3316]  eta: 12:22:09  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 29.9081  data: 0.0031
Test:  [1870/3316]  eta: 11:46:47  loss: 1.1050 (1.1490)  acc1: 75.0000 (75.7282)  acc5: 93.7500 (91.4852)  time: 29.4411  data: 0.0030
Test:  [1870/3316]  eta: 11:47:50  loss: 1.1050 (1.1490)  acc1: 75.0000 (75.7282)  acc5: 93.7500 (91.4852)  time: 29.3679  data: 0.0029
Test:  [1830/3316]  eta: 12:24:00  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 30.1950  data: 0.0034
Test:  [1850/3316]  eta: 12:07:43  loss: 1.0927 (1.1495)  acc1: 75.0000 (75.7226)  acc5: 87.5000 (91.4708)  time: 29.9110  data: 0.0041
Test:  [1830/3316]  eta: 12:25:49  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 30.2450  data: 0.0038
Test:  [1830/3316]  eta: 12:25:53  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 30.3023  data: 0.0039
Test:  [1830/3316]  eta: 12:26:07  loss: 1.0134 (1.1496)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4801)  time: 30.4147  data: 0.0041
Test:  [1840/3316]  eta: 12:17:15  loss: 1.0703 (1.1493)  acc1: 75.0000 (75.7299)  acc5: 87.5000 (91.4754)  time: 30.0623  data: 0.0037
Test:  [1880/3316]  eta: 11:42:02  loss: 1.1126 (1.1491)  acc1: 75.0000 (75.7243)  acc5: 93.7500 (91.4872)  time: 29.6894  data: 0.0040
Test:  [1880/3316]  eta: 11:43:04  loss: 1.1126 (1.1491)  acc1: 75.0000 (75.7243)  acc5: 93.7500 (91.4872)  time: 29.6358  data: 0.0037
Test:  [1840/3316]  eta: 12:19:06  loss: 1.0703 (1.1493)  acc1: 75.0000 (75.7299)  acc5: 87.5000 (91.4754)  time: 30.3322  data: 0.0042
Test:  [1860/3316]  eta: 12:02:44  loss: 1.2040 (1.1499)  acc1: 75.0000 (75.7019)  acc5: 93.7500 (91.4764)  time: 29.9595  data: 0.0044
Test:  [1840/3316]  eta: 12:20:47  loss: 1.0703 (1.1493)  acc1: 75.0000 (75.7299)  acc5: 87.5000 (91.4754)  time: 30.3791  data: 0.0039
Test:  [1840/3316]  eta: 12:20:50  loss: 1.0703 (1.1493)  acc1: 75.0000 (75.7299)  acc5: 87.5000 (91.4754)  time: 30.3425  data: 0.0041
Test:  [1840/3316]  eta: 12:21:05  loss: 1.0703 (1.1493)  acc1: 75.0000 (75.7299)  acc5: 87.5000 (91.4754)  time: 30.5656  data: 0.0041
Test:  [1890/3316]  eta: 11:37:06  loss: 1.1329 (1.1490)  acc1: 75.0000 (75.7271)  acc5: 93.7500 (91.4893)  time: 29.6743  data: 0.0043
Test:  [1850/3316]  eta: 12:12:12  loss: 1.0927 (1.1495)  acc1: 75.0000 (75.7226)  acc5: 87.5000 (91.4708)  time: 30.1169  data: 0.0036
Test:  [1890/3316]  eta: 11:38:05  loss: 1.1329 (1.1490)  acc1: 75.0000 (75.7271)  acc5: 93.7500 (91.4893)  time: 29.5643  data: 0.0038
Test:  [1850/3316]  eta: 12:14:01  loss: 1.0927 (1.1495)  acc1: 75.0000 (75.7226)  acc5: 87.5000 (91.4708)  time: 30.1342  data: 0.0037
Test:  [1870/3316]  eta: 11:57:45  loss: 1.1050 (1.1490)  acc1: 75.0000 (75.7282)  acc5: 93.7500 (91.4852)  time: 29.6040  data: 0.0030
Test:  [1850/3316]  eta: 12:15:46  loss: 1.0927 (1.1495)  acc1: 75.0000 (75.7226)  acc5: 87.5000 (91.4708)  time: 30.1058  data: 0.0030
Test:  [1850/3316]  eta: 12:15:49  loss: 1.0927 (1.1495)  acc1: 75.0000 (75.7226)  acc5: 87.5000 (91.4708)  time: 30.0519  data: 0.0032
Test:  [1900/3316]  eta: 11:32:15  loss: 1.1438 (1.1501)  acc1: 68.7500 (75.6937)  acc5: 87.5000 (91.4650)  time: 29.3096  data: 0.0036
Test:  [1850/3316]  eta: 12:16:07  loss: 1.0927 (1.1495)  acc1: 75.0000 (75.7226)  acc5: 87.5000 (91.4708)  time: 30.2685  data: 0.0033
Test:  [1860/3316]  eta: 12:07:14  loss: 1.2040 (1.1499)  acc1: 75.0000 (75.7019)  acc5: 93.7500 (91.4764)  time: 29.8908  data: 0.0032
Test:  [1900/3316]  eta: 11:33:15  loss: 1.1438 (1.1501)  acc1: 68.7500 (75.6937)  acc5: 87.5000 (91.4650)  time: 29.2963  data: 0.0038
Test:  [1860/3316]  eta: 12:09:07  loss: 1.2040 (1.1499)  acc1: 75.0000 (75.7019)  acc5: 93.7500 (91.4764)  time: 30.1417  data: 0.0038
Test:  [1880/3316]  eta: 11:52:49  loss: 1.1126 (1.1491)  acc1: 75.0000 (75.7243)  acc5: 93.7500 (91.4872)  time: 29.8751  data: 0.0040
Test:  [1860/3316]  eta: 12:10:45  loss: 1.2040 (1.1499)  acc1: 75.0000 (75.7019)  acc5: 93.7500 (91.4764)  time: 30.1164  data: 0.0038
Test:  [1860/3316]  eta: 12:10:49  loss: 1.2040 (1.1499)  acc1: 75.0000 (75.7019)  acc5: 93.7500 (91.4764)  time: 30.2088  data: 0.0040
Test:  [1910/3316]  eta: 11:27:21  loss: 1.1263 (1.1499)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4639)  time: 29.4385  data: 0.0039
Test:  [1860/3316]  eta: 12:11:05  loss: 1.2040 (1.1499)  acc1: 75.0000 (75.7019)  acc5: 93.7500 (91.4764)  time: 30.2722  data: 0.0040
Test:  [1870/3316]  eta: 12:02:14  loss: 1.1050 (1.1490)  acc1: 75.0000 (75.7282)  acc5: 93.7500 (91.4852)  time: 30.0763  data: 0.0041
Test:  [1910/3316]  eta: 11:28:18  loss: 1.1263 (1.1499)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4639)  time: 29.3931  data: 0.0043
Test:  [1870/3316]  eta: 12:04:03  loss: 1.1050 (1.1490)  acc1: 75.0000 (75.7282)  acc5: 93.7500 (91.4852)  time: 30.2545  data: 0.0039
Test:  [1890/3316]  eta: 11:47:49  loss: 1.1329 (1.1490)  acc1: 75.0000 (75.7271)  acc5: 93.7500 (91.4893)  time: 29.7485  data: 0.0042
Test:  [1870/3316]  eta: 12:05:42  loss: 1.1050 (1.1490)  acc1: 75.0000 (75.7282)  acc5: 93.7500 (91.4852)  time: 29.9855  data: 0.0036
Test:  [1870/3316]  eta: 12:05:47  loss: 1.1050 (1.1490)  acc1: 75.0000 (75.7282)  acc5: 93.7500 (91.4852)  time: 30.0914  data: 0.0037
Test:  [1920/3316]  eta: 11:22:27  loss: 1.0874 (1.1503)  acc1: 75.0000 (75.6963)  acc5: 87.5000 (91.4563)  time: 29.2476  data: 0.0035
Test:  [1870/3316]  eta: 12:06:04  loss: 1.1050 (1.1490)  acc1: 75.0000 (75.7282)  acc5: 93.7500 (91.4852)  time: 30.1152  data: 0.0035
Test:  [1880/3316]  eta: 11:57:17  loss: 1.1126 (1.1491)  acc1: 75.0000 (75.7243)  acc5: 93.7500 (91.4872)  time: 30.0919  data: 0.0037
Test:  [1920/3316]  eta: 11:23:32  loss: 1.0874 (1.1503)  acc1: 75.0000 (75.6963)  acc5: 87.5000 (91.4563)  time: 29.6659  data: 0.0032
Test:  [1880/3316]  eta: 11:59:15  loss: 1.1126 (1.1491)  acc1: 75.0000 (75.7243)  acc5: 93.7500 (91.4872)  time: 30.6188  data: 0.0032
Test:  [1900/3316]  eta: 11:43:01  loss: 1.1438 (1.1501)  acc1: 68.7500 (75.6937)  acc5: 87.5000 (91.4650)  time: 30.2617  data: 0.0032
Test:  [1930/3316]  eta: 11:17:42  loss: 1.0995 (1.1500)  acc1: 75.0000 (75.6991)  acc5: 87.5000 (91.4649)  time: 29.8356  data: 0.0029
Test:  [1880/3316]  eta: 12:00:49  loss: 1.1126 (1.1491)  acc1: 75.0000 (75.7243)  acc5: 93.7500 (91.4872)  time: 30.5635  data: 0.0032
Test:  [1880/3316]  eta: 12:00:54  loss: 1.1126 (1.1491)  acc1: 75.0000 (75.7243)  acc5: 93.7500 (91.4872)  time: 30.6153  data: 0.0028
Test:  [1880/3316]  eta: 12:01:13  loss: 1.1126 (1.1491)  acc1: 75.0000 (75.7243)  acc5: 93.7500 (91.4872)  time: 30.8532  data: 0.0031
Test:  [1890/3316]  eta: 11:52:26  loss: 1.1329 (1.1490)  acc1: 75.0000 (75.7271)  acc5: 93.7500 (91.4893)  time: 30.7133  data: 0.0031
Test:  [1930/3316]  eta: 11:18:41  loss: 1.0995 (1.1500)  acc1: 75.0000 (75.6991)  acc5: 87.5000 (91.4649)  time: 30.1239  data: 0.0034
Test:  [1890/3316]  eta: 11:54:18  loss: 1.1329 (1.1490)  acc1: 75.0000 (75.7271)  acc5: 93.7500 (91.4893)  time: 31.0696  data: 0.0040
Test:  [1910/3316]  eta: 11:38:05  loss: 1.1263 (1.1499)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4639)  time: 30.6119  data: 0.0047
Test:  [1940/3316]  eta: 11:12:51  loss: 1.0613 (1.1497)  acc1: 75.0000 (75.7148)  acc5: 87.5000 (91.4541)  time: 30.0370  data: 0.0040
Test:  [1890/3316]  eta: 11:55:51  loss: 1.1329 (1.1490)  acc1: 75.0000 (75.7271)  acc5: 93.7500 (91.4893)  time: 30.8444  data: 0.0043
Test:  [1890/3316]  eta: 11:55:56  loss: 1.1329 (1.1490)  acc1: 75.0000 (75.7271)  acc5: 93.7500 (91.4893)  time: 30.8592  data: 0.0041
Test:  [1900/3316]  eta: 11:47:27  loss: 1.1438 (1.1501)  acc1: 68.7500 (75.6937)  acc5: 87.5000 (91.4650)  time: 30.5987  data: 0.0044
Test:  [1890/3316]  eta: 11:56:13  loss: 1.1329 (1.1490)  acc1: 75.0000 (75.7271)  acc5: 93.7500 (91.4893)  time: 30.9190  data: 0.0044
Test:  [1940/3316]  eta: 11:13:52  loss: 1.0613 (1.1497)  acc1: 75.0000 (75.7148)  acc5: 87.5000 (91.4541)  time: 29.9156  data: 0.0042
Test:  [1900/3316]  eta: 11:49:31  loss: 1.1438 (1.1501)  acc1: 68.7500 (75.6937)  acc5: 87.5000 (91.4650)  time: 31.2067  data: 0.0043
Test:  [1920/3316]  eta: 11:33:19  loss: 1.0874 (1.1503)  acc1: 75.0000 (75.6963)  acc5: 87.5000 (91.4563)  time: 30.7734  data: 0.0046
Test:  [1950/3316]  eta: 11:08:09  loss: 1.0613 (1.1501)  acc1: 75.0000 (75.7112)  acc5: 87.5000 (91.4499)  time: 30.3715  data: 0.0044
Test:  [1900/3316]  eta: 11:51:02  loss: 1.1438 (1.1501)  acc1: 68.7500 (75.6937)  acc5: 87.5000 (91.4650)  time: 31.1518  data: 0.0042
Test:  [1900/3316]  eta: 11:51:07  loss: 1.1438 (1.1501)  acc1: 68.7500 (75.6937)  acc5: 87.5000 (91.4650)  time: 31.1629  data: 0.0046
Test:  [1910/3316]  eta: 11:42:38  loss: 1.1263 (1.1499)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4639)  time: 30.7436  data: 0.0045
Test:  [1900/3316]  eta: 11:51:23  loss: 1.1438 (1.1501)  acc1: 68.7500 (75.6937)  acc5: 87.5000 (91.4650)  time: 30.9521  data: 0.0047
Test:  [1950/3316]  eta: 11:09:04  loss: 1.0613 (1.1501)  acc1: 75.0000 (75.7112)  acc5: 87.5000 (91.4499)  time: 30.1331  data: 0.0039
Test:  [1910/3316]  eta: 11:44:31  loss: 1.1263 (1.1499)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4639)  time: 31.0191  data: 0.0036
Test:  [1930/3316]  eta: 11:28:21  loss: 1.0995 (1.1500)  acc1: 75.0000 (75.6991)  acc5: 87.5000 (91.4649)  time: 30.5784  data: 0.0033
Test:  [1960/3316]  eta: 11:03:16  loss: 1.2054 (1.1507)  acc1: 75.0000 (75.6948)  acc5: 87.5000 (91.4329)  time: 30.1796  data: 0.0036
Test:  [1910/3316]  eta: 11:45:59  loss: 1.1263 (1.1499)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4639)  time: 30.8362  data: 0.0033
Test:  [1910/3316]  eta: 11:46:04  loss: 1.1263 (1.1499)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4639)  time: 30.8618  data: 0.0037
Test:  [1920/3316]  eta: 11:37:36  loss: 1.0874 (1.1503)  acc1: 75.0000 (75.6963)  acc5: 87.5000 (91.4563)  time: 30.5871  data: 0.0034
Test:  [1910/3316]  eta: 11:46:18  loss: 1.1263 (1.1499)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4639)  time: 30.6359  data: 0.0036
Test:  [1960/3316]  eta: 11:04:07  loss: 1.2054 (1.1507)  acc1: 75.0000 (75.6948)  acc5: 87.5000 (91.4329)  time: 29.5760  data: 0.0033
Test:  [1920/3316]  eta: 11:39:26  loss: 1.0874 (1.1503)  acc1: 75.0000 (75.6963)  acc5: 87.5000 (91.4563)  time: 29.8548  data: 0.0032
Test:  [1940/3316]  eta: 11:23:18  loss: 1.0613 (1.1497)  acc1: 75.0000 (75.7148)  acc5: 87.5000 (91.4541)  time: 29.4625  data: 0.0040
Test:  [1970/3316]  eta: 10:58:18  loss: 1.1322 (1.1505)  acc1: 75.0000 (75.7040)  acc5: 93.7500 (91.4384)  time: 28.9926  data: 0.0038
Test:  [1920/3316]  eta: 11:40:52  loss: 1.0874 (1.1503)  acc1: 75.0000 (75.6963)  acc5: 87.5000 (91.4563)  time: 29.5626  data: 0.0039
Test:  [1920/3316]  eta: 11:40:58  loss: 1.0874 (1.1503)  acc1: 75.0000 (75.6963)  acc5: 87.5000 (91.4563)  time: 29.6665  data: 0.0039
Test:  [1930/3316]  eta: 11:32:31  loss: 1.0995 (1.1500)  acc1: 75.0000 (75.6991)  acc5: 87.5000 (91.4649)  time: 29.5192  data: 0.0039
Test:  [1920/3316]  eta: 11:41:11  loss: 1.0874 (1.1503)  acc1: 75.0000 (75.6963)  acc5: 87.5000 (91.4563)  time: 29.5152  data: 0.0039
Test:  [1970/3316]  eta: 10:59:08  loss: 1.1322 (1.1505)  acc1: 75.0000 (75.7040)  acc5: 93.7500 (91.4384)  time: 28.8025  data: 0.0038
Test:  [1930/3316]  eta: 11:34:17  loss: 1.0995 (1.1500)  acc1: 75.0000 (75.6991)  acc5: 87.5000 (91.4649)  time: 29.2175  data: 0.0035
Test:  [1950/3316]  eta: 11:18:12  loss: 1.0613 (1.1501)  acc1: 75.0000 (75.7112)  acc5: 87.5000 (91.4499)  time: 28.8640  data: 0.0039
Test:  [1980/3316]  eta: 10:53:16  loss: 1.0883 (1.1506)  acc1: 81.2500 (75.6878)  acc5: 93.7500 (91.4374)  time: 28.4404  data: 0.0035
Test:  [1930/3316]  eta: 11:35:40  loss: 1.0995 (1.1500)  acc1: 75.0000 (75.6991)  acc5: 87.5000 (91.4649)  time: 28.9848  data: 0.0035
Test:  [1930/3316]  eta: 11:35:47  loss: 1.0995 (1.1500)  acc1: 75.0000 (75.6991)  acc5: 87.5000 (91.4649)  time: 29.1157  data: 0.0035
Test:  [1940/3316]  eta: 11:27:23  loss: 1.0613 (1.1497)  acc1: 75.0000 (75.7148)  acc5: 87.5000 (91.4541)  time: 29.0278  data: 0.0035
Test:  [1930/3316]  eta: 11:36:00  loss: 1.0995 (1.1500)  acc1: 75.0000 (75.6991)  acc5: 87.5000 (91.4649)  time: 29.0584  data: 0.0035
Test:  [1980/3316]  eta: 10:54:06  loss: 1.0883 (1.1506)  acc1: 81.2500 (75.6878)  acc5: 93.7500 (91.4374)  time: 28.4029  data: 0.0034
Test:  [1940/3316]  eta: 11:29:08  loss: 1.0613 (1.1497)  acc1: 75.0000 (75.7148)  acc5: 87.5000 (91.4541)  time: 28.8163  data: 0.0030
Test:  [1960/3316]  eta: 11:13:05  loss: 1.2054 (1.1507)  acc1: 75.0000 (75.6948)  acc5: 87.5000 (91.4329)  time: 28.4919  data: 0.0029
Test:  [1990/3316]  eta: 10:48:16  loss: 1.0180 (1.1498)  acc1: 81.2500 (75.7094)  acc5: 93.7500 (91.4584)  time: 28.2351  data: 0.0026
Test:  [1940/3316]  eta: 11:30:30  loss: 1.0613 (1.1497)  acc1: 75.0000 (75.7148)  acc5: 87.5000 (91.4541)  time: 28.7851  data: 0.0023
Test:  [1940/3316]  eta: 11:30:37  loss: 1.0613 (1.1497)  acc1: 75.0000 (75.7148)  acc5: 87.5000 (91.4541)  time: 28.8408  data: 0.0024
Test:  [1950/3316]  eta: 11:22:15  loss: 1.0613 (1.1501)  acc1: 75.0000 (75.7112)  acc5: 87.5000 (91.4499)  time: 28.7934  data: 0.0023
Test:  [1940/3316]  eta: 11:30:50  loss: 1.0613 (1.1497)  acc1: 75.0000 (75.7148)  acc5: 87.5000 (91.4541)  time: 28.8516  data: 0.0025
Test:  [1990/3316]  eta: 10:49:04  loss: 1.0180 (1.1498)  acc1: 81.2500 (75.7094)  acc5: 93.7500 (91.4584)  time: 28.1480  data: 0.0024
Test:  [1950/3316]  eta: 11:24:02  loss: 1.0613 (1.1501)  acc1: 75.0000 (75.7112)  acc5: 87.5000 (91.4499)  time: 29.0163  data: 0.0024
Test:  [2000/3316]  eta: 10:43:22  loss: 1.0228 (1.1502)  acc1: 75.0000 (75.7059)  acc5: 93.7500 (91.4543)  time: 28.7387  data: 0.0028
Test:  [1970/3316]  eta: 11:08:05  loss: 1.1322 (1.1505)  acc1: 75.0000 (75.7040)  acc5: 93.7500 (91.4384)  time: 29.0149  data: 0.0032
Test:  [1950/3316]  eta: 11:25:29  loss: 1.0613 (1.1501)  acc1: 75.0000 (75.7112)  acc5: 87.5000 (91.4499)  time: 29.4859  data: 0.0033
Test:  [1950/3316]  eta: 11:25:35  loss: 1.0613 (1.1501)  acc1: 75.0000 (75.7112)  acc5: 87.5000 (91.4499)  time: 29.4470  data: 0.0038
Test:  [1960/3316]  eta: 11:17:15  loss: 1.2054 (1.1507)  acc1: 75.0000 (75.6948)  acc5: 87.5000 (91.4329)  time: 29.3690  data: 0.0032
Test:  [2000/3316]  eta: 10:44:10  loss: 1.0228 (1.1502)  acc1: 75.0000 (75.7059)  acc5: 93.7500 (91.4543)  time: 28.7145  data: 0.0034
Test:  [1950/3316]  eta: 11:25:47  loss: 1.0613 (1.1501)  acc1: 75.0000 (75.7112)  acc5: 87.5000 (91.4499)  time: 29.4287  data: 0.0034
Test:  [1960/3316]  eta: 11:19:02  loss: 1.2054 (1.1507)  acc1: 75.0000 (75.6948)  acc5: 87.5000 (91.4329)  time: 29.6900  data: 0.0039
Test:  [2010/3316]  eta: 10:38:24  loss: 1.0228 (1.1491)  acc1: 81.2500 (75.7366)  acc5: 93.7500 (91.4688)  time: 28.9567  data: 0.0037
Test:  [1980/3316]  eta: 11:03:03  loss: 1.0883 (1.1506)  acc1: 81.2500 (75.6878)  acc5: 93.7500 (91.4374)  time: 29.3593  data: 0.0042
Test:  [1960/3316]  eta: 11:20:25  loss: 1.2054 (1.1507)  acc1: 75.0000 (75.6948)  acc5: 87.5000 (91.4329)  time: 29.8637  data: 0.0043
Test:  [1960/3316]  eta: 11:20:30  loss: 1.2054 (1.1507)  acc1: 75.0000 (75.6948)  acc5: 87.5000 (91.4329)  time: 29.7538  data: 0.0045
Test:  [2010/3316]  eta: 10:39:12  loss: 1.0228 (1.1491)  acc1: 81.2500 (75.7366)  acc5: 93.7500 (91.4688)  time: 28.9925  data: 0.0040
Test:  [1970/3316]  eta: 11:12:11  loss: 1.1322 (1.1505)  acc1: 75.0000 (75.7040)  acc5: 93.7500 (91.4384)  time: 29.5807  data: 0.0041
Test:  [1960/3316]  eta: 11:20:42  loss: 1.2054 (1.1507)  acc1: 75.0000 (75.6948)  acc5: 87.5000 (91.4329)  time: 29.7446  data: 0.0042
Test:  [1970/3316]  eta: 11:13:59  loss: 1.1322 (1.1505)  acc1: 75.0000 (75.7040)  acc5: 93.7500 (91.4384)  time: 29.8983  data: 0.0040
Test:  [2020/3316]  eta: 10:33:28  loss: 0.9632 (1.1498)  acc1: 81.2500 (75.7329)  acc5: 93.7500 (91.4615)  time: 28.7735  data: 0.0036
Test:  [1990/3316]  eta: 10:58:02  loss: 1.0180 (1.1498)  acc1: 81.2500 (75.7094)  acc5: 93.7500 (91.4584)  time: 29.2017  data: 0.0039
Test:  [1970/3316]  eta: 11:15:20  loss: 1.1322 (1.1505)  acc1: 75.0000 (75.7040)  acc5: 93.7500 (91.4384)  time: 29.6604  data: 0.0035
Test:  [1970/3316]  eta: 11:15:25  loss: 1.1322 (1.1505)  acc1: 75.0000 (75.7040)  acc5: 93.7500 (91.4384)  time: 29.5320  data: 0.0033
Test:  [2020/3316]  eta: 10:34:15  loss: 0.9632 (1.1498)  acc1: 81.2500 (75.7329)  acc5: 93.7500 (91.4615)  time: 28.8085  data: 0.0033
Test:  [1980/3316]  eta: 11:07:07  loss: 1.0883 (1.1506)  acc1: 81.2500 (75.6878)  acc5: 93.7500 (91.4374)  time: 29.3250  data: 0.0034
Test:  [1970/3316]  eta: 11:15:38  loss: 1.1322 (1.1505)  acc1: 75.0000 (75.7040)  acc5: 93.7500 (91.4384)  time: 29.6137  data: 0.0034
Test:  [1980/3316]  eta: 11:08:56  loss: 1.0883 (1.1506)  acc1: 81.2500 (75.6878)  acc5: 93.7500 (91.4374)  time: 29.7325  data: 0.0027
Test:  [2030/3316]  eta: 10:28:32  loss: 1.3026 (1.1504)  acc1: 68.7500 (75.7170)  acc5: 93.7500 (91.4543)  time: 28.8689  data: 0.0030
Test:  [2000/3316]  eta: 10:53:02  loss: 1.0228 (1.1502)  acc1: 75.0000 (75.7059)  acc5: 93.7500 (91.4543)  time: 29.3184  data: 0.0031
Test:  [2030/3316]  eta: 10:29:20  loss: 1.3026 (1.1504)  acc1: 68.7500 (75.7170)  acc5: 93.7500 (91.4543)  time: 28.9566  data: 0.0029
Test:  [1980/3316]  eta: 11:10:18  loss: 1.0883 (1.1506)  acc1: 81.2500 (75.6878)  acc5: 93.7500 (91.4374)  time: 29.8081  data: 0.0028
Test:  [1980/3316]  eta: 11:10:21  loss: 1.0883 (1.1506)  acc1: 81.2500 (75.6878)  acc5: 93.7500 (91.4374)  time: 29.6255  data: 0.0030
Test:  [1990/3316]  eta: 11:02:05  loss: 1.0180 (1.1498)  acc1: 81.2500 (75.7094)  acc5: 93.7500 (91.4584)  time: 29.5005  data: 0.0030
Test:  [1980/3316]  eta: 11:10:34  loss: 1.0883 (1.1506)  acc1: 81.2500 (75.6878)  acc5: 93.7500 (91.4374)  time: 29.7156  data: 0.0030
Test:  [1990/3316]  eta: 11:04:02  loss: 1.0180 (1.1498)  acc1: 81.2500 (75.7094)  acc5: 93.7500 (91.4584)  time: 30.3966  data: 0.0031
Test:  [2040/3316]  eta: 10:23:49  loss: 1.3026 (1.1515)  acc1: 68.7500 (75.6921)  acc5: 87.5000 (91.4380)  time: 29.9419  data: 0.0032
Test:  [2010/3316]  eta: 10:48:15  loss: 1.0228 (1.1491)  acc1: 81.2500 (75.7366)  acc5: 93.7500 (91.4688)  time: 30.4288  data: 0.0044
Test:  [2040/3316]  eta: 10:24:40  loss: 1.3026 (1.1515)  acc1: 68.7500 (75.6921)  acc5: 87.5000 (91.4380)  time: 30.3330  data: 0.0046
Test:  [1990/3316]  eta: 11:05:34  loss: 1.0180 (1.1498)  acc1: 81.2500 (75.7094)  acc5: 93.7500 (91.4584)  time: 31.2962  data: 0.0043
Test:  [1990/3316]  eta: 11:05:36  loss: 1.0180 (1.1498)  acc1: 81.2500 (75.7094)  acc5: 93.7500 (91.4584)  time: 31.1226  data: 0.0042
Test:  [2000/3316]  eta: 10:57:22  loss: 1.0228 (1.1502)  acc1: 75.0000 (75.7059)  acc5: 93.7500 (91.4543)  time: 31.0723  data: 0.0048
Test:  [1990/3316]  eta: 11:05:51  loss: 1.0180 (1.1498)  acc1: 81.2500 (75.7094)  acc5: 93.7500 (91.4584)  time: 31.2521  data: 0.0051
Test:  [2000/3316]  eta: 10:59:20  loss: 1.0228 (1.1502)  acc1: 75.0000 (75.7059)  acc5: 93.7500 (91.4543)  time: 31.8758  data: 0.0059
Test:  [2050/3316]  eta: 10:19:07  loss: 1.1824 (1.1518)  acc1: 75.0000 (75.6948)  acc5: 93.7500 (91.4432)  time: 31.1100  data: 0.0057
Test:  [2020/3316]  eta: 10:43:29  loss: 0.9632 (1.1498)  acc1: 81.2500 (75.7329)  acc5: 93.7500 (91.4615)  time: 31.5683  data: 0.0064
Test:  [2050/3316]  eta: 10:19:53  loss: 1.1824 (1.1518)  acc1: 75.0000 (75.6948)  acc5: 93.7500 (91.4432)  time: 31.0265  data: 0.0061
Test:  [2000/3316]  eta: 11:00:42  loss: 1.0228 (1.1502)  acc1: 75.0000 (75.7059)  acc5: 93.7500 (91.4543)  time: 32.0566  data: 0.0058
Test:  [2000/3316]  eta: 11:00:42  loss: 1.0228 (1.1502)  acc1: 75.0000 (75.7059)  acc5: 93.7500 (91.4543)  time: 31.8709  data: 0.0053
Test:  [2010/3316]  eta: 10:52:29  loss: 1.0228 (1.1491)  acc1: 81.2500 (75.7366)  acc5: 93.7500 (91.4688)  time: 31.7369  data: 0.0058
Test:  [2000/3316]  eta: 11:00:55  loss: 1.0228 (1.1502)  acc1: 75.0000 (75.7059)  acc5: 93.7500 (91.4543)  time: 31.8587  data: 0.0062
Test:  [2010/3316]  eta: 10:54:16  loss: 1.0228 (1.1491)  acc1: 81.2500 (75.7366)  acc5: 93.7500 (91.4688)  time: 31.1308  data: 0.0056
Test:  [2060/3316]  eta: 10:14:11  loss: 1.0943 (1.1513)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4483)  time: 30.0116  data: 0.0053
Test:  [2030/3316]  eta: 10:38:26  loss: 1.3026 (1.1504)  acc1: 68.7500 (75.7170)  acc5: 93.7500 (91.4543)  time: 30.3560  data: 0.0049
Test:  [2060/3316]  eta: 10:14:54  loss: 1.0943 (1.1513)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4483)  time: 29.4646  data: 0.0043
Test:  [2010/3316]  eta: 10:55:35  loss: 1.0228 (1.1491)  acc1: 81.2500 (75.7366)  acc5: 93.7500 (91.4688)  time: 30.1805  data: 0.0040
Test:  [2010/3316]  eta: 10:55:35  loss: 1.0228 (1.1491)  acc1: 81.2500 (75.7366)  acc5: 93.7500 (91.4688)  time: 30.3803  data: 0.0041
Test:  [2020/3316]  eta: 10:47:22  loss: 0.9632 (1.1498)  acc1: 81.2500 (75.7329)  acc5: 93.7500 (91.4615)  time: 29.9198  data: 0.0042
Test:  [2010/3316]  eta: 10:55:47  loss: 1.0228 (1.1491)  acc1: 81.2500 (75.7366)  acc5: 93.7500 (91.4688)  time: 30.0485  data: 0.0039
Test:  [2020/3316]  eta: 10:49:07  loss: 0.9632 (1.1498)  acc1: 81.2500 (75.7329)  acc5: 93.7500 (91.4615)  time: 29.1996  data: 0.0027
Test:  [2070/3316]  eta: 10:09:11  loss: 0.9967 (1.1499)  acc1: 81.2500 (75.7514)  acc5: 93.7500 (91.4655)  time: 28.5082  data: 0.0026
Test:  [2040/3316]  eta: 10:33:20  loss: 1.3026 (1.1515)  acc1: 68.7500 (75.6921)  acc5: 87.5000 (91.4380)  time: 28.7337  data: 0.0026
Test:  [2070/3316]  eta: 10:09:53  loss: 0.9967 (1.1499)  acc1: 81.2500 (75.7514)  acc5: 93.7500 (91.4655)  time: 28.3516  data: 0.0027
Test:  [2020/3316]  eta: 10:50:25  loss: 0.9632 (1.1498)  acc1: 81.2500 (75.7329)  acc5: 93.7500 (91.4615)  time: 29.0006  data: 0.0026
Test:  [2030/3316]  eta: 10:42:15  loss: 1.3026 (1.1504)  acc1: 68.7500 (75.7170)  acc5: 93.7500 (91.4543)  time: 28.8230  data: 0.0027
Test:  [2020/3316]  eta: 10:50:27  loss: 0.9632 (1.1498)  acc1: 81.2500 (75.7329)  acc5: 93.7500 (91.4615)  time: 29.1432  data: 0.0025
Test:  [2020/3316]  eta: 10:50:37  loss: 0.9632 (1.1498)  acc1: 81.2500 (75.7329)  acc5: 93.7500 (91.4615)  time: 28.9707  data: 0.0026
Test:  [2030/3316]  eta: 10:43:59  loss: 1.3026 (1.1504)  acc1: 68.7500 (75.7170)  acc5: 93.7500 (91.4543)  time: 28.8219  data: 0.0024
Test:  [2080/3316]  eta: 10:04:10  loss: 0.9609 (1.1491)  acc1: 81.2500 (75.7749)  acc5: 93.7500 (91.4704)  time: 28.2014  data: 0.0025
Test:  [2050/3316]  eta: 10:28:14  loss: 1.1824 (1.1518)  acc1: 75.0000 (75.6948)  acc5: 93.7500 (91.4432)  time: 28.4261  data: 0.0023
Test:  [2080/3316]  eta: 10:04:55  loss: 0.9609 (1.1491)  acc1: 81.2500 (75.7749)  acc5: 93.7500 (91.4704)  time: 28.4285  data: 0.0032
Test:  [2030/3316]  eta: 10:45:19  loss: 1.3026 (1.1504)  acc1: 68.7500 (75.7170)  acc5: 93.7500 (91.4543)  time: 29.0902  data: 0.0032
Test:  [2040/3316]  eta: 10:37:10  loss: 1.3026 (1.1515)  acc1: 68.7500 (75.6921)  acc5: 87.5000 (91.4380)  time: 28.9645  data: 0.0030
Test:  [2030/3316]  eta: 10:45:22  loss: 1.3026 (1.1504)  acc1: 68.7500 (75.7170)  acc5: 93.7500 (91.4543)  time: 29.2347  data: 0.0030
Test:  [2030/3316]  eta: 10:45:33  loss: 1.3026 (1.1504)  acc1: 68.7500 (75.7170)  acc5: 93.7500 (91.4543)  time: 29.1859  data: 0.0034
Test:  [2040/3316]  eta: 10:38:57  loss: 1.3026 (1.1515)  acc1: 68.7500 (75.6921)  acc5: 87.5000 (91.4380)  time: 29.2871  data: 0.0043
Test:  [2090/3316]  eta: 9:59:17  loss: 0.9998 (1.1498)  acc1: 75.0000 (75.7622)  acc5: 93.7500 (91.4574)  time: 28.7292  data: 0.0044
Test:  [2060/3316]  eta: 10:23:16  loss: 1.0943 (1.1513)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4483)  time: 29.0687  data: 0.0039
Test:  [2090/3316]  eta: 9:59:59  loss: 0.9998 (1.1498)  acc1: 75.0000 (75.7622)  acc5: 93.7500 (91.4574)  time: 28.7878  data: 0.0047
Test:  [2040/3316]  eta: 10:40:16  loss: 1.3026 (1.1515)  acc1: 68.7500 (75.6921)  acc5: 87.5000 (91.4380)  time: 29.5100  data: 0.0045
Test:  [2050/3316]  eta: 10:32:09  loss: 1.1824 (1.1518)  acc1: 75.0000 (75.6948)  acc5: 93.7500 (91.4432)  time: 29.4077  data: 0.0043
Test:  [2040/3316]  eta: 10:40:19  loss: 1.3026 (1.1515)  acc1: 68.7500 (75.6921)  acc5: 87.5000 (91.4380)  time: 29.6463  data: 0.0044
Test:  [2040/3316]  eta: 10:40:30  loss: 1.3026 (1.1515)  acc1: 68.7500 (75.6921)  acc5: 87.5000 (91.4380)  time: 29.7577  data: 0.0046
Test:  [2100/3316]  eta: 9:54:24  loss: 1.0153 (1.1497)  acc1: 81.2500 (75.7586)  acc5: 93.7500 (91.4654)  time: 29.3189  data: 0.0045
Test:  [2050/3316]  eta: 10:33:56  loss: 1.1824 (1.1518)  acc1: 75.0000 (75.6948)  acc5: 93.7500 (91.4432)  time: 29.9021  data: 0.0048
Test:  [2070/3316]  eta: 10:18:18  loss: 0.9967 (1.1499)  acc1: 81.2500 (75.7514)  acc5: 93.7500 (91.4655)  time: 29.7022  data: 0.0044
Test:  [2100/3316]  eta: 9:55:04  loss: 1.0153 (1.1497)  acc1: 81.2500 (75.7586)  acc5: 93.7500 (91.4654)  time: 29.0259  data: 0.0042
Test:  [2060/3316]  eta: 10:27:08  loss: 1.0943 (1.1513)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4483)  time: 29.7171  data: 0.0040
Test:  [2050/3316]  eta: 10:35:13  loss: 1.1824 (1.1518)  acc1: 75.0000 (75.6948)  acc5: 93.7500 (91.4432)  time: 29.8195  data: 0.0041
Test:  [2050/3316]  eta: 10:35:18  loss: 1.1824 (1.1518)  acc1: 75.0000 (75.6948)  acc5: 93.7500 (91.4432)  time: 29.9554  data: 0.0043
Test:  [2050/3316]  eta: 10:35:28  loss: 1.1824 (1.1518)  acc1: 75.0000 (75.6948)  acc5: 93.7500 (91.4432)  time: 29.9287  data: 0.0040
Test:  [2110/3316]  eta: 9:49:29  loss: 1.1031 (1.1499)  acc1: 75.0000 (75.7431)  acc5: 93.7500 (91.4614)  time: 29.1851  data: 0.0026
Test:  [2060/3316]  eta: 10:28:55  loss: 1.0943 (1.1513)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4483)  time: 29.9582  data: 0.0031
Test:  [2080/3316]  eta: 10:13:17  loss: 0.9609 (1.1491)  acc1: 81.2500 (75.7749)  acc5: 93.7500 (91.4704)  time: 29.4816  data: 0.0031
Test:  [2110/3316]  eta: 9:50:07  loss: 1.1031 (1.1499)  acc1: 75.0000 (75.7431)  acc5: 93.7500 (91.4614)  time: 28.9745  data: 0.0028
Test:  [2070/3316]  eta: 10:22:04  loss: 0.9967 (1.1499)  acc1: 81.2500 (75.7514)  acc5: 93.7500 (91.4655)  time: 29.5428  data: 0.0030
Test:  [2060/3316]  eta: 10:30:09  loss: 1.0943 (1.1513)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4483)  time: 29.7404  data: 0.0029
Test:  [2060/3316]  eta: 10:30:14  loss: 1.0943 (1.1513)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4483)  time: 29.9172  data: 0.0031
Test:  [2060/3316]  eta: 10:30:23  loss: 1.0943 (1.1513)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4483)  time: 29.7324  data: 0.0028
Test:  [2120/3316]  eta: 9:44:33  loss: 1.1069 (1.1500)  acc1: 68.7500 (75.7278)  acc5: 93.7500 (91.4751)  time: 28.9796  data: 0.0025
Test:  [2070/3316]  eta: 10:23:52  loss: 0.9967 (1.1499)  acc1: 81.2500 (75.7514)  acc5: 93.7500 (91.4655)  time: 29.7653  data: 0.0028
Test:  [2090/3316]  eta: 10:08:16  loss: 0.9998 (1.1498)  acc1: 75.0000 (75.7622)  acc5: 93.7500 (91.4574)  time: 29.2716  data: 0.0030
Test:  [2120/3316]  eta: 9:45:10  loss: 1.1069 (1.1500)  acc1: 68.7500 (75.7278)  acc5: 93.7500 (91.4751)  time: 28.7820  data: 0.0027
Test:  [2080/3316]  eta: 10:17:03  loss: 0.9609 (1.1491)  acc1: 81.2500 (75.7749)  acc5: 93.7500 (91.4704)  time: 29.4302  data: 0.0029
Test:  [2070/3316]  eta: 10:25:06  loss: 0.9967 (1.1499)  acc1: 81.2500 (75.7514)  acc5: 93.7500 (91.4655)  time: 29.6608  data: 0.0029
Test:  [2070/3316]  eta: 10:25:12  loss: 0.9967 (1.1499)  acc1: 81.2500 (75.7514)  acc5: 93.7500 (91.4655)  time: 29.8129  data: 0.0029
Test:  [2070/3316]  eta: 10:25:21  loss: 0.9967 (1.1499)  acc1: 81.2500 (75.7514)  acc5: 93.7500 (91.4655)  time: 29.7079  data: 0.0029
Test:  [2130/3316]  eta: 9:39:40  loss: 1.0476 (1.1496)  acc1: 75.0000 (75.7303)  acc5: 93.7500 (91.4946)  time: 29.1674  data: 0.0028
Test:  [2080/3316]  eta: 10:18:52  loss: 0.9609 (1.1491)  acc1: 81.2500 (75.7749)  acc5: 93.7500 (91.4704)  time: 29.9190  data: 0.0029
Test:  [2100/3316]  eta: 10:03:22  loss: 1.0153 (1.1497)  acc1: 81.2500 (75.7586)  acc5: 93.7500 (91.4654)  time: 29.8203  data: 0.0030
Test:  [2130/3316]  eta: 9:40:20  loss: 1.0476 (1.1496)  acc1: 75.0000 (75.7303)  acc5: 93.7500 (91.4946)  time: 29.3344  data: 0.0029
Test:  [2090/3316]  eta: 10:12:06  loss: 0.9998 (1.1498)  acc1: 75.0000 (75.7622)  acc5: 93.7500 (91.4574)  time: 30.0064  data: 0.0041
Test:  [2080/3316]  eta: 10:20:09  loss: 0.9609 (1.1491)  acc1: 81.2500 (75.7749)  acc5: 93.7500 (91.4704)  time: 30.2675  data: 0.0040
Test:  [2080/3316]  eta: 10:20:14  loss: 0.9609 (1.1491)  acc1: 81.2500 (75.7749)  acc5: 93.7500 (91.4704)  time: 30.2940  data: 0.0039
Test:  [2080/3316]  eta: 10:20:23  loss: 0.9609 (1.1491)  acc1: 81.2500 (75.7749)  acc5: 93.7500 (91.4704)  time: 30.3214  data: 0.0044
Test:  [2140/3316]  eta: 9:34:49  loss: 1.0492 (1.1495)  acc1: 75.0000 (75.7356)  acc5: 93.7500 (91.4993)  time: 29.5223  data: 0.0048
Test:  [2090/3316]  eta: 10:13:55  loss: 0.9998 (1.1498)  acc1: 75.0000 (75.7622)  acc5: 93.7500 (91.4574)  time: 30.3496  data: 0.0047
Test:  [2110/3316]  eta: 9:58:24  loss: 1.1031 (1.1499)  acc1: 75.0000 (75.7431)  acc5: 93.7500 (91.4614)  time: 30.0411  data: 0.0043
Test:  [2140/3316]  eta: 9:35:25  loss: 1.0492 (1.1495)  acc1: 75.0000 (75.7356)  acc5: 93.7500 (91.4993)  time: 29.5574  data: 0.0048
Test:  [2100/3316]  eta: 10:07:05  loss: 1.0153 (1.1497)  acc1: 81.2500 (75.7586)  acc5: 93.7500 (91.4654)  time: 30.0944  data: 0.0051
Test:  [2090/3316]  eta: 10:15:06  loss: 0.9998 (1.1498)  acc1: 75.0000 (75.7622)  acc5: 93.7500 (91.4574)  time: 30.2713  data: 0.0049
Test:  [2090/3316]  eta: 10:15:12  loss: 0.9998 (1.1498)  acc1: 75.0000 (75.7622)  acc5: 93.7500 (91.4574)  time: 30.2709  data: 0.0050
Test:  [2090/3316]  eta: 10:15:20  loss: 0.9998 (1.1498)  acc1: 75.0000 (75.7622)  acc5: 93.7500 (91.4574)  time: 30.2549  data: 0.0052
Test:  [2150/3316]  eta: 9:29:51  loss: 1.0492 (1.1493)  acc1: 75.0000 (75.7351)  acc5: 93.7500 (91.4923)  time: 29.0988  data: 0.0048
Test:  [2100/3316]  eta: 10:08:49  loss: 1.0153 (1.1497)  acc1: 81.2500 (75.7586)  acc5: 93.7500 (91.4654)  time: 29.7977  data: 0.0043
Test:  [2120/3316]  eta: 9:53:21  loss: 1.1069 (1.1500)  acc1: 68.7500 (75.7278)  acc5: 93.7500 (91.4751)  time: 29.2631  data: 0.0041
Test:  [2150/3316]  eta: 9:30:26  loss: 1.0492 (1.1493)  acc1: 75.0000 (75.7351)  acc5: 93.7500 (91.4923)  time: 28.7363  data: 0.0046
Test:  [2110/3316]  eta: 10:01:58  loss: 1.1031 (1.1499)  acc1: 75.0000 (75.7431)  acc5: 93.7500 (91.4614)  time: 29.2052  data: 0.0037
Test:  [2100/3316]  eta: 10:09:57  loss: 1.0153 (1.1497)  acc1: 81.2500 (75.7586)  acc5: 93.7500 (91.4654)  time: 29.2941  data: 0.0035
Test:  [2100/3316]  eta: 10:10:04  loss: 1.0153 (1.1497)  acc1: 81.2500 (75.7586)  acc5: 93.7500 (91.4654)  time: 29.3897  data: 0.0037
Test:  [2100/3316]  eta: 10:10:11  loss: 1.0153 (1.1497)  acc1: 81.2500 (75.7586)  acc5: 93.7500 (91.4654)  time: 29.2890  data: 0.0036
Test:  [2160/3316]  eta: 9:24:52  loss: 1.0057 (1.1501)  acc1: 75.0000 (75.7144)  acc5: 93.7500 (91.4767)  time: 28.3413  data: 0.0026
Test:  [2110/3316]  eta: 10:03:42  loss: 1.1031 (1.1499)  acc1: 75.0000 (75.7431)  acc5: 93.7500 (91.4614)  time: 29.0093  data: 0.0024
Test:  [2160/3316]  eta: 9:25:25  loss: 1.0057 (1.1501)  acc1: 75.0000 (75.7144)  acc5: 93.7500 (91.4767)  time: 28.1692  data: 0.0025
Test:  [2130/3316]  eta: 9:48:16  loss: 1.0476 (1.1496)  acc1: 75.0000 (75.7303)  acc5: 93.7500 (91.4946)  time: 28.6708  data: 0.0025
Test:  [2120/3316]  eta: 9:56:52  loss: 1.1069 (1.1500)  acc1: 68.7500 (75.7278)  acc5: 93.7500 (91.4751)  time: 28.6947  data: 0.0024
Test:  [2110/3316]  eta: 10:04:51  loss: 1.1031 (1.1499)  acc1: 75.0000 (75.7431)  acc5: 93.7500 (91.4614)  time: 28.9524  data: 0.0024
Test:  [2110/3316]  eta: 10:04:58  loss: 1.1031 (1.1499)  acc1: 75.0000 (75.7431)  acc5: 93.7500 (91.4614)  time: 29.1095  data: 0.0024
Test:  [2110/3316]  eta: 10:05:05  loss: 1.1031 (1.1499)  acc1: 75.0000 (75.7431)  acc5: 93.7500 (91.4614)  time: 28.9936  data: 0.0025
Test:  [2170/3316]  eta: 9:19:55  loss: 1.0264 (1.1505)  acc1: 75.0000 (75.7024)  acc5: 93.7500 (91.4757)  time: 28.4142  data: 0.0023
Test:  [2120/3316]  eta: 9:58:38  loss: 1.1069 (1.1500)  acc1: 68.7500 (75.7278)  acc5: 93.7500 (91.4751)  time: 29.1664  data: 0.0025
Test:  [2170/3316]  eta: 9:20:28  loss: 1.0264 (1.1505)  acc1: 75.0000 (75.7024)  acc5: 93.7500 (91.4757)  time: 28.3885  data: 0.0025
Test:  [2140/3316]  eta: 9:43:16  loss: 1.0492 (1.1495)  acc1: 75.0000 (75.7356)  acc5: 93.7500 (91.4993)  time: 28.9018  data: 0.0024
Test:  [2130/3316]  eta: 9:51:50  loss: 1.0476 (1.1496)  acc1: 75.0000 (75.7303)  acc5: 93.7500 (91.4946)  time: 29.1322  data: 0.0026
Test:  [2120/3316]  eta: 9:59:46  loss: 1.1069 (1.1500)  acc1: 68.7500 (75.7278)  acc5: 93.7500 (91.4751)  time: 29.2999  data: 0.0028
Test:  [2120/3316]  eta: 9:59:54  loss: 1.1069 (1.1500)  acc1: 68.7500 (75.7278)  acc5: 93.7500 (91.4751)  time: 29.4240  data: 0.0026
Test:  [2120/3316]  eta: 10:00:00  loss: 1.1069 (1.1500)  acc1: 68.7500 (75.7278)  acc5: 93.7500 (91.4751)  time: 29.2945  data: 0.0026
Test:  [2180/3316]  eta: 9:14:59  loss: 1.0323 (1.1503)  acc1: 75.0000 (75.6992)  acc5: 93.7500 (91.4775)  time: 28.7033  data: 0.0026
Test:  [2130/3316]  eta: 9:53:34  loss: 1.0476 (1.1496)  acc1: 75.0000 (75.7303)  acc5: 93.7500 (91.4946)  time: 29.4235  data: 0.0027
Test:  [2180/3316]  eta: 9:15:31  loss: 1.0323 (1.1503)  acc1: 75.0000 (75.6992)  acc5: 93.7500 (91.4775)  time: 28.7024  data: 0.0027
Test:  [2150/3316]  eta: 9:38:15  loss: 1.0492 (1.1493)  acc1: 75.0000 (75.7351)  acc5: 93.7500 (91.4923)  time: 29.1690  data: 0.0029
Test:  [2140/3316]  eta: 9:46:47  loss: 1.0492 (1.1495)  acc1: 75.0000 (75.7356)  acc5: 93.7500 (91.4993)  time: 29.4342  data: 0.0028
Test:  [2130/3316]  eta: 9:54:42  loss: 1.0476 (1.1496)  acc1: 75.0000 (75.7303)  acc5: 93.7500 (91.4946)  time: 29.5047  data: 0.0029
Test:  [2130/3316]  eta: 9:54:49  loss: 1.0476 (1.1496)  acc1: 75.0000 (75.7303)  acc5: 93.7500 (91.4946)  time: 29.4996  data: 0.0028
Test:  [2130/3316]  eta: 9:54:56  loss: 1.0476 (1.1496)  acc1: 75.0000 (75.7303)  acc5: 93.7500 (91.4946)  time: 29.4637  data: 0.0026
Test:  [2190/3316]  eta: 9:10:03  loss: 0.8674 (1.1494)  acc1: 81.2500 (75.7246)  acc5: 93.7500 (91.4822)  time: 28.7313  data: 0.0028
Test:  [2190/3316]  eta: 9:10:39  loss: 0.8674 (1.1494)  acc1: 81.2500 (75.7246)  acc5: 93.7500 (91.4822)  time: 29.1546  data: 0.0035
Test:  [2140/3316]  eta: 9:48:36  loss: 1.0492 (1.1495)  acc1: 75.0000 (75.7356)  acc5: 93.7500 (91.4993)  time: 29.8820  data: 0.0034
Test:  [2160/3316]  eta: 9:33:19  loss: 1.0057 (1.1501)  acc1: 75.0000 (75.7144)  acc5: 93.7500 (91.4767)  time: 29.6111  data: 0.0034
Test:  [2150/3316]  eta: 9:41:51  loss: 1.0492 (1.1493)  acc1: 75.0000 (75.7351)  acc5: 93.7500 (91.4923)  time: 29.8776  data: 0.0039
Test:  [2140/3316]  eta: 9:49:44  loss: 1.0492 (1.1495)  acc1: 75.0000 (75.7356)  acc5: 93.7500 (91.4993)  time: 30.0315  data: 0.0040
Test:  [2140/3316]  eta: 9:49:51  loss: 1.0492 (1.1495)  acc1: 75.0000 (75.7356)  acc5: 93.7500 (91.4993)  time: 29.9875  data: 0.0042
Test:  [2140/3316]  eta: 9:49:57  loss: 1.0492 (1.1495)  acc1: 75.0000 (75.7356)  acc5: 93.7500 (91.4993)  time: 30.0313  data: 0.0041
Test:  [2200/3316]  eta: 9:05:12  loss: 0.9696 (1.1495)  acc1: 81.2500 (75.7269)  acc5: 93.7500 (91.4897)  time: 29.2360  data: 0.0045
Test:  [2200/3316]  eta: 9:05:44  loss: 0.9696 (1.1495)  acc1: 81.2500 (75.7269)  acc5: 93.7500 (91.4897)  time: 29.2654  data: 0.0049
Test:  [2150/3316]  eta: 9:43:33  loss: 1.0492 (1.1493)  acc1: 75.0000 (75.7351)  acc5: 93.7500 (91.4923)  time: 29.9669  data: 0.0048
Test:  [2170/3316]  eta: 9:28:19  loss: 1.0264 (1.1505)  acc1: 75.0000 (75.7024)  acc5: 93.7500 (91.4757)  time: 29.7544  data: 0.0045
Test:  [2160/3316]  eta: 9:36:50  loss: 1.0057 (1.1501)  acc1: 75.0000 (75.7144)  acc5: 93.7500 (91.4767)  time: 30.1002  data: 0.0047
Test:  [2150/3316]  eta: 9:44:41  loss: 1.0492 (1.1493)  acc1: 75.0000 (75.7351)  acc5: 93.7500 (91.4923)  time: 30.1219  data: 0.0047
Test:  [2150/3316]  eta: 9:44:50  loss: 1.0492 (1.1493)  acc1: 75.0000 (75.7351)  acc5: 93.7500 (91.4923)  time: 30.3253  data: 0.0048
Test:  [2150/3316]  eta: 9:44:57  loss: 1.0492 (1.1493)  acc1: 75.0000 (75.7351)  acc5: 93.7500 (91.4923)  time: 30.3778  data: 0.0049
Test:  [2210/3316]  eta: 9:00:19  loss: 1.0827 (1.1495)  acc1: 75.0000 (75.7350)  acc5: 93.7500 (91.4886)  time: 29.6009  data: 0.0047
Test:  [2210/3316]  eta: 9:00:51  loss: 1.0827 (1.1495)  acc1: 75.0000 (75.7350)  acc5: 93.7500 (91.4886)  time: 29.1903  data: 0.0046
Test:  [2160/3316]  eta: 9:38:33  loss: 1.0057 (1.1501)  acc1: 75.0000 (75.7144)  acc5: 93.7500 (91.4767)  time: 29.8482  data: 0.0044
Test:  [2180/3316]  eta: 9:23:23  loss: 1.0323 (1.1503)  acc1: 75.0000 (75.6992)  acc5: 93.7500 (91.4775)  time: 29.7018  data: 0.0045
Test:  [2170/3316]  eta: 9:31:54  loss: 1.0264 (1.1505)  acc1: 75.0000 (75.7024)  acc5: 93.7500 (91.4757)  time: 30.1256  data: 0.0040
Test:  [2160/3316]  eta: 9:39:42  loss: 1.0057 (1.1501)  acc1: 75.0000 (75.7144)  acc5: 93.7500 (91.4767)  time: 30.0801  data: 0.0037
Test:  [2160/3316]  eta: 9:39:52  loss: 1.0057 (1.1501)  acc1: 75.0000 (75.7144)  acc5: 93.7500 (91.4767)  time: 30.3214  data: 0.0037
Test:  [2160/3316]  eta: 9:39:59  loss: 1.0057 (1.1501)  acc1: 75.0000 (75.7144)  acc5: 93.7500 (91.4767)  time: 30.4153  data: 0.0038
Test:  [2220/3316]  eta: 8:55:28  loss: 1.2623 (1.1503)  acc1: 68.7500 (75.7148)  acc5: 87.5000 (91.4791)  time: 29.5367  data: 0.0031
Test:  [2220/3316]  eta: 8:55:59  loss: 1.2623 (1.1503)  acc1: 68.7500 (75.7148)  acc5: 87.5000 (91.4791)  time: 29.5311  data: 0.0035
Test:  [2170/3316]  eta: 9:33:33  loss: 1.0264 (1.1505)  acc1: 75.0000 (75.7024)  acc5: 93.7500 (91.4757)  time: 30.0697  data: 0.0032
Test:  [2190/3316]  eta: 9:18:27  loss: 0.8674 (1.1494)  acc1: 81.2500 (75.7246)  acc5: 93.7500 (91.4822)  time: 29.9674  data: 0.0033
Test:  [2180/3316]  eta: 9:26:54  loss: 1.0323 (1.1503)  acc1: 75.0000 (75.6992)  acc5: 93.7500 (91.4775)  time: 30.1824  data: 0.0034
Test:  [2170/3316]  eta: 9:34:39  loss: 1.0264 (1.1505)  acc1: 75.0000 (75.7024)  acc5: 93.7500 (91.4757)  time: 30.0419  data: 0.0032
Test:  [2170/3316]  eta: 9:34:49  loss: 1.0264 (1.1505)  acc1: 75.0000 (75.7024)  acc5: 93.7500 (91.4757)  time: 30.1325  data: 0.0031
Test:  [2170/3316]  eta: 9:34:55  loss: 1.0264 (1.1505)  acc1: 75.0000 (75.7024)  acc5: 93.7500 (91.4757)  time: 30.1386  data: 0.0033
Test:  [2230/3316]  eta: 8:50:32  loss: 1.3272 (1.1514)  acc1: 68.7500 (75.6779)  acc5: 87.5000 (91.4668)  time: 29.2170  data: 0.0028
Test:  [2230/3316]  eta: 8:51:02  loss: 1.3272 (1.1514)  acc1: 68.7500 (75.6779)  acc5: 87.5000 (91.4668)  time: 29.1098  data: 0.0030
Test:  [2180/3316]  eta: 9:28:29  loss: 1.0323 (1.1503)  acc1: 75.0000 (75.6992)  acc5: 93.7500 (91.4775)  time: 29.6295  data: 0.0028
Test:  [2200/3316]  eta: 9:13:26  loss: 0.9696 (1.1495)  acc1: 81.2500 (75.7269)  acc5: 93.7500 (91.4897)  time: 29.5079  data: 0.0030
Test:  [2190/3316]  eta: 9:21:51  loss: 0.8674 (1.1494)  acc1: 81.2500 (75.7246)  acc5: 93.7500 (91.4822)  time: 29.5502  data: 0.0028
Test:  [2180/3316]  eta: 9:29:33  loss: 1.0323 (1.1503)  acc1: 75.0000 (75.6992)  acc5: 93.7500 (91.4775)  time: 29.4047  data: 0.0028
Test:  [2180/3316]  eta: 9:29:42  loss: 1.0323 (1.1503)  acc1: 75.0000 (75.6992)  acc5: 93.7500 (91.4775)  time: 29.3900  data: 0.0025
Test:  [2180/3316]  eta: 9:29:49  loss: 1.0323 (1.1503)  acc1: 75.0000 (75.6992)  acc5: 93.7500 (91.4775)  time: 29.3321  data: 0.0028
Test:  [2240/3316]  eta: 8:45:34  loss: 1.3454 (1.1519)  acc1: 68.7500 (75.6749)  acc5: 87.5000 (91.4463)  time: 28.5563  data: 0.0025
Test:  [2240/3316]  eta: 8:46:05  loss: 1.3454 (1.1519)  acc1: 68.7500 (75.6749)  acc5: 87.5000 (91.4463)  time: 28.5684  data: 0.0025
Test:  [2190/3316]  eta: 9:23:21  loss: 0.8674 (1.1494)  acc1: 81.2500 (75.7246)  acc5: 93.7500 (91.4822)  time: 28.9743  data: 0.0025
Test:  [2210/3316]  eta: 9:08:23  loss: 1.0827 (1.1495)  acc1: 75.0000 (75.7350)  acc5: 93.7500 (91.4886)  time: 28.9263  data: 0.0027
Test:  [2200/3316]  eta: 9:16:46  loss: 0.9696 (1.1495)  acc1: 81.2500 (75.7269)  acc5: 93.7500 (91.4897)  time: 29.0785  data: 0.0027
Test:  [2190/3316]  eta: 9:24:27  loss: 0.8674 (1.1494)  acc1: 81.2500 (75.7246)  acc5: 93.7500 (91.4822)  time: 29.0701  data: 0.0025
Test:  [2190/3316]  eta: 9:24:36  loss: 0.8674 (1.1494)  acc1: 81.2500 (75.7246)  acc5: 93.7500 (91.4822)  time: 29.0565  data: 0.0023
Test:  [2190/3316]  eta: 9:24:42  loss: 0.8674 (1.1494)  acc1: 81.2500 (75.7246)  acc5: 93.7500 (91.4822)  time: 29.0348  data: 0.0025
Test:  [2250/3316]  eta: 8:40:38  loss: 1.1010 (1.1524)  acc1: 68.7500 (75.6553)  acc5: 87.5000 (91.4455)  time: 28.4760  data: 0.0024
Test:  [2250/3316]  eta: 8:41:07  loss: 1.1010 (1.1524)  acc1: 68.7500 (75.6553)  acc5: 87.5000 (91.4455)  time: 28.4952  data: 0.0027
Test:  [2200/3316]  eta: 9:18:15  loss: 0.9696 (1.1495)  acc1: 81.2500 (75.7269)  acc5: 93.7500 (91.4897)  time: 28.7852  data: 0.0029
Test:  [2220/3316]  eta: 9:03:21  loss: 1.2623 (1.1503)  acc1: 68.7500 (75.7148)  acc5: 87.5000 (91.4791)  time: 28.7860  data: 0.0030
Test:  [2210/3316]  eta: 9:11:42  loss: 1.0827 (1.1495)  acc1: 75.0000 (75.7350)  acc5: 93.7500 (91.4886)  time: 28.9401  data: 0.0035
Test:  [2200/3316]  eta: 9:19:21  loss: 0.9696 (1.1495)  acc1: 81.2500 (75.7269)  acc5: 93.7500 (91.4897)  time: 29.0244  data: 0.0032
Test:  [2200/3316]  eta: 9:19:31  loss: 0.9696 (1.1495)  acc1: 81.2500 (75.7269)  acc5: 93.7500 (91.4897)  time: 29.1177  data: 0.0029
Test:  [2200/3316]  eta: 9:19:37  loss: 0.9696 (1.1495)  acc1: 81.2500 (75.7269)  acc5: 93.7500 (91.4897)  time: 29.1069  data: 0.0038
Test:  [2260/3316]  eta: 8:35:39  loss: 1.0792 (1.1527)  acc1: 75.0000 (75.6524)  acc5: 93.7500 (91.4446)  time: 28.3290  data: 0.0032
Test:  [2260/3316]  eta: 8:36:08  loss: 1.0792 (1.1527)  acc1: 75.0000 (75.6524)  acc5: 93.7500 (91.4446)  time: 28.2521  data: 0.0029
Test:  [2210/3316]  eta: 9:13:08  loss: 1.0827 (1.1495)  acc1: 75.0000 (75.7350)  acc5: 93.7500 (91.4886)  time: 28.6741  data: 0.0035
Test:  [2230/3316]  eta: 8:58:18  loss: 1.3272 (1.1514)  acc1: 68.7500 (75.6779)  acc5: 87.5000 (91.4668)  time: 28.6695  data: 0.0033
Test:  [2220/3316]  eta: 9:06:36  loss: 1.2623 (1.1503)  acc1: 68.7500 (75.7148)  acc5: 87.5000 (91.4791)  time: 28.7381  data: 0.0034
Test:  [2210/3316]  eta: 9:14:13  loss: 1.0827 (1.1495)  acc1: 75.0000 (75.7350)  acc5: 93.7500 (91.4886)  time: 28.8916  data: 0.0033
Test:  [2210/3316]  eta: 9:14:23  loss: 1.0827 (1.1495)  acc1: 75.0000 (75.7350)  acc5: 93.7500 (91.4886)  time: 28.9249  data: 0.0030
Test:  [2210/3316]  eta: 9:14:29  loss: 1.0827 (1.1495)  acc1: 75.0000 (75.7350)  acc5: 93.7500 (91.4886)  time: 28.9414  data: 0.0039
Test:  [2270/3316]  eta: 8:30:40  loss: 1.0463 (1.1526)  acc1: 75.0000 (75.6550)  acc5: 93.7500 (91.4437)  time: 27.9939  data: 0.0032
Test:  [2270/3316]  eta: 8:31:08  loss: 1.0463 (1.1526)  acc1: 75.0000 (75.6550)  acc5: 93.7500 (91.4437)  time: 27.9946  data: 0.0026
Test:  [2220/3316]  eta: 9:08:01  loss: 1.2623 (1.1503)  acc1: 68.7500 (75.7148)  acc5: 87.5000 (91.4791)  time: 28.5345  data: 0.0029
Test:  [2240/3316]  eta: 8:53:14  loss: 1.3454 (1.1519)  acc1: 68.7500 (75.6749)  acc5: 87.5000 (91.4463)  time: 28.4591  data: 0.0027
Test:  [2230/3316]  eta: 9:01:30  loss: 1.3272 (1.1514)  acc1: 68.7500 (75.6779)  acc5: 87.5000 (91.4668)  time: 28.5238  data: 0.0025
Test:  [2220/3316]  eta: 9:09:05  loss: 1.2623 (1.1503)  acc1: 68.7500 (75.7148)  acc5: 87.5000 (91.4791)  time: 28.6147  data: 0.0024
Test:  [2220/3316]  eta: 9:09:15  loss: 1.2623 (1.1503)  acc1: 68.7500 (75.7148)  acc5: 87.5000 (91.4791)  time: 28.6341  data: 0.0024
Test:  [2220/3316]  eta: 9:09:21  loss: 1.2623 (1.1503)  acc1: 68.7500 (75.7148)  acc5: 87.5000 (91.4791)  time: 28.6684  data: 0.0025
Test:  [2280/3316]  eta: 8:25:41  loss: 1.0526 (1.1528)  acc1: 75.0000 (75.6494)  acc5: 93.7500 (91.4319)  time: 28.0093  data: 0.0023
Test:  [2280/3316]  eta: 8:26:09  loss: 1.0526 (1.1528)  acc1: 75.0000 (75.6494)  acc5: 93.7500 (91.4319)  time: 27.9696  data: 0.0023
Test:  [2230/3316]  eta: 9:02:53  loss: 1.3272 (1.1514)  acc1: 68.7500 (75.6779)  acc5: 87.5000 (91.4668)  time: 28.5119  data: 0.0023
Test:  [2250/3316]  eta: 8:48:11  loss: 1.1010 (1.1524)  acc1: 68.7500 (75.6553)  acc5: 87.5000 (91.4455)  time: 28.4783  data: 0.0023
Test:  [2240/3316]  eta: 8:56:25  loss: 1.3454 (1.1519)  acc1: 68.7500 (75.6749)  acc5: 87.5000 (91.4463)  time: 28.6413  data: 0.0022
Test:  [2230/3316]  eta: 9:03:58  loss: 1.3272 (1.1514)  acc1: 68.7500 (75.6779)  acc5: 87.5000 (91.4668)  time: 28.6935  data: 0.0026
Test:  [2230/3316]  eta: 9:04:08  loss: 1.3272 (1.1514)  acc1: 68.7500 (75.6779)  acc5: 87.5000 (91.4668)  time: 28.7675  data: 0.0023
Test:  [2230/3316]  eta: 9:04:14  loss: 1.3272 (1.1514)  acc1: 68.7500 (75.6779)  acc5: 87.5000 (91.4668)  time: 28.7644  data: 0.0023
Test:  [2290/3316]  eta: 8:20:43  loss: 1.0981 (1.1526)  acc1: 75.0000 (75.6520)  acc5: 87.5000 (91.4257)  time: 28.0716  data: 0.0022
Test:  [2290/3316]  eta: 8:21:10  loss: 1.0981 (1.1526)  acc1: 75.0000 (75.6520)  acc5: 87.5000 (91.4257)  time: 28.0457  data: 0.0023
Test:  [2240/3316]  eta: 8:57:49  loss: 1.3454 (1.1519)  acc1: 68.7500 (75.6749)  acc5: 87.5000 (91.4463)  time: 28.7632  data: 0.0023
Test:  [2260/3316]  eta: 8:43:11  loss: 1.0792 (1.1527)  acc1: 75.0000 (75.6524)  acc5: 93.7500 (91.4446)  time: 28.8549  data: 0.0026
Test:  [2250/3316]  eta: 8:51:22  loss: 1.1010 (1.1524)  acc1: 68.7500 (75.6553)  acc5: 87.5000 (91.4455)  time: 28.8776  data: 0.0031
Test:  [2240/3316]  eta: 8:58:54  loss: 1.3454 (1.1519)  acc1: 68.7500 (75.6749)  acc5: 87.5000 (91.4463)  time: 28.9926  data: 0.0037
Test:  [2240/3316]  eta: 8:59:04  loss: 1.3454 (1.1519)  acc1: 68.7500 (75.6749)  acc5: 87.5000 (91.4463)  time: 29.0545  data: 0.0035
Test:  [2300/3316]  eta: 8:15:47  loss: 1.0603 (1.1523)  acc1: 75.0000 (75.6546)  acc5: 93.7500 (91.4358)  time: 28.2969  data: 0.0036
Test:  [2240/3316]  eta: 8:59:09  loss: 1.3454 (1.1519)  acc1: 68.7500 (75.6749)  acc5: 87.5000 (91.4463)  time: 29.0009  data: 0.0036
Test:  [2300/3316]  eta: 8:16:14  loss: 1.0603 (1.1523)  acc1: 75.0000 (75.6546)  acc5: 93.7500 (91.4358)  time: 28.3003  data: 0.0037
Test:  [2250/3316]  eta: 8:52:41  loss: 1.1010 (1.1524)  acc1: 68.7500 (75.6553)  acc5: 87.5000 (91.4455)  time: 28.7351  data: 0.0039
Test:  [2270/3316]  eta: 8:38:09  loss: 1.0463 (1.1526)  acc1: 75.0000 (75.6550)  acc5: 93.7500 (91.4437)  time: 28.9109  data: 0.0039
Test:  [2260/3316]  eta: 8:46:16  loss: 1.0792 (1.1527)  acc1: 75.0000 (75.6524)  acc5: 93.7500 (91.4446)  time: 28.7974  data: 0.0039
Test:  [2250/3316]  eta: 8:53:47  loss: 1.1010 (1.1524)  acc1: 68.7500 (75.6553)  acc5: 87.5000 (91.4455)  time: 28.9121  data: 0.0040
Test:  [2250/3316]  eta: 8:53:57  loss: 1.1010 (1.1524)  acc1: 68.7500 (75.6553)  acc5: 87.5000 (91.4455)  time: 28.9527  data: 0.0040
Test:  [2310/3316]  eta: 8:10:48  loss: 1.0099 (1.1522)  acc1: 75.0000 (75.6626)  acc5: 93.7500 (91.4404)  time: 28.2309  data: 0.0040
Test:  [2250/3316]  eta: 8:54:02  loss: 1.1010 (1.1524)  acc1: 68.7500 (75.6553)  acc5: 87.5000 (91.4455)  time: 28.9034  data: 0.0039
Test:  [2310/3316]  eta: 8:11:15  loss: 1.0099 (1.1522)  acc1: 75.0000 (75.6626)  acc5: 93.7500 (91.4404)  time: 28.2463  data: 0.0037
Test:  [2260/3316]  eta: 8:47:34  loss: 1.0792 (1.1527)  acc1: 75.0000 (75.6524)  acc5: 93.7500 (91.4446)  time: 28.4173  data: 0.0040
Test:  [2280/3316]  eta: 8:33:06  loss: 1.0526 (1.1528)  acc1: 75.0000 (75.6494)  acc5: 93.7500 (91.4319)  time: 28.5652  data: 0.0036
Test:  [2270/3316]  eta: 8:41:11  loss: 1.0463 (1.1526)  acc1: 75.0000 (75.6550)  acc5: 93.7500 (91.4437)  time: 28.6026  data: 0.0031
Test:  [2260/3316]  eta: 8:48:40  loss: 1.0792 (1.1527)  acc1: 75.0000 (75.6524)  acc5: 93.7500 (91.4446)  time: 28.7069  data: 0.0029
Test:  [2320/3316]  eta: 8:05:50  loss: 1.1708 (1.1523)  acc1: 75.0000 (75.6678)  acc5: 93.7500 (91.4315)  time: 27.9787  data: 0.0027
Test:  [2260/3316]  eta: 8:48:50  loss: 1.0792 (1.1527)  acc1: 75.0000 (75.6524)  acc5: 93.7500 (91.4446)  time: 28.7301  data: 0.0028
Test:  [2260/3316]  eta: 8:48:55  loss: 1.0792 (1.1527)  acc1: 75.0000 (75.6524)  acc5: 93.7500 (91.4446)  time: 28.6940  data: 0.0027
Test:  [2320/3316]  eta: 8:06:17  loss: 1.1708 (1.1523)  acc1: 75.0000 (75.6678)  acc5: 93.7500 (91.4315)  time: 28.0643  data: 0.0024
Test:  [2270/3316]  eta: 8:42:28  loss: 1.0463 (1.1526)  acc1: 75.0000 (75.6550)  acc5: 93.7500 (91.4437)  time: 28.4733  data: 0.0025
Test:  [2290/3316]  eta: 8:28:03  loss: 1.0981 (1.1526)  acc1: 75.0000 (75.6520)  acc5: 87.5000 (91.4257)  time: 28.4809  data: 0.0024
Test:  [2280/3316]  eta: 8:36:07  loss: 1.0526 (1.1528)  acc1: 75.0000 (75.6494)  acc5: 93.7500 (91.4319)  time: 28.6652  data: 0.0024
Test:  [2270/3316]  eta: 8:43:34  loss: 1.0463 (1.1526)  acc1: 75.0000 (75.6550)  acc5: 93.7500 (91.4437)  time: 28.7640  data: 0.0024
Test:  [2330/3316]  eta: 8:00:53  loss: 1.1951 (1.1531)  acc1: 75.0000 (75.6462)  acc5: 93.7500 (91.4173)  time: 28.1132  data: 0.0023
Test:  [2270/3316]  eta: 8:43:44  loss: 1.0463 (1.1526)  acc1: 75.0000 (75.6550)  acc5: 93.7500 (91.4437)  time: 28.8073  data: 0.0024
Test:  [2270/3316]  eta: 8:43:49  loss: 1.0463 (1.1526)  acc1: 75.0000 (75.6550)  acc5: 93.7500 (91.4437)  time: 28.7677  data: 0.0024
Test:  [2330/3316]  eta: 8:01:20  loss: 1.1951 (1.1531)  acc1: 75.0000 (75.6462)  acc5: 93.7500 (91.4173)  time: 28.2046  data: 0.0023
Test:  [2280/3316]  eta: 8:37:24  loss: 1.0526 (1.1528)  acc1: 75.0000 (75.6494)  acc5: 93.7500 (91.4319)  time: 28.7567  data: 0.0024
Test:  [2300/3316]  eta: 8:23:02  loss: 1.0603 (1.1523)  acc1: 75.0000 (75.6546)  acc5: 93.7500 (91.4358)  time: 28.6197  data: 0.0023
Test:  [2290/3316]  eta: 8:31:06  loss: 1.0981 (1.1526)  acc1: 75.0000 (75.6520)  acc5: 87.5000 (91.4257)  time: 29.0201  data: 0.0024
Test:  [2280/3316]  eta: 8:38:31  loss: 1.0526 (1.1528)  acc1: 75.0000 (75.6494)  acc5: 93.7500 (91.4319)  time: 29.0986  data: 0.0025
Test:  [2340/3316]  eta: 7:55:58  loss: 1.1468 (1.1530)  acc1: 75.0000 (75.6594)  acc5: 93.7500 (91.4193)  time: 28.4921  data: 0.0025
Test:  [2280/3316]  eta: 8:38:40  loss: 1.0526 (1.1528)  acc1: 75.0000 (75.6494)  acc5: 93.7500 (91.4319)  time: 29.0360  data: 0.0025
Test:  [2280/3316]  eta: 8:38:45  loss: 1.0526 (1.1528)  acc1: 75.0000 (75.6494)  acc5: 93.7500 (91.4319)  time: 29.0915  data: 0.0026
Test:  [2340/3316]  eta: 7:56:24  loss: 1.1468 (1.1530)  acc1: 75.0000 (75.6594)  acc5: 93.7500 (91.4193)  time: 28.4725  data: 0.0026
Test:  [2290/3316]  eta: 8:32:20  loss: 1.0981 (1.1526)  acc1: 75.0000 (75.6520)  acc5: 87.5000 (91.4257)  time: 29.0909  data: 0.0024
Test:  [2310/3316]  eta: 8:18:02  loss: 1.0099 (1.1522)  acc1: 75.0000 (75.6626)  acc5: 93.7500 (91.4404)  time: 28.9313  data: 0.0024
Test:  [2300/3316]  eta: 8:26:04  loss: 1.0603 (1.1523)  acc1: 75.0000 (75.6546)  acc5: 93.7500 (91.4358)  time: 29.3472  data: 0.0029
Test:  [2350/3316]  eta: 7:51:03  loss: 1.1382 (1.1534)  acc1: 75.0000 (75.6487)  acc5: 87.5000 (91.4132)  time: 28.7251  data: 0.0031
Test:  [2290/3316]  eta: 8:33:28  loss: 1.0981 (1.1526)  acc1: 75.0000 (75.6520)  acc5: 87.5000 (91.4257)  time: 29.4397  data: 0.0029
Test:  [2290/3316]  eta: 8:33:36  loss: 1.0981 (1.1526)  acc1: 75.0000 (75.6520)  acc5: 87.5000 (91.4257)  time: 29.2813  data: 0.0030
Test:  [2290/3316]  eta: 8:33:42  loss: 1.0981 (1.1526)  acc1: 75.0000 (75.6520)  acc5: 87.5000 (91.4257)  time: 29.3738  data: 0.0031
Test:  [2350/3316]  eta: 7:51:29  loss: 1.1382 (1.1534)  acc1: 75.0000 (75.6487)  acc5: 87.5000 (91.4132)  time: 28.6429  data: 0.0038
Test:  [2300/3316]  eta: 8:27:16  loss: 1.0603 (1.1523)  acc1: 75.0000 (75.6546)  acc5: 93.7500 (91.4358)  time: 29.0520  data: 0.0036
Test:  [2320/3316]  eta: 8:13:02  loss: 1.1708 (1.1523)  acc1: 75.0000 (75.6678)  acc5: 93.7500 (91.4315)  time: 29.0625  data: 0.0039
Test:  [2310/3316]  eta: 8:21:01  loss: 1.0099 (1.1522)  acc1: 75.0000 (75.6626)  acc5: 93.7500 (91.4404)  time: 29.0672  data: 0.0032
Test:  [2360/3316]  eta: 7:46:06  loss: 1.1382 (1.1533)  acc1: 75.0000 (75.6539)  acc5: 93.7500 (91.4099)  time: 28.3855  data: 0.0034
Test:  [2300/3316]  eta: 8:28:22  loss: 1.0603 (1.1523)  acc1: 75.0000 (75.6546)  acc5: 93.7500 (91.4358)  time: 29.1378  data: 0.0033
Test:  [2300/3316]  eta: 8:28:30  loss: 1.0603 (1.1523)  acc1: 75.0000 (75.6546)  acc5: 93.7500 (91.4358)  time: 29.0512  data: 0.0035
Test:  [2300/3316]  eta: 8:28:36  loss: 1.0603 (1.1523)  acc1: 75.0000 (75.6546)  acc5: 93.7500 (91.4358)  time: 29.0977  data: 0.0034
Test:  [2360/3316]  eta: 7:46:31  loss: 1.1382 (1.1533)  acc1: 75.0000 (75.6539)  acc5: 93.7500 (91.4099)  time: 28.3873  data: 0.0039
Test:  [2310/3316]  eta: 8:22:12  loss: 1.0099 (1.1522)  acc1: 75.0000 (75.6626)  acc5: 93.7500 (91.4404)  time: 28.8903  data: 0.0037
Test:  [2330/3316]  eta: 8:08:01  loss: 1.1951 (1.1531)  acc1: 75.0000 (75.6462)  acc5: 93.7500 (91.4173)  time: 28.8842  data: 0.0039
Test:  [2320/3316]  eta: 8:15:59  loss: 1.1708 (1.1523)  acc1: 75.0000 (75.6678)  acc5: 93.7500 (91.4315)  time: 28.9782  data: 0.0027
Test:  [2370/3316]  eta: 7:41:10  loss: 1.0504 (1.1532)  acc1: 75.0000 (75.6616)  acc5: 93.7500 (91.4066)  time: 28.2795  data: 0.0030
Test:  [2310/3316]  eta: 8:23:20  loss: 1.0099 (1.1522)  acc1: 75.0000 (75.6626)  acc5: 93.7500 (91.4404)  time: 29.1327  data: 0.0032
Test:  [2310/3316]  eta: 8:23:27  loss: 1.0099 (1.1522)  acc1: 75.0000 (75.6626)  acc5: 93.7500 (91.4404)  time: 29.0131  data: 0.0033
Test:  [2310/3316]  eta: 8:23:33  loss: 1.0099 (1.1522)  acc1: 75.0000 (75.6626)  acc5: 93.7500 (91.4404)  time: 29.0624  data: 0.0031
Test:  [2370/3316]  eta: 7:41:36  loss: 1.0504 (1.1532)  acc1: 75.0000 (75.6616)  acc5: 93.7500 (91.4066)  time: 28.3943  data: 0.0029
Test:  [2320/3316]  eta: 8:17:09  loss: 1.1708 (1.1523)  acc1: 75.0000 (75.6678)  acc5: 93.7500 (91.4315)  time: 29.0770  data: 0.0028
Test:  [2340/3316]  eta: 8:03:02  loss: 1.1468 (1.1530)  acc1: 75.0000 (75.6594)  acc5: 93.7500 (91.4193)  time: 28.9561  data: 0.0028
Test:  [2380/3316]  eta: 7:36:17  loss: 1.0029 (1.1524)  acc1: 75.0000 (75.6825)  acc5: 93.7500 (91.4190)  time: 28.7219  data: 0.0028
Test:  [2330/3316]  eta: 8:10:59  loss: 1.1951 (1.1531)  acc1: 75.0000 (75.6462)  acc5: 93.7500 (91.4173)  time: 29.3450  data: 0.0027
Test:  [2320/3316]  eta: 8:18:18  loss: 1.1708 (1.1523)  acc1: 75.0000 (75.6678)  acc5: 93.7500 (91.4315)  time: 29.6259  data: 0.0028
Test:  [2320/3316]  eta: 8:18:26  loss: 1.1708 (1.1523)  acc1: 75.0000 (75.6678)  acc5: 93.7500 (91.4315)  time: 29.5812  data: 0.0028
Test:  [2320/3316]  eta: 8:18:30  loss: 1.1708 (1.1523)  acc1: 75.0000 (75.6678)  acc5: 93.7500 (91.4315)  time: 29.4433  data: 0.0027
Test:  [2380/3316]  eta: 7:36:42  loss: 1.0029 (1.1524)  acc1: 75.0000 (75.6825)  acc5: 93.7500 (91.4190)  time: 28.7957  data: 0.0026
Test:  [2330/3316]  eta: 8:12:07  loss: 1.1951 (1.1531)  acc1: 75.0000 (75.6462)  acc5: 93.7500 (91.4173)  time: 29.3229  data: 0.0028
Test:  [2350/3316]  eta: 7:58:03  loss: 1.1382 (1.1534)  acc1: 75.0000 (75.6487)  acc5: 87.5000 (91.4132)  time: 29.2506  data: 0.0029
Test:  [2390/3316]  eta: 7:31:24  loss: 0.8704 (1.1517)  acc1: 81.2500 (75.7058)  acc5: 93.7500 (91.4210)  time: 29.0941  data: 0.0033
Test:  [2340/3316]  eta: 8:05:59  loss: 1.1468 (1.1530)  acc1: 75.0000 (75.6594)  acc5: 93.7500 (91.4193)  time: 29.6527  data: 0.0037
Test:  [2330/3316]  eta: 8:13:19  loss: 1.1951 (1.1531)  acc1: 75.0000 (75.6462)  acc5: 93.7500 (91.4173)  time: 29.9593  data: 0.0036
Test:  [2330/3316]  eta: 8:13:26  loss: 1.1951 (1.1531)  acc1: 75.0000 (75.6462)  acc5: 93.7500 (91.4173)  time: 30.0075  data: 0.0035
Test:  [2390/3316]  eta: 7:31:49  loss: 0.8704 (1.1517)  acc1: 81.2500 (75.7058)  acc5: 93.7500 (91.4210)  time: 29.1459  data: 0.0041
Test:  [2330/3316]  eta: 8:13:30  loss: 1.1951 (1.1531)  acc1: 75.0000 (75.6462)  acc5: 93.7500 (91.4173)  time: 29.7952  data: 0.0040
Test:  [2360/3316]  eta: 7:53:07  loss: 1.1382 (1.1533)  acc1: 75.0000 (75.6539)  acc5: 93.7500 (91.4099)  time: 29.5550  data: 0.0043
Test:  [2340/3316]  eta: 8:07:09  loss: 1.1468 (1.1530)  acc1: 75.0000 (75.6594)  acc5: 93.7500 (91.4193)  time: 29.7446  data: 0.0045
Test:  [2400/3316]  eta: 7:26:30  loss: 0.9740 (1.1513)  acc1: 81.2500 (75.7289)  acc5: 93.7500 (91.4254)  time: 29.0430  data: 0.0045
Test:  [2350/3316]  eta: 8:00:58  loss: 1.1382 (1.1534)  acc1: 75.0000 (75.6487)  acc5: 87.5000 (91.4132)  time: 29.5734  data: 0.0048
Test:  [2340/3316]  eta: 8:08:16  loss: 1.1468 (1.1530)  acc1: 75.0000 (75.6594)  acc5: 93.7500 (91.4193)  time: 29.7763  data: 0.0046
Test:  [2400/3316]  eta: 7:26:54  loss: 0.9740 (1.1513)  acc1: 81.2500 (75.7289)  acc5: 93.7500 (91.4254)  time: 29.0498  data: 0.0050
Test:  [2340/3316]  eta: 8:08:24  loss: 1.1468 (1.1530)  acc1: 75.0000 (75.6594)  acc5: 93.7500 (91.4193)  time: 29.8236  data: 0.0044
Test:  [2340/3316]  eta: 8:08:27  loss: 1.1468 (1.1530)  acc1: 75.0000 (75.6594)  acc5: 93.7500 (91.4193)  time: 29.7203  data: 0.0048
Test:  [2370/3316]  eta: 7:48:07  loss: 1.0504 (1.1532)  acc1: 75.0000 (75.6616)  acc5: 93.7500 (91.4066)  time: 29.3578  data: 0.0045
Test:  [2350/3316]  eta: 8:02:06  loss: 1.1382 (1.1534)  acc1: 75.0000 (75.6487)  acc5: 87.5000 (91.4132)  time: 29.6220  data: 0.0046
Test:  [2410/3316]  eta: 7:21:35  loss: 1.0540 (1.1508)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4299)  time: 28.7104  data: 0.0040
Test:  [2360/3316]  eta: 7:55:56  loss: 1.1382 (1.1533)  acc1: 75.0000 (75.6539)  acc5: 93.7500 (91.4099)  time: 29.1983  data: 0.0039
Test:  [2350/3316]  eta: 8:03:12  loss: 1.1382 (1.1534)  acc1: 75.0000 (75.6487)  acc5: 87.5000 (91.4132)  time: 29.2994  data: 0.0038
Test:  [2410/3316]  eta: 7:21:58  loss: 1.0540 (1.1508)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4299)  time: 28.5723  data: 0.0035
Test:  [2350/3316]  eta: 8:03:20  loss: 1.1382 (1.1534)  acc1: 75.0000 (75.6487)  acc5: 87.5000 (91.4132)  time: 29.3928  data: 0.0037
Test:  [2350/3316]  eta: 8:03:23  loss: 1.1382 (1.1534)  acc1: 75.0000 (75.6487)  acc5: 87.5000 (91.4132)  time: 29.3018  data: 0.0036
Test:  [2380/3316]  eta: 7:43:07  loss: 1.0029 (1.1524)  acc1: 75.0000 (75.6825)  acc5: 93.7500 (91.4190)  time: 28.9257  data: 0.0027
Test:  [2360/3316]  eta: 7:57:02  loss: 1.1382 (1.1533)  acc1: 75.0000 (75.6539)  acc5: 93.7500 (91.4099)  time: 29.0395  data: 0.0029
Test:  [2420/3316]  eta: 7:16:40  loss: 1.2250 (1.1519)  acc1: 68.7500 (75.7099)  acc5: 87.5000 (91.4163)  time: 28.5033  data: 0.0026
Test:  [2370/3316]  eta: 7:50:54  loss: 1.0504 (1.1532)  acc1: 75.0000 (75.6616)  acc5: 93.7500 (91.4066)  time: 28.9891  data: 0.0026
Test:  [2360/3316]  eta: 7:58:09  loss: 1.1382 (1.1533)  acc1: 75.0000 (75.6539)  acc5: 93.7500 (91.4099)  time: 29.2131  data: 0.0026
Test:  [2420/3316]  eta: 7:17:03  loss: 1.2250 (1.1519)  acc1: 68.7500 (75.7099)  acc5: 87.5000 (91.4163)  time: 28.5158  data: 0.0025
Test:  [2360/3316]  eta: 7:58:17  loss: 1.1382 (1.1533)  acc1: 75.0000 (75.6539)  acc5: 93.7500 (91.4099)  time: 29.2886  data: 0.0027
Test:  [2360/3316]  eta: 7:58:20  loss: 1.1382 (1.1533)  acc1: 75.0000 (75.6539)  acc5: 93.7500 (91.4099)  time: 29.2173  data: 0.0032
Test:  [2390/3316]  eta: 7:38:07  loss: 0.8704 (1.1517)  acc1: 81.2500 (75.7058)  acc5: 93.7500 (91.4210)  time: 28.9464  data: 0.0025
Test:  [2370/3316]  eta: 7:51:59  loss: 1.0504 (1.1532)  acc1: 75.0000 (75.6616)  acc5: 93.7500 (91.4066)  time: 29.0104  data: 0.0029
Test:  [2430/3316]  eta: 7:11:45  loss: 1.2250 (1.1522)  acc1: 68.7500 (75.6942)  acc5: 87.5000 (91.4181)  time: 28.4713  data: 0.0026
Test:  [2380/3316]  eta: 7:45:52  loss: 1.0029 (1.1524)  acc1: 75.0000 (75.6825)  acc5: 93.7500 (91.4190)  time: 29.0148  data: 0.0029
Test:  [2430/3316]  eta: 7:12:07  loss: 1.2250 (1.1522)  acc1: 68.7500 (75.6942)  acc5: 87.5000 (91.4181)  time: 28.4520  data: 0.0029
Test:  [2370/3316]  eta: 7:53:05  loss: 1.0504 (1.1532)  acc1: 75.0000 (75.6616)  acc5: 93.7500 (91.4066)  time: 29.1608  data: 0.0029
Test:  [2370/3316]  eta: 7:53:13  loss: 1.0504 (1.1532)  acc1: 75.0000 (75.6616)  acc5: 93.7500 (91.4066)  time: 29.1622  data: 0.0026
Test:  [2370/3316]  eta: 7:53:15  loss: 1.0504 (1.1532)  acc1: 75.0000 (75.6616)  acc5: 93.7500 (91.4066)  time: 29.0910  data: 0.0032
Test:  [2400/3316]  eta: 7:33:07  loss: 0.9740 (1.1513)  acc1: 81.2500 (75.7289)  acc5: 93.7500 (91.4254)  time: 28.8143  data: 0.0029
Test:  [2380/3316]  eta: 7:46:56  loss: 1.0029 (1.1524)  acc1: 75.0000 (75.6825)  acc5: 93.7500 (91.4190)  time: 28.9142  data: 0.0031
Test:  [2440/3316]  eta: 7:06:49  loss: 1.1626 (1.1522)  acc1: 75.0000 (75.6888)  acc5: 93.7500 (91.4328)  time: 28.4455  data: 0.0025
Test:  [2390/3316]  eta: 7:40:50  loss: 0.8704 (1.1517)  acc1: 81.2500 (75.7058)  acc5: 93.7500 (91.4210)  time: 28.9659  data: 0.0029
Test:  [2440/3316]  eta: 7:07:11  loss: 1.1626 (1.1522)  acc1: 75.0000 (75.6888)  acc5: 93.7500 (91.4328)  time: 28.2559  data: 0.0029
Test:  [2380/3316]  eta: 7:48:01  loss: 1.0029 (1.1524)  acc1: 75.0000 (75.6825)  acc5: 93.7500 (91.4190)  time: 28.9970  data: 0.0029
Test:  [2380/3316]  eta: 7:48:08  loss: 1.0029 (1.1524)  acc1: 75.0000 (75.6825)  acc5: 93.7500 (91.4190)  time: 28.9506  data: 0.0025
Test:  [2380/3316]  eta: 7:48:11  loss: 1.0029 (1.1524)  acc1: 75.0000 (75.6825)  acc5: 93.7500 (91.4190)  time: 28.9432  data: 0.0026
Test:  [2410/3316]  eta: 7:28:05  loss: 1.0540 (1.1508)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4299)  time: 28.6004  data: 0.0027
Test:  [2390/3316]  eta: 7:41:51  loss: 0.8704 (1.1517)  acc1: 81.2500 (75.7058)  acc5: 93.7500 (91.4210)  time: 28.6812  data: 0.0027
Test:  [2450/3316]  eta: 7:01:55  loss: 1.1414 (1.1521)  acc1: 75.0000 (75.6987)  acc5: 93.7500 (91.4321)  time: 28.4872  data: 0.0028
Test:  [2400/3316]  eta: 7:35:48  loss: 0.9740 (1.1513)  acc1: 81.2500 (75.7289)  acc5: 93.7500 (91.4254)  time: 28.9460  data: 0.0032
Test:  [2450/3316]  eta: 7:02:16  loss: 1.1414 (1.1521)  acc1: 75.0000 (75.6987)  acc5: 93.7500 (91.4321)  time: 28.4767  data: 0.0038
Test:  [2390/3316]  eta: 7:42:59  loss: 0.8704 (1.1517)  acc1: 81.2500 (75.7058)  acc5: 93.7500 (91.4210)  time: 29.1546  data: 0.0034
Test:  [2390/3316]  eta: 7:43:06  loss: 0.8704 (1.1517)  acc1: 81.2500 (75.7058)  acc5: 93.7500 (91.4210)  time: 29.0961  data: 0.0034
Test:  [2390/3316]  eta: 7:43:08  loss: 0.8704 (1.1517)  acc1: 81.2500 (75.7058)  acc5: 93.7500 (91.4210)  time: 29.1631  data: 0.0033
Test:  [2420/3316]  eta: 7:23:07  loss: 1.2250 (1.1519)  acc1: 68.7500 (75.7099)  acc5: 87.5000 (91.4163)  time: 28.8958  data: 0.0040
Test:  [2400/3316]  eta: 7:36:50  loss: 0.9740 (1.1513)  acc1: 81.2500 (75.7289)  acc5: 93.7500 (91.4254)  time: 28.9618  data: 0.0041
Test:  [2460/3316]  eta: 6:56:59  loss: 1.0409 (1.1519)  acc1: 81.2500 (75.7136)  acc5: 93.7500 (91.4339)  time: 28.5190  data: 0.0037
Test:  [2410/3316]  eta: 7:30:46  loss: 1.0540 (1.1508)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4299)  time: 29.0195  data: 0.0042
Test:  [2460/3316]  eta: 6:57:21  loss: 1.0409 (1.1519)  acc1: 81.2500 (75.7136)  acc5: 93.7500 (91.4339)  time: 28.6675  data: 0.0048
Test:  [2400/3316]  eta: 7:37:56  loss: 0.9740 (1.1513)  acc1: 81.2500 (75.7289)  acc5: 93.7500 (91.4254)  time: 29.3955  data: 0.0044
Test:  [2400/3316]  eta: 7:38:03  loss: 0.9740 (1.1513)  acc1: 81.2500 (75.7289)  acc5: 93.7500 (91.4254)  time: 29.3044  data: 0.0044
Test:  [2400/3316]  eta: 7:38:06  loss: 0.9740 (1.1513)  acc1: 81.2500 (75.7289)  acc5: 93.7500 (91.4254)  time: 29.3544  data: 0.0040
Test:  [2430/3316]  eta: 7:18:08  loss: 1.2250 (1.1522)  acc1: 68.7500 (75.6942)  acc5: 87.5000 (91.4181)  time: 29.1803  data: 0.0044
Test:  [2410/3316]  eta: 7:31:48  loss: 1.0540 (1.1508)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4299)  time: 29.2988  data: 0.0043
Test:  [2470/3316]  eta: 6:52:05  loss: 1.0890 (1.1523)  acc1: 75.0000 (75.7082)  acc5: 93.7500 (91.4230)  time: 28.5306  data: 0.0034
Test:  [2420/3316]  eta: 7:25:45  loss: 1.2250 (1.1519)  acc1: 68.7500 (75.7099)  acc5: 87.5000 (91.4163)  time: 29.0431  data: 0.0035
Test:  [2470/3316]  eta: 6:52:27  loss: 1.0890 (1.1523)  acc1: 75.0000 (75.7082)  acc5: 93.7500 (91.4230)  time: 28.6332  data: 0.0035
Test:  [2410/3316]  eta: 7:32:54  loss: 1.0540 (1.1508)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4299)  time: 29.4150  data: 0.0035
Test:  [2410/3316]  eta: 7:33:01  loss: 1.0540 (1.1508)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4299)  time: 29.3527  data: 0.0034
Test:  [2410/3316]  eta: 7:33:03  loss: 1.0540 (1.1508)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4299)  time: 29.3050  data: 0.0033
Test:  [2440/3316]  eta: 7:13:10  loss: 1.1626 (1.1522)  acc1: 75.0000 (75.6888)  acc5: 93.7500 (91.4328)  time: 29.0949  data: 0.0028
Test:  [2420/3316]  eta: 7:26:45  loss: 1.2250 (1.1519)  acc1: 68.7500 (75.7099)  acc5: 87.5000 (91.4163)  time: 29.1553  data: 0.0027
Test:  [2480/3316]  eta: 6:47:10  loss: 1.0455 (1.1513)  acc1: 75.0000 (75.7331)  acc5: 93.7500 (91.4349)  time: 28.5892  data: 0.0024
Test:  [2430/3316]  eta: 7:20:43  loss: 1.2250 (1.1522)  acc1: 68.7500 (75.6942)  acc5: 87.5000 (91.4181)  time: 29.0903  data: 0.0026
Test:  [2480/3316]  eta: 6:47:32  loss: 1.0455 (1.1513)  acc1: 75.0000 (75.7331)  acc5: 93.7500 (91.4349)  time: 28.6721  data: 0.0026
Test:  [2420/3316]  eta: 7:27:52  loss: 1.2250 (1.1519)  acc1: 68.7500 (75.7099)  acc5: 87.5000 (91.4163)  time: 29.4664  data: 0.0027
Test:  [2420/3316]  eta: 7:27:58  loss: 1.2250 (1.1519)  acc1: 68.7500 (75.7099)  acc5: 87.5000 (91.4163)  time: 29.3936  data: 0.0026
Test:  [2420/3316]  eta: 7:28:01  loss: 1.2250 (1.1519)  acc1: 68.7500 (75.7099)  acc5: 87.5000 (91.4163)  time: 29.3902  data: 0.0025
Test:  [2450/3316]  eta: 7:08:11  loss: 1.1414 (1.1521)  acc1: 75.0000 (75.6987)  acc5: 93.7500 (91.4321)  time: 29.0852  data: 0.0026
Test:  [2430/3316]  eta: 7:21:43  loss: 1.2250 (1.1522)  acc1: 68.7500 (75.6942)  acc5: 87.5000 (91.4181)  time: 29.0862  data: 0.0025
Test:  [2490/3316]  eta: 6:42:16  loss: 0.9817 (1.1513)  acc1: 75.0000 (75.7276)  acc5: 93.7500 (91.4342)  time: 28.5443  data: 0.0025
Test:  [2440/3316]  eta: 7:15:44  loss: 1.1626 (1.1522)  acc1: 75.0000 (75.6888)  acc5: 93.7500 (91.4328)  time: 29.3306  data: 0.0031
Test:  [2490/3316]  eta: 6:42:39  loss: 0.9817 (1.1513)  acc1: 75.0000 (75.7276)  acc5: 93.7500 (91.4342)  time: 28.7869  data: 0.0027
Test:  [2430/3316]  eta: 7:22:51  loss: 1.2250 (1.1522)  acc1: 68.7500 (75.6942)  acc5: 87.5000 (91.4181)  time: 29.5719  data: 0.0038
Test:  [2430/3316]  eta: 7:22:57  loss: 1.2250 (1.1522)  acc1: 68.7500 (75.6942)  acc5: 87.5000 (91.4181)  time: 29.5196  data: 0.0034
Test:  [2430/3316]  eta: 7:22:59  loss: 1.2250 (1.1522)  acc1: 68.7500 (75.6942)  acc5: 87.5000 (91.4181)  time: 29.5261  data: 0.0034
Test:  [2460/3316]  eta: 7:03:14  loss: 1.0409 (1.1519)  acc1: 81.2500 (75.7136)  acc5: 93.7500 (91.4339)  time: 29.3048  data: 0.0042
Test:  [2440/3316]  eta: 7:16:42  loss: 1.1626 (1.1522)  acc1: 75.0000 (75.6888)  acc5: 93.7500 (91.4328)  time: 29.2454  data: 0.0041
Test:  [2500/3316]  eta: 6:37:24  loss: 1.0616 (1.1511)  acc1: 75.0000 (75.7147)  acc5: 93.7500 (91.4384)  time: 28.8637  data: 0.0042
Test:  [2500/3316]  eta: 6:37:44  loss: 1.0616 (1.1511)  acc1: 75.0000 (75.7147)  acc5: 93.7500 (91.4384)  time: 28.7321  data: 0.0039
Test:  [2450/3316]  eta: 7:10:43  loss: 1.1414 (1.1521)  acc1: 75.0000 (75.6987)  acc5: 93.7500 (91.4321)  time: 29.3763  data: 0.0044
Test:  [2440/3316]  eta: 7:17:48  loss: 1.1626 (1.1522)  acc1: 75.0000 (75.6888)  acc5: 93.7500 (91.4328)  time: 29.3617  data: 0.0047
Test:  [2440/3316]  eta: 7:17:55  loss: 1.1626 (1.1522)  acc1: 75.0000 (75.6888)  acc5: 93.7500 (91.4328)  time: 29.5263  data: 0.0043
Test:  [2440/3316]  eta: 7:17:57  loss: 1.1626 (1.1522)  acc1: 75.0000 (75.6888)  acc5: 93.7500 (91.4328)  time: 29.4234  data: 0.0043
Test:  [2470/3316]  eta: 6:58:15  loss: 1.0890 (1.1523)  acc1: 75.0000 (75.7082)  acc5: 93.7500 (91.4230)  time: 29.2448  data: 0.0046
Test:  [2450/3316]  eta: 7:11:41  loss: 1.1414 (1.1521)  acc1: 75.0000 (75.6987)  acc5: 93.7500 (91.4321)  time: 29.3396  data: 0.0046
Test:  [2510/3316]  eta: 6:32:30  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.7044)  acc5: 93.7500 (91.4352)  time: 28.9484  data: 0.0043
Test:  [2510/3316]  eta: 6:32:50  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.7044)  acc5: 93.7500 (91.4352)  time: 28.6112  data: 0.0038
Test:  [2460/3316]  eta: 7:05:42  loss: 1.0409 (1.1519)  acc1: 81.2500 (75.7136)  acc5: 93.7500 (91.4339)  time: 29.1141  data: 0.0041
Test:  [2450/3316]  eta: 7:12:46  loss: 1.1414 (1.1521)  acc1: 75.0000 (75.6987)  acc5: 93.7500 (91.4321)  time: 29.1751  data: 0.0036
Test:  [2450/3316]  eta: 7:12:53  loss: 1.1414 (1.1521)  acc1: 75.0000 (75.6987)  acc5: 93.7500 (91.4321)  time: 29.3457  data: 0.0036
Test:  [2450/3316]  eta: 7:12:55  loss: 1.1414 (1.1521)  acc1: 75.0000 (75.6987)  acc5: 93.7500 (91.4321)  time: 29.3374  data: 0.0035
Test:  [2480/3316]  eta: 6:53:15  loss: 1.0455 (1.1513)  acc1: 75.0000 (75.7331)  acc5: 93.7500 (91.4349)  time: 28.9502  data: 0.0031
Test:  [2460/3316]  eta: 7:06:40  loss: 1.0409 (1.1519)  acc1: 81.2500 (75.7136)  acc5: 93.7500 (91.4339)  time: 29.3076  data: 0.0032
Test:  [2520/3316]  eta: 6:27:36  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.6942)  acc5: 93.7500 (91.4369)  time: 28.6891  data: 0.0026
Test:  [2520/3316]  eta: 6:27:55  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.6942)  acc5: 93.7500 (91.4369)  time: 28.6442  data: 0.0025
Test:  [2470/3316]  eta: 7:00:41  loss: 1.0890 (1.1523)  acc1: 75.0000 (75.7082)  acc5: 93.7500 (91.4230)  time: 29.1230  data: 0.0028
Test:  [2460/3316]  eta: 7:07:44  loss: 1.0409 (1.1519)  acc1: 81.2500 (75.7136)  acc5: 93.7500 (91.4339)  time: 29.3243  data: 0.0028
Test:  [2460/3316]  eta: 7:07:50  loss: 1.0409 (1.1519)  acc1: 81.2500 (75.7136)  acc5: 93.7500 (91.4339)  time: 29.2554  data: 0.0027
Test:  [2460/3316]  eta: 7:07:52  loss: 1.0409 (1.1519)  acc1: 81.2500 (75.7136)  acc5: 93.7500 (91.4339)  time: 29.3168  data: 0.0029
Test:  [2490/3316]  eta: 6:48:17  loss: 0.9817 (1.1513)  acc1: 75.0000 (75.7276)  acc5: 93.7500 (91.4342)  time: 28.9999  data: 0.0026
Test:  [2470/3316]  eta: 7:01:38  loss: 1.0890 (1.1523)  acc1: 75.0000 (75.7082)  acc5: 93.7500 (91.4230)  time: 29.2277  data: 0.0028
Test:  [2530/3316]  eta: 6:22:42  loss: 1.1400 (1.1512)  acc1: 75.0000 (75.7137)  acc5: 93.7500 (91.4337)  time: 28.6571  data: 0.0027
Test:  [2530/3316]  eta: 6:23:01  loss: 1.1400 (1.1512)  acc1: 75.0000 (75.7137)  acc5: 93.7500 (91.4337)  time: 28.6422  data: 0.0027
Test:  [2480/3316]  eta: 6:55:41  loss: 1.0455 (1.1513)  acc1: 75.0000 (75.7331)  acc5: 93.7500 (91.4349)  time: 29.1214  data: 0.0026
Test:  [2470/3316]  eta: 7:02:45  loss: 1.0890 (1.1523)  acc1: 75.0000 (75.7082)  acc5: 93.7500 (91.4230)  time: 29.8140  data: 0.0031
Test:  [2470/3316]  eta: 7:02:51  loss: 1.0890 (1.1523)  acc1: 75.0000 (75.7082)  acc5: 93.7500 (91.4230)  time: 29.7617  data: 0.0037
Test:  [2470/3316]  eta: 7:02:54  loss: 1.0890 (1.1523)  acc1: 75.0000 (75.7082)  acc5: 93.7500 (91.4230)  time: 29.8300  data: 0.0037
Test:  [2500/3316]  eta: 6:43:24  loss: 1.0616 (1.1511)  acc1: 75.0000 (75.7147)  acc5: 93.7500 (91.4384)  time: 29.9678  data: 0.0040
Test:  [2480/3316]  eta: 6:56:43  loss: 1.0455 (1.1513)  acc1: 75.0000 (75.7331)  acc5: 93.7500 (91.4349)  time: 30.1378  data: 0.0044
Test:  [2540/3316]  eta: 6:17:55  loss: 1.2597 (1.1519)  acc1: 75.0000 (75.7010)  acc5: 87.5000 (91.4133)  time: 29.7243  data: 0.0052
Test:  [2540/3316]  eta: 6:18:13  loss: 1.2597 (1.1519)  acc1: 75.0000 (75.7010)  acc5: 87.5000 (91.4133)  time: 29.6815  data: 0.0051
Test:  [2490/3316]  eta: 6:50:48  loss: 0.9817 (1.1513)  acc1: 75.0000 (75.7276)  acc5: 93.7500 (91.4342)  time: 30.3280  data: 0.0053
Test:  [2480/3316]  eta: 6:57:49  loss: 1.0455 (1.1513)  acc1: 75.0000 (75.7331)  acc5: 93.7500 (91.4349)  time: 30.6786  data: 0.0046
Test:  [2480/3316]  eta: 6:57:54  loss: 1.0455 (1.1513)  acc1: 75.0000 (75.7331)  acc5: 93.7500 (91.4349)  time: 30.6408  data: 0.0053
Test:  [2480/3316]  eta: 6:57:56  loss: 1.0455 (1.1513)  acc1: 75.0000 (75.7331)  acc5: 93.7500 (91.4349)  time: 30.5941  data: 0.0051
Test:  [2510/3316]  eta: 6:38:28  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.7044)  acc5: 93.7500 (91.4352)  time: 30.3939  data: 0.0050
Test:  [2490/3316]  eta: 6:51:47  loss: 0.9817 (1.1513)  acc1: 75.0000 (75.7276)  acc5: 93.7500 (91.4342)  time: 30.9325  data: 0.0053
Test:  [2550/3316]  eta: 6:13:04  loss: 1.1936 (1.1519)  acc1: 75.0000 (75.6958)  acc5: 93.7500 (91.4176)  time: 30.3357  data: 0.0055
Test:  [2550/3316]  eta: 6:13:24  loss: 1.1936 (1.1519)  acc1: 75.0000 (75.6958)  acc5: 93.7500 (91.4176)  time: 30.4815  data: 0.0054
Test:  [2500/3316]  eta: 6:45:54  loss: 1.0616 (1.1511)  acc1: 75.0000 (75.7147)  acc5: 93.7500 (91.4384)  time: 31.3188  data: 0.0056
Test:  [2490/3316]  eta: 6:52:53  loss: 0.9817 (1.1513)  acc1: 75.0000 (75.7276)  acc5: 93.7500 (91.4342)  time: 31.1645  data: 0.0050
Test:  [2490/3316]  eta: 6:52:59  loss: 0.9817 (1.1513)  acc1: 75.0000 (75.7276)  acc5: 93.7500 (91.4342)  time: 31.1042  data: 0.0048
Test:  [2490/3316]  eta: 6:53:00  loss: 0.9817 (1.1513)  acc1: 75.0000 (75.7276)  acc5: 93.7500 (91.4342)  time: 30.9614  data: 0.0048
Test:  [2520/3316]  eta: 6:33:35  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.6942)  acc5: 93.7500 (91.4369)  time: 30.3261  data: 0.0041
Test:  [2500/3316]  eta: 6:46:51  loss: 1.0616 (1.1511)  acc1: 75.0000 (75.7147)  acc5: 93.7500 (91.4384)  time: 30.8752  data: 0.0040
Test:  [2560/3316]  eta: 6:08:14  loss: 0.9531 (1.1513)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.4242)  time: 29.9048  data: 0.0035
Test:  [2560/3316]  eta: 6:08:32  loss: 0.9531 (1.1513)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.4242)  time: 29.8738  data: 0.0034
Test:  [2510/3316]  eta: 6:40:55  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.7044)  acc5: 93.7500 (91.4352)  time: 30.4607  data: 0.0032
Test:  [2500/3316]  eta: 6:47:54  loss: 1.0616 (1.1511)  acc1: 75.0000 (75.7147)  acc5: 93.7500 (91.4384)  time: 30.7173  data: 0.0036
Test:  [2500/3316]  eta: 6:47:59  loss: 1.0616 (1.1511)  acc1: 75.0000 (75.7147)  acc5: 93.7500 (91.4384)  time: 30.7306  data: 0.0034
Test:  [2500/3316]  eta: 6:48:01  loss: 1.0616 (1.1511)  acc1: 75.0000 (75.7147)  acc5: 93.7500 (91.4384)  time: 30.6572  data: 0.0036
Test:  [2530/3316]  eta: 6:28:39  loss: 1.1400 (1.1512)  acc1: 75.0000 (75.7137)  acc5: 93.7500 (91.4337)  time: 30.2304  data: 0.0036
Test:  [2510/3316]  eta: 6:41:53  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.7044)  acc5: 93.7500 (91.4352)  time: 30.5874  data: 0.0034
Test:  [2570/3316]  eta: 6:03:22  loss: 1.0130 (1.1513)  acc1: 75.0000 (75.7001)  acc5: 93.7500 (91.4260)  time: 29.5616  data: 0.0033
Test:  [2570/3316]  eta: 6:03:39  loss: 1.0130 (1.1513)  acc1: 75.0000 (75.7001)  acc5: 93.7500 (91.4260)  time: 29.3523  data: 0.0032
Test:  [2520/3316]  eta: 6:35:56  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.6942)  acc5: 93.7500 (91.4369)  time: 29.8063  data: 0.0032
Test:  [2510/3316]  eta: 6:42:55  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.7044)  acc5: 93.7500 (91.4352)  time: 30.2014  data: 0.0035
Test:  [2510/3316]  eta: 6:43:00  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.7044)  acc5: 93.7500 (91.4352)  time: 30.2376  data: 0.0035
Test:  [2510/3316]  eta: 6:43:01  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.7044)  acc5: 93.7500 (91.4352)  time: 30.1856  data: 0.0038
Test:  [2540/3316]  eta: 6:23:42  loss: 1.2597 (1.1519)  acc1: 75.0000 (75.7010)  acc5: 87.5000 (91.4133)  time: 29.7120  data: 0.0042
Test:  [2520/3316]  eta: 6:36:53  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.6942)  acc5: 93.7500 (91.4369)  time: 29.9707  data: 0.0041
Test:  [2580/3316]  eta: 5:58:29  loss: 1.0130 (1.1507)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4277)  time: 29.1410  data: 0.0039
Test:  [2580/3316]  eta: 5:58:46  loss: 1.0130 (1.1507)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4277)  time: 29.0679  data: 0.0050
Test:  [2530/3316]  eta: 6:30:57  loss: 1.1400 (1.1512)  acc1: 75.0000 (75.7137)  acc5: 93.7500 (91.4337)  time: 29.5958  data: 0.0047
Test:  [2520/3316]  eta: 6:37:51  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.6942)  acc5: 93.7500 (91.4369)  time: 29.4987  data: 0.0041
Test:  [2520/3316]  eta: 6:37:57  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.6942)  acc5: 93.7500 (91.4369)  time: 29.6210  data: 0.0042
Test:  [2520/3316]  eta: 6:37:58  loss: 1.2164 (1.1515)  acc1: 75.0000 (75.6942)  acc5: 93.7500 (91.4369)  time: 29.5248  data: 0.0044
Test:  [2550/3316]  eta: 6:18:42  loss: 1.1936 (1.1519)  acc1: 75.0000 (75.6958)  acc5: 93.7500 (91.4176)  time: 29.1128  data: 0.0042
Test:  [2530/3316]  eta: 6:31:51  loss: 1.1400 (1.1512)  acc1: 75.0000 (75.7137)  acc5: 93.7500 (91.4337)  time: 29.3604  data: 0.0044
Test:  [2590/3316]  eta: 5:53:34  loss: 1.2355 (1.1517)  acc1: 75.0000 (75.6947)  acc5: 93.7500 (91.4053)  time: 28.6169  data: 0.0040
Test:  [2590/3316]  eta: 5:53:50  loss: 1.2355 (1.1517)  acc1: 75.0000 (75.6947)  acc5: 93.7500 (91.4053)  time: 28.5193  data: 0.0049
Test:  [2540/3316]  eta: 6:25:55  loss: 1.2597 (1.1519)  acc1: 75.0000 (75.7010)  acc5: 87.5000 (91.4133)  time: 29.0736  data: 0.0043
Test:  [2530/3316]  eta: 6:32:47  loss: 1.1400 (1.1512)  acc1: 75.0000 (75.7137)  acc5: 93.7500 (91.4337)  time: 28.7724  data: 0.0036
Test:  [2530/3316]  eta: 6:32:53  loss: 1.1400 (1.1512)  acc1: 75.0000 (75.7137)  acc5: 93.7500 (91.4337)  time: 28.8830  data: 0.0035
Test:  [2530/3316]  eta: 6:32:54  loss: 1.1400 (1.1512)  acc1: 75.0000 (75.7137)  acc5: 93.7500 (91.4337)  time: 28.8495  data: 0.0036
Test:  [2560/3316]  eta: 6:13:42  loss: 0.9531 (1.1513)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.4242)  time: 28.5661  data: 0.0030
Test:  [2600/3316]  eta: 5:48:39  loss: 1.2822 (1.1519)  acc1: 68.7500 (75.6896)  acc5: 87.5000 (91.4023)  time: 28.1661  data: 0.0028
Test:  [2540/3316]  eta: 6:26:49  loss: 1.2597 (1.1519)  acc1: 75.0000 (75.7010)  acc5: 87.5000 (91.4133)  time: 28.8926  data: 0.0031
Test:  [2600/3316]  eta: 5:48:55  loss: 1.2822 (1.1519)  acc1: 68.7500 (75.6896)  acc5: 87.5000 (91.4023)  time: 28.1678  data: 0.0028
Test:  [2550/3316]  eta: 6:20:53  loss: 1.1936 (1.1519)  acc1: 75.0000 (75.6958)  acc5: 93.7500 (91.4176)  time: 28.7439  data: 0.0026
Test:  [2540/3316]  eta: 6:27:45  loss: 1.2597 (1.1519)  acc1: 75.0000 (75.7010)  acc5: 87.5000 (91.4133)  time: 28.9696  data: 0.0027
Test:  [2540/3316]  eta: 6:27:51  loss: 1.2597 (1.1519)  acc1: 75.0000 (75.7010)  acc5: 87.5000 (91.4133)  time: 29.0122  data: 0.0026
Test:  [2540/3316]  eta: 6:27:52  loss: 1.2597 (1.1519)  acc1: 75.0000 (75.7010)  acc5: 87.5000 (91.4133)  time: 29.0241  data: 0.0026
Test:  [2570/3316]  eta: 6:08:44  loss: 1.0130 (1.1513)  acc1: 75.0000 (75.7001)  acc5: 93.7500 (91.4260)  time: 28.8045  data: 0.0025
Test:  [2610/3316]  eta: 5:43:45  loss: 1.1982 (1.1521)  acc1: 75.0000 (75.6870)  acc5: 93.7500 (91.4042)  time: 28.4243  data: 0.0026
Test:  [2550/3316]  eta: 6:21:48  loss: 1.1936 (1.1519)  acc1: 75.0000 (75.6958)  acc5: 93.7500 (91.4176)  time: 29.0859  data: 0.0025
Test:  [2610/3316]  eta: 5:44:01  loss: 1.1982 (1.1521)  acc1: 75.0000 (75.6870)  acc5: 93.7500 (91.4042)  time: 28.4569  data: 0.0029
Test:  [2560/3316]  eta: 6:15:53  loss: 0.9531 (1.1513)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.4242)  time: 28.9417  data: 0.0028
Test:  [2550/3316]  eta: 6:22:43  loss: 1.1936 (1.1519)  acc1: 75.0000 (75.6958)  acc5: 93.7500 (91.4176)  time: 29.2813  data: 0.0026
Test:  [2550/3316]  eta: 6:22:51  loss: 1.1936 (1.1519)  acc1: 75.0000 (75.6958)  acc5: 93.7500 (91.4176)  time: 29.5520  data: 0.0026
Test:  [2550/3316]  eta: 6:22:51  loss: 1.1936 (1.1519)  acc1: 75.0000 (75.6958)  acc5: 93.7500 (91.4176)  time: 29.5097  data: 0.0026
Test:  [2580/3316]  eta: 6:03:47  loss: 1.0130 (1.1507)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4277)  time: 29.3578  data: 0.0039
Test:  [2620/3316]  eta: 5:38:53  loss: 1.1660 (1.1520)  acc1: 75.0000 (75.6963)  acc5: 93.7500 (91.4036)  time: 28.9100  data: 0.0035
Test:  [2560/3316]  eta: 6:16:48  loss: 0.9531 (1.1513)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.4242)  time: 29.5419  data: 0.0039
Test:  [2620/3316]  eta: 5:39:09  loss: 1.1660 (1.1520)  acc1: 75.0000 (75.6963)  acc5: 93.7500 (91.4036)  time: 28.9966  data: 0.0040
Test:  [2570/3316]  eta: 6:10:54  loss: 1.0130 (1.1513)  acc1: 75.0000 (75.7001)  acc5: 93.7500 (91.4260)  time: 29.4214  data: 0.0043
Test:  [2560/3316]  eta: 6:17:43  loss: 0.9531 (1.1513)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.4242)  time: 29.6008  data: 0.0045
Test:  [2560/3316]  eta: 6:17:49  loss: 0.9531 (1.1513)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.4242)  time: 29.6450  data: 0.0044
Test:  [2560/3316]  eta: 6:17:49  loss: 0.9531 (1.1513)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.4242)  time: 29.5667  data: 0.0045
Test:  [2590/3316]  eta: 5:58:49  loss: 1.2355 (1.1517)  acc1: 75.0000 (75.6947)  acc5: 93.7500 (91.4053)  time: 29.3699  data: 0.0050
Test:  [2630/3316]  eta: 5:33:59  loss: 1.0876 (1.1520)  acc1: 75.0000 (75.6913)  acc5: 93.7500 (91.3982)  time: 28.9007  data: 0.0042
Test:  [2570/3316]  eta: 6:11:47  loss: 1.0130 (1.1513)  acc1: 75.0000 (75.7001)  acc5: 93.7500 (91.4260)  time: 29.4958  data: 0.0047
Test:  [2630/3316]  eta: 5:34:15  loss: 1.0876 (1.1520)  acc1: 75.0000 (75.6913)  acc5: 93.7500 (91.3982)  time: 28.9692  data: 0.0042
Test:  [2580/3316]  eta: 6:05:54  loss: 1.0130 (1.1507)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4277)  time: 29.4460  data: 0.0046
Test:  [2570/3316]  eta: 6:12:41  loss: 1.0130 (1.1513)  acc1: 75.0000 (75.7001)  acc5: 93.7500 (91.4260)  time: 29.5880  data: 0.0047
Test:  [2570/3316]  eta: 6:12:48  loss: 1.0130 (1.1513)  acc1: 75.0000 (75.7001)  acc5: 93.7500 (91.4260)  time: 29.3910  data: 0.0044
Test:  [2570/3316]  eta: 6:12:48  loss: 1.0130 (1.1513)  acc1: 75.0000 (75.7001)  acc5: 93.7500 (91.4260)  time: 29.3364  data: 0.0046
Test:  [2600/3316]  eta: 5:53:50  loss: 1.2822 (1.1519)  acc1: 68.7500 (75.6896)  acc5: 87.5000 (91.4023)  time: 28.9755  data: 0.0039
Test:  [2640/3316]  eta: 5:29:06  loss: 1.0775 (1.1519)  acc1: 75.0000 (75.6981)  acc5: 93.7500 (91.4048)  time: 28.6111  data: 0.0035
Test:  [2580/3316]  eta: 6:06:46  loss: 1.0130 (1.1507)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4277)  time: 29.1224  data: 0.0034
Test:  [2640/3316]  eta: 5:29:21  loss: 1.0775 (1.1519)  acc1: 75.0000 (75.6981)  acc5: 93.7500 (91.4048)  time: 28.6043  data: 0.0031
Test:  [2590/3316]  eta: 6:00:54  loss: 1.2355 (1.1517)  acc1: 75.0000 (75.6947)  acc5: 93.7500 (91.4053)  time: 29.1576  data: 0.0031
Test:  [2580/3316]  eta: 6:07:40  loss: 1.0130 (1.1507)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4277)  time: 29.3523  data: 0.0028
Test:  [2580/3316]  eta: 6:07:46  loss: 1.0130 (1.1507)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4277)  time: 29.3938  data: 0.0027
Test:  [2580/3316]  eta: 6:07:46  loss: 1.0130 (1.1507)  acc1: 75.0000 (75.7168)  acc5: 93.7500 (91.4277)  time: 29.3928  data: 0.0028
Test:  [2610/3316]  eta: 5:48:53  loss: 1.1982 (1.1521)  acc1: 75.0000 (75.6870)  acc5: 93.7500 (91.4042)  time: 29.0450  data: 0.0028
Test:  [2650/3316]  eta: 5:24:12  loss: 1.0659 (1.1523)  acc1: 75.0000 (75.6790)  acc5: 93.7500 (91.3971)  time: 28.5776  data: 0.0028
Test:  [2590/3316]  eta: 6:01:45  loss: 1.2355 (1.1517)  acc1: 75.0000 (75.6947)  acc5: 93.7500 (91.4053)  time: 29.2160  data: 0.0028
Test:  [2650/3316]  eta: 5:24:28  loss: 1.0659 (1.1523)  acc1: 75.0000 (75.6790)  acc5: 93.7500 (91.3971)  time: 28.7643  data: 0.0032
Test:  [2600/3316]  eta: 5:55:54  loss: 1.2822 (1.1519)  acc1: 68.7500 (75.6896)  acc5: 87.5000 (91.4023)  time: 29.2907  data: 0.0030
Test:  [2590/3316]  eta: 6:02:39  loss: 1.2355 (1.1517)  acc1: 75.0000 (75.6947)  acc5: 93.7500 (91.4053)  time: 29.4937  data: 0.0029
Test:  [2590/3316]  eta: 6:02:45  loss: 1.2355 (1.1517)  acc1: 75.0000 (75.6947)  acc5: 93.7500 (91.4053)  time: 29.4757  data: 0.0028
Test:  [2590/3316]  eta: 6:02:45  loss: 1.2355 (1.1517)  acc1: 75.0000 (75.6947)  acc5: 93.7500 (91.4053)  time: 29.5375  data: 0.0032
Test:  [2620/3316]  eta: 5:43:55  loss: 1.1660 (1.1520)  acc1: 75.0000 (75.6963)  acc5: 93.7500 (91.4036)  time: 29.2019  data: 0.0027
Test:  [2660/3316]  eta: 5:19:19  loss: 1.0608 (1.1518)  acc1: 75.0000 (75.6882)  acc5: 93.7500 (91.4083)  time: 28.7150  data: 0.0030
Test:  [2600/3316]  eta: 5:56:44  loss: 1.2822 (1.1519)  acc1: 68.7500 (75.6896)  acc5: 87.5000 (91.4023)  time: 29.2924  data: 0.0028
Test:  [2660/3316]  eta: 5:19:34  loss: 1.0608 (1.1518)  acc1: 75.0000 (75.6882)  acc5: 93.7500 (91.4083)  time: 28.8020  data: 0.0032
Test:  [2610/3316]  eta: 5:50:54  loss: 1.1982 (1.1521)  acc1: 75.0000 (75.6870)  acc5: 93.7500 (91.4042)  time: 29.2692  data: 0.0032
Test:  [2600/3316]  eta: 5:57:38  loss: 1.2822 (1.1519)  acc1: 68.7500 (75.6896)  acc5: 87.5000 (91.4023)  time: 29.6086  data: 0.0029
Test:  [2600/3316]  eta: 5:57:44  loss: 1.2822 (1.1519)  acc1: 68.7500 (75.6896)  acc5: 87.5000 (91.4023)  time: 29.5592  data: 0.0033
Test:  [2600/3316]  eta: 5:57:44  loss: 1.2822 (1.1519)  acc1: 68.7500 (75.6896)  acc5: 87.5000 (91.4023)  time: 29.5190  data: 0.0029
Test:  [2630/3316]  eta: 5:38:57  loss: 1.0876 (1.1520)  acc1: 75.0000 (75.6913)  acc5: 93.7500 (91.3982)  time: 29.1680  data: 0.0027
Test:  [2670/3316]  eta: 5:14:26  loss: 1.0784 (1.1522)  acc1: 81.2500 (75.6950)  acc5: 87.5000 (91.3913)  time: 28.8211  data: 0.0029
Test:  [2610/3316]  eta: 5:51:44  loss: 1.1982 (1.1521)  acc1: 75.0000 (75.6870)  acc5: 93.7500 (91.4042)  time: 29.3131  data: 0.0026
Test:  [2670/3316]  eta: 5:14:41  loss: 1.0784 (1.1522)  acc1: 81.2500 (75.6950)  acc5: 87.5000 (91.3913)  time: 28.7726  data: 0.0030
Test:  [2620/3316]  eta: 5:45:55  loss: 1.1660 (1.1520)  acc1: 75.0000 (75.6963)  acc5: 93.7500 (91.4036)  time: 29.2523  data: 0.0031
Test:  [2610/3316]  eta: 5:52:39  loss: 1.1982 (1.1521)  acc1: 75.0000 (75.6870)  acc5: 93.7500 (91.4042)  time: 29.7580  data: 0.0032
Test:  [2610/3316]  eta: 5:52:44  loss: 1.1982 (1.1521)  acc1: 75.0000 (75.6870)  acc5: 93.7500 (91.4042)  time: 29.6576  data: 0.0036
Test:  [2610/3316]  eta: 5:52:44  loss: 1.1982 (1.1521)  acc1: 75.0000 (75.6870)  acc5: 93.7500 (91.4042)  time: 29.6878  data: 0.0037
Test:  [2640/3316]  eta: 5:34:00  loss: 1.0775 (1.1519)  acc1: 75.0000 (75.6981)  acc5: 93.7500 (91.4048)  time: 29.3260  data: 0.0037
Test:  [2680/3316]  eta: 5:09:34  loss: 1.1330 (1.1524)  acc1: 75.0000 (75.6947)  acc5: 87.5000 (91.3908)  time: 28.9687  data: 0.0033
Test:  [2620/3316]  eta: 5:46:45  loss: 1.1660 (1.1520)  acc1: 75.0000 (75.6963)  acc5: 93.7500 (91.4036)  time: 29.6743  data: 0.0046
Test:  [2680/3316]  eta: 5:09:49  loss: 1.1330 (1.1524)  acc1: 75.0000 (75.6947)  acc5: 87.5000 (91.3908)  time: 29.0605  data: 0.0040
Test:  [2630/3316]  eta: 5:40:56  loss: 1.0876 (1.1520)  acc1: 75.0000 (75.6913)  acc5: 93.7500 (91.3982)  time: 29.5942  data: 0.0047
Test:  [2620/3316]  eta: 5:47:37  loss: 1.1660 (1.1520)  acc1: 75.0000 (75.6963)  acc5: 93.7500 (91.4036)  time: 29.6880  data: 0.0046
Test:  [2620/3316]  eta: 5:47:43  loss: 1.1660 (1.1520)  acc1: 75.0000 (75.6963)  acc5: 93.7500 (91.4036)  time: 29.6304  data: 0.0046
Test:  [2620/3316]  eta: 5:47:43  loss: 1.1660 (1.1520)  acc1: 75.0000 (75.6963)  acc5: 93.7500 (91.4036)  time: 29.6733  data: 0.0047
Test:  [2690/3316]  eta: 5:04:40  loss: 1.1652 (1.1520)  acc1: 75.0000 (75.7014)  acc5: 87.5000 (91.3949)  time: 28.8973  data: 0.0042
Test:  [2650/3316]  eta: 5:29:02  loss: 1.0659 (1.1523)  acc1: 75.0000 (75.6790)  acc5: 93.7500 (91.3971)  time: 29.2952  data: 0.0046
Test:  [2630/3316]  eta: 5:41:45  loss: 1.0876 (1.1520)  acc1: 75.0000 (75.6913)  acc5: 93.7500 (91.3982)  time: 29.6315  data: 0.0051
Test:  [2690/3316]  eta: 5:04:55  loss: 1.1652 (1.1520)  acc1: 75.0000 (75.7014)  acc5: 87.5000 (91.3949)  time: 28.9881  data: 0.0045
Test:  [2640/3316]  eta: 5:35:57  loss: 1.0775 (1.1519)  acc1: 75.0000 (75.6981)  acc5: 93.7500 (91.4048)  time: 29.5451  data: 0.0046
Test:  [2630/3316]  eta: 5:42:36  loss: 1.0876 (1.1520)  acc1: 75.0000 (75.6913)  acc5: 93.7500 (91.3982)  time: 29.4568  data: 0.0040
Test:  [2630/3316]  eta: 5:42:42  loss: 1.0876 (1.1520)  acc1: 75.0000 (75.6913)  acc5: 93.7500 (91.3982)  time: 29.4459  data: 0.0039
Test:  [2630/3316]  eta: 5:42:42  loss: 1.0876 (1.1520)  acc1: 75.0000 (75.6913)  acc5: 93.7500 (91.3982)  time: 29.4427  data: 0.0038
Test:  [2700/3316]  eta: 4:59:47  loss: 1.1743 (1.1522)  acc1: 75.0000 (75.6919)  acc5: 87.5000 (91.3990)  time: 28.6886  data: 0.0037
Test:  [2660/3316]  eta: 5:24:05  loss: 1.0608 (1.1518)  acc1: 75.0000 (75.6882)  acc5: 93.7500 (91.4083)  time: 29.0870  data: 0.0037
Test:  [2700/3316]  eta: 5:00:02  loss: 1.1743 (1.1522)  acc1: 75.0000 (75.6919)  acc5: 87.5000 (91.3990)  time: 28.7503  data: 0.0034
Test:  [2640/3316]  eta: 5:36:45  loss: 1.0775 (1.1519)  acc1: 75.0000 (75.6981)  acc5: 93.7500 (91.4048)  time: 29.3741  data: 0.0033
Test:  [2650/3316]  eta: 5:30:58  loss: 1.0659 (1.1523)  acc1: 75.0000 (75.6790)  acc5: 93.7500 (91.3971)  time: 29.3734  data: 0.0028
Test:  [2640/3316]  eta: 5:37:36  loss: 1.0775 (1.1519)  acc1: 75.0000 (75.6981)  acc5: 93.7500 (91.4048)  time: 29.6199  data: 0.0028
Test:  [2640/3316]  eta: 5:37:42  loss: 1.0775 (1.1519)  acc1: 75.0000 (75.6981)  acc5: 93.7500 (91.4048)  time: 29.6636  data: 0.0029
Test:  [2640/3316]  eta: 5:37:42  loss: 1.0775 (1.1519)  acc1: 75.0000 (75.6981)  acc5: 93.7500 (91.4048)  time: 29.6658  data: 0.0031
Test:  [2710/3316]  eta: 4:54:55  loss: 1.1675 (1.1522)  acc1: 75.0000 (75.6939)  acc5: 87.5000 (91.3939)  time: 28.8655  data: 0.0031
Test:  [2670/3316]  eta: 5:19:08  loss: 1.0784 (1.1522)  acc1: 81.2500 (75.6950)  acc5: 87.5000 (91.3913)  time: 29.3839  data: 0.0028
Test:  [2710/3316]  eta: 4:55:10  loss: 1.1675 (1.1522)  acc1: 75.0000 (75.6939)  acc5: 87.5000 (91.3939)  time: 28.9968  data: 0.0030
Test:  [2650/3316]  eta: 5:31:46  loss: 1.0659 (1.1523)  acc1: 75.0000 (75.6790)  acc5: 93.7500 (91.3971)  time: 29.6619  data: 0.0031
Test:  [2660/3316]  eta: 5:25:59  loss: 1.0608 (1.1518)  acc1: 75.0000 (75.6882)  acc5: 93.7500 (91.4083)  time: 29.5975  data: 0.0029
Test:  [2650/3316]  eta: 5:32:38  loss: 1.0659 (1.1523)  acc1: 75.0000 (75.6790)  acc5: 93.7500 (91.3971)  time: 30.0791  data: 0.0031
Test:  [2720/3316]  eta: 4:50:03  loss: 1.0409 (1.1520)  acc1: 75.0000 (75.6914)  acc5: 93.7500 (91.3979)  time: 29.2495  data: 0.0036
Test:  [2650/3316]  eta: 5:32:43  loss: 1.0659 (1.1523)  acc1: 75.0000 (75.6790)  acc5: 93.7500 (91.3971)  time: 30.0638  data: 0.0036
Test:  [2650/3316]  eta: 5:32:43  loss: 1.0659 (1.1523)  acc1: 75.0000 (75.6790)  acc5: 93.7500 (91.3971)  time: 30.1088  data: 0.0037
Test:  [2680/3316]  eta: 5:14:12  loss: 1.1330 (1.1524)  acc1: 75.0000 (75.6947)  acc5: 87.5000 (91.3908)  time: 29.6699  data: 0.0032
Test:  [2720/3316]  eta: 4:50:18  loss: 1.0409 (1.1520)  acc1: 75.0000 (75.6914)  acc5: 93.7500 (91.3979)  time: 29.2367  data: 0.0039
Test:  [2660/3316]  eta: 5:26:47  loss: 1.0608 (1.1518)  acc1: 75.0000 (75.6882)  acc5: 93.7500 (91.4083)  time: 29.8891  data: 0.0043
Test:  [2670/3316]  eta: 5:21:01  loss: 1.0784 (1.1522)  acc1: 81.2500 (75.6950)  acc5: 87.5000 (91.3913)  time: 29.7707  data: 0.0044
Test:  [2660/3316]  eta: 5:27:37  loss: 1.0608 (1.1518)  acc1: 75.0000 (75.6882)  acc5: 93.7500 (91.4083)  time: 29.8939  data: 0.0047
Test:  [2730/3316]  eta: 4:45:10  loss: 1.0028 (1.1521)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.3928)  time: 29.0420  data: 0.0046
Test:  [2660/3316]  eta: 5:27:41  loss: 1.0608 (1.1518)  acc1: 75.0000 (75.6882)  acc5: 93.7500 (91.4083)  time: 29.7787  data: 0.0046
Test:  [2660/3316]  eta: 5:27:42  loss: 1.0608 (1.1518)  acc1: 75.0000 (75.6882)  acc5: 93.7500 (91.4083)  time: 29.8055  data: 0.0048
Test:  [2690/3316]  eta: 5:09:14  loss: 1.1652 (1.1520)  acc1: 75.0000 (75.7014)  acc5: 87.5000 (91.3949)  time: 29.3897  data: 0.0043
Test:  [2730/3316]  eta: 4:45:24  loss: 1.0028 (1.1521)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.3928)  time: 28.9657  data: 0.0046
Test:  [2670/3316]  eta: 5:21:46  loss: 1.0784 (1.1522)  acc1: 81.2500 (75.6950)  acc5: 87.5000 (91.3913)  time: 29.4643  data: 0.0046
Test:  [2680/3316]  eta: 5:16:02  loss: 1.1330 (1.1524)  acc1: 75.0000 (75.6947)  acc5: 87.5000 (91.3908)  time: 29.4828  data: 0.0044
Test:  [2740/3316]  eta: 4:40:17  loss: 1.0055 (1.1517)  acc1: 75.0000 (75.7000)  acc5: 93.7500 (91.3968)  time: 28.6483  data: 0.0041
Test:  [2670/3316]  eta: 5:22:36  loss: 1.0784 (1.1522)  acc1: 81.2500 (75.6950)  acc5: 87.5000 (91.3913)  time: 29.4626  data: 0.0045
Test:  [2670/3316]  eta: 5:22:40  loss: 1.0784 (1.1522)  acc1: 81.2500 (75.6950)  acc5: 87.5000 (91.3913)  time: 29.3817  data: 0.0040
Test:  [2670/3316]  eta: 5:22:40  loss: 1.0784 (1.1522)  acc1: 81.2500 (75.6950)  acc5: 87.5000 (91.3913)  time: 29.3509  data: 0.0042
Test:  [2700/3316]  eta: 5:04:16  loss: 1.1743 (1.1522)  acc1: 75.0000 (75.6919)  acc5: 87.5000 (91.3990)  time: 29.0545  data: 0.0039
Test:  [2740/3316]  eta: 4:40:31  loss: 1.0055 (1.1517)  acc1: 75.0000 (75.7000)  acc5: 93.7500 (91.3968)  time: 28.7942  data: 0.0037
Test:  [2680/3316]  eta: 5:16:46  loss: 1.1330 (1.1524)  acc1: 75.0000 (75.6947)  acc5: 87.5000 (91.3908)  time: 29.2009  data: 0.0035
Test:  [2690/3316]  eta: 5:11:03  loss: 1.1652 (1.1520)  acc1: 75.0000 (75.7014)  acc5: 87.5000 (91.3949)  time: 29.3217  data: 0.0030
Test:  [2750/3316]  eta: 4:35:25  loss: 1.0318 (1.1514)  acc1: 75.0000 (75.6997)  acc5: 93.7500 (91.4009)  time: 28.8227  data: 0.0028
Test:  [2680/3316]  eta: 5:17:36  loss: 1.1330 (1.1524)  acc1: 75.0000 (75.6947)  acc5: 87.5000 (91.3908)  time: 29.5946  data: 0.0030
Test:  [2710/3316]  eta: 4:59:19  loss: 1.1675 (1.1522)  acc1: 75.0000 (75.6939)  acc5: 87.5000 (91.3939)  time: 29.1447  data: 0.0031
Test:  [2680/3316]  eta: 5:17:40  loss: 1.1330 (1.1524)  acc1: 75.0000 (75.6947)  acc5: 87.5000 (91.3908)  time: 29.5804  data: 0.0028
Test:  [2680/3316]  eta: 5:17:40  loss: 1.1330 (1.1524)  acc1: 75.0000 (75.6947)  acc5: 87.5000 (91.3908)  time: 29.5295  data: 0.0030
Test:  [2750/3316]  eta: 4:35:38  loss: 1.0318 (1.1514)  acc1: 75.0000 (75.6997)  acc5: 93.7500 (91.4009)  time: 28.8768  data: 0.0029
Test:  [2690/3316]  eta: 5:11:46  loss: 1.1652 (1.1520)  acc1: 75.0000 (75.7014)  acc5: 87.5000 (91.3949)  time: 29.3817  data: 0.0029
Test:  [2700/3316]  eta: 5:06:04  loss: 1.1743 (1.1522)  acc1: 75.0000 (75.6919)  acc5: 87.5000 (91.3990)  time: 29.4514  data: 0.0028
Test:  [2760/3316]  eta: 4:30:32  loss: 1.1812 (1.1520)  acc1: 75.0000 (75.6836)  acc5: 93.7500 (91.3980)  time: 28.9072  data: 0.0028
Test:  [2690/3316]  eta: 5:12:35  loss: 1.1652 (1.1520)  acc1: 75.0000 (75.7014)  acc5: 87.5000 (91.3949)  time: 29.6293  data: 0.0030
Test:  [2720/3316]  eta: 4:54:23  loss: 1.0409 (1.1520)  acc1: 75.0000 (75.6914)  acc5: 93.7500 (91.3979)  time: 29.5219  data: 0.0032
Test:  [2690/3316]  eta: 5:12:40  loss: 1.1652 (1.1520)  acc1: 75.0000 (75.7014)  acc5: 87.5000 (91.3949)  time: 29.8019  data: 0.0028
Test:  [2690/3316]  eta: 5:12:41  loss: 1.1652 (1.1520)  acc1: 75.0000 (75.7014)  acc5: 87.5000 (91.3949)  time: 29.9499  data: 0.0029
Test:  [2760/3316]  eta: 4:30:46  loss: 1.1812 (1.1520)  acc1: 75.0000 (75.6836)  acc5: 93.7500 (91.3980)  time: 29.1180  data: 0.0035
Test:  [2700/3316]  eta: 5:06:47  loss: 1.1743 (1.1522)  acc1: 75.0000 (75.6919)  acc5: 87.5000 (91.3990)  time: 29.6145  data: 0.0032
Test:  [2710/3316]  eta: 5:01:06  loss: 1.1675 (1.1522)  acc1: 75.0000 (75.6939)  acc5: 87.5000 (91.3939)  time: 29.7094  data: 0.0045
Test:  [2770/3316]  eta: 4:25:40  loss: 1.2545 (1.1524)  acc1: 75.0000 (75.6699)  acc5: 93.7500 (91.3907)  time: 29.0003  data: 0.0048
Test:  [2700/3316]  eta: 5:07:35  loss: 1.1743 (1.1522)  acc1: 75.0000 (75.6919)  acc5: 87.5000 (91.3990)  time: 29.7074  data: 0.0045
Test:  [2730/3316]  eta: 4:49:26  loss: 1.0028 (1.1521)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.3928)  time: 29.4630  data: 0.0045
Test:  [2700/3316]  eta: 5:07:39  loss: 1.1743 (1.1522)  acc1: 75.0000 (75.6919)  acc5: 87.5000 (91.3990)  time: 29.6711  data: 0.0046
Test:  [2700/3316]  eta: 5:07:40  loss: 1.1743 (1.1522)  acc1: 75.0000 (75.6919)  acc5: 87.5000 (91.3990)  time: 29.8188  data: 0.0045
Test:  [2770/3316]  eta: 4:25:53  loss: 1.2545 (1.1524)  acc1: 75.0000 (75.6699)  acc5: 93.7500 (91.3907)  time: 29.0400  data: 0.0047
Test:  [2710/3316]  eta: 5:01:47  loss: 1.1675 (1.1522)  acc1: 75.0000 (75.6939)  acc5: 87.5000 (91.3939)  time: 29.4675  data: 0.0040
Test:  [2720/3316]  eta: 4:56:06  loss: 1.0409 (1.1520)  acc1: 75.0000 (75.6914)  acc5: 93.7500 (91.3979)  time: 29.5314  data: 0.0048
Test:  [2780/3316]  eta: 4:20:46  loss: 1.2132 (1.1520)  acc1: 75.0000 (75.6877)  acc5: 93.7500 (91.3947)  time: 28.7850  data: 0.0048
Test:  [2710/3316]  eta: 5:02:34  loss: 1.1675 (1.1522)  acc1: 75.0000 (75.6939)  acc5: 87.5000 (91.3939)  time: 29.4610  data: 0.0045
Test:  [2740/3316]  eta: 4:44:28  loss: 1.0055 (1.1517)  acc1: 75.0000 (75.7000)  acc5: 93.7500 (91.3968)  time: 28.9907  data: 0.0044
Test:  [2710/3316]  eta: 5:02:38  loss: 1.1675 (1.1522)  acc1: 75.0000 (75.6939)  acc5: 87.5000 (91.3939)  time: 29.2765  data: 0.0046
Test:  [2710/3316]  eta: 5:02:38  loss: 1.1675 (1.1522)  acc1: 75.0000 (75.6939)  acc5: 87.5000 (91.3939)  time: 29.3187  data: 0.0044
Test:  [2780/3316]  eta: 4:21:00  loss: 1.2132 (1.1520)  acc1: 75.0000 (75.6877)  acc5: 93.7500 (91.3947)  time: 28.5849  data: 0.0040
Test:  [2720/3316]  eta: 4:56:46  loss: 1.0409 (1.1520)  acc1: 75.0000 (75.6914)  acc5: 93.7500 (91.3979)  time: 29.0863  data: 0.0035
Test:  [2730/3316]  eta: 4:51:06  loss: 1.0028 (1.1521)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.3928)  time: 29.0601  data: 0.0030
Test:  [2790/3316]  eta: 4:15:53  loss: 0.9861 (1.1513)  acc1: 81.2500 (75.7143)  acc5: 93.7500 (91.4009)  time: 28.4612  data: 0.0027
Test:  [2720/3316]  eta: 4:57:32  loss: 1.0409 (1.1520)  acc1: 75.0000 (75.6914)  acc5: 93.7500 (91.3979)  time: 29.0655  data: 0.0027
Test:  [2750/3316]  eta: 4:39:30  loss: 1.0318 (1.1514)  acc1: 75.0000 (75.6997)  acc5: 93.7500 (91.4009)  time: 28.8762  data: 0.0026
Test:  [2720/3316]  eta: 4:57:36  loss: 1.0409 (1.1520)  acc1: 75.0000 (75.6914)  acc5: 93.7500 (91.3979)  time: 29.1425  data: 0.0026
Test:  [2720/3316]  eta: 4:57:37  loss: 1.0409 (1.1520)  acc1: 75.0000 (75.6914)  acc5: 93.7500 (91.3979)  time: 29.2000  data: 0.0024
Test:  [2790/3316]  eta: 4:16:06  loss: 0.9861 (1.1513)  acc1: 81.2500 (75.7143)  acc5: 93.7500 (91.4009)  time: 28.5112  data: 0.0025
Test:  [2730/3316]  eta: 4:51:45  loss: 1.0028 (1.1521)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.3928)  time: 28.9816  data: 0.0025
Test:  [2740/3316]  eta: 4:46:07  loss: 1.0055 (1.1517)  acc1: 75.0000 (75.7000)  acc5: 93.7500 (91.3968)  time: 29.0014  data: 0.0026
Test:  [2800/3316]  eta: 4:11:00  loss: 0.9719 (1.1510)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4182)  time: 28.4617  data: 0.0026
Test:  [2730/3316]  eta: 4:52:31  loss: 1.0028 (1.1521)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.3928)  time: 29.0574  data: 0.0027
Test:  [2760/3316]  eta: 4:34:32  loss: 1.1812 (1.1520)  acc1: 75.0000 (75.6836)  acc5: 93.7500 (91.3980)  time: 28.9092  data: 0.0026
Test:  [2730/3316]  eta: 4:52:35  loss: 1.0028 (1.1521)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.3928)  time: 29.1474  data: 0.0024
Test:  [2730/3316]  eta: 4:52:36  loss: 1.0028 (1.1521)  acc1: 81.2500 (75.6980)  acc5: 93.7500 (91.3928)  time: 29.1819  data: 0.0024
Test:  [2800/3316]  eta: 4:11:13  loss: 0.9719 (1.1510)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4182)  time: 28.5665  data: 0.0024
Test:  [2740/3316]  eta: 4:46:46  loss: 1.0055 (1.1517)  acc1: 75.0000 (75.7000)  acc5: 93.7500 (91.3968)  time: 29.2718  data: 0.0024
Test:  [2750/3316]  eta: 4:41:08  loss: 1.0318 (1.1514)  acc1: 75.0000 (75.6997)  acc5: 93.7500 (91.4009)  time: 29.2102  data: 0.0035
Test:  [2810/3316]  eta: 4:06:08  loss: 1.1034 (1.1513)  acc1: 68.7500 (75.6915)  acc5: 93.7500 (91.4199)  time: 28.7302  data: 0.0038
Test:  [2740/3316]  eta: 4:47:30  loss: 1.0055 (1.1517)  acc1: 75.0000 (75.7000)  acc5: 93.7500 (91.3968)  time: 29.3161  data: 0.0045
Test:  [2770/3316]  eta: 4:29:36  loss: 1.2545 (1.1524)  acc1: 75.0000 (75.6699)  acc5: 93.7500 (91.3907)  time: 29.1911  data: 0.0042
Test:  [2740/3316]  eta: 4:47:35  loss: 1.0055 (1.1517)  acc1: 75.0000 (75.7000)  acc5: 93.7500 (91.3968)  time: 29.4839  data: 0.0041
Test:  [2810/3316]  eta: 4:06:21  loss: 1.1034 (1.1513)  acc1: 68.7500 (75.6915)  acc5: 93.7500 (91.4199)  time: 28.8584  data: 0.0042
Test:  [2740/3316]  eta: 4:47:36  loss: 1.0055 (1.1517)  acc1: 75.0000 (75.7000)  acc5: 93.7500 (91.3968)  time: 29.4159  data: 0.0038
Test:  [2750/3316]  eta: 4:41:45  loss: 1.0318 (1.1514)  acc1: 75.0000 (75.6997)  acc5: 93.7500 (91.4009)  time: 29.2879  data: 0.0041
Test:  [2760/3316]  eta: 4:36:08  loss: 1.1812 (1.1520)  acc1: 75.0000 (75.6836)  acc5: 93.7500 (91.3980)  time: 29.1238  data: 0.0042
Test:  [2820/3316]  eta: 4:01:14  loss: 1.0460 (1.1512)  acc1: 75.0000 (75.6890)  acc5: 93.7500 (91.4148)  time: 28.6098  data: 0.0041
Test:  [2750/3316]  eta: 4:42:28  loss: 1.0318 (1.1514)  acc1: 75.0000 (75.6997)  acc5: 93.7500 (91.4009)  time: 29.0703  data: 0.0044
Test:  [2780/3316]  eta: 4:24:37  loss: 1.2132 (1.1520)  acc1: 75.0000 (75.6877)  acc5: 93.7500 (91.3947)  time: 28.9608  data: 0.0043
Test:  [2820/3316]  eta: 4:01:27  loss: 1.0460 (1.1512)  acc1: 75.0000 (75.6890)  acc5: 93.7500 (91.4148)  time: 28.7002  data: 0.0042
Test:  [2750/3316]  eta: 4:42:33  loss: 1.0318 (1.1514)  acc1: 75.0000 (75.6997)  acc5: 93.7500 (91.4009)  time: 29.2845  data: 0.0042
Test:  [2750/3316]  eta: 4:42:33  loss: 1.0318 (1.1514)  acc1: 75.0000 (75.6997)  acc5: 93.7500 (91.4009)  time: 29.1908  data: 0.0037
Test:  [2760/3316]  eta: 4:36:44  loss: 1.1812 (1.1520)  acc1: 75.0000 (75.6836)  acc5: 93.7500 (91.3980)  time: 28.7645  data: 0.0040
Test:  [2770/3316]  eta: 4:31:07  loss: 1.2545 (1.1524)  acc1: 75.0000 (75.6699)  acc5: 93.7500 (91.3907)  time: 28.6768  data: 0.0031
Test:  [2830/3316]  eta: 3:56:21  loss: 1.0308 (1.1509)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4143)  time: 28.2150  data: 0.0027
Test:  [2760/3316]  eta: 4:37:26  loss: 1.1812 (1.1520)  acc1: 75.0000 (75.6836)  acc5: 93.7500 (91.3980)  time: 28.7440  data: 0.0025
Test:  [2790/3316]  eta: 4:19:39  loss: 0.9861 (1.1513)  acc1: 81.2500 (75.7143)  acc5: 93.7500 (91.4009)  time: 28.5774  data: 0.0025
Test:  [2830/3316]  eta: 3:56:33  loss: 1.0308 (1.1509)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4143)  time: 28.3479  data: 0.0024
Test:  [2760/3316]  eta: 4:37:32  loss: 1.1812 (1.1520)  acc1: 75.0000 (75.6836)  acc5: 93.7500 (91.3980)  time: 28.8826  data: 0.0025
Test:  [2760/3316]  eta: 4:37:32  loss: 1.1812 (1.1520)  acc1: 75.0000 (75.6836)  acc5: 93.7500 (91.3980)  time: 28.8960  data: 0.0027
Test:  [2770/3316]  eta: 4:31:44  loss: 1.2545 (1.1524)  acc1: 75.0000 (75.6699)  acc5: 93.7500 (91.3907)  time: 28.8610  data: 0.0023
Test:  [2780/3316]  eta: 4:26:08  loss: 1.2132 (1.1520)  acc1: 75.0000 (75.6877)  acc5: 93.7500 (91.3947)  time: 28.9424  data: 0.0026
Test:  [2840/3316]  eta: 3:51:28  loss: 1.0997 (1.1510)  acc1: 75.0000 (75.7018)  acc5: 87.5000 (91.4093)  time: 28.4444  data: 0.0026
Test:  [2800/3316]  eta: 4:14:42  loss: 0.9719 (1.1510)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4182)  time: 28.8467  data: 0.0025
Test:  [2770/3316]  eta: 4:32:25  loss: 1.2545 (1.1524)  acc1: 75.0000 (75.6699)  acc5: 93.7500 (91.3907)  time: 29.0298  data: 0.0025
Test:  [2840/3316]  eta: 3:51:40  loss: 1.0997 (1.1510)  acc1: 75.0000 (75.7018)  acc5: 87.5000 (91.4093)  time: 28.4389  data: 0.0025
Test:  [2770/3316]  eta: 4:32:31  loss: 1.2545 (1.1524)  acc1: 75.0000 (75.6699)  acc5: 93.7500 (91.3907)  time: 29.0791  data: 0.0026
Test:  [2770/3316]  eta: 4:32:31  loss: 1.2545 (1.1524)  acc1: 75.0000 (75.6699)  acc5: 93.7500 (91.3907)  time: 29.1357  data: 0.0027
Test:  [2780/3316]  eta: 4:26:44  loss: 1.2132 (1.1520)  acc1: 75.0000 (75.6877)  acc5: 93.7500 (91.3947)  time: 29.2476  data: 0.0025
Test:  [2790/3316]  eta: 4:21:09  loss: 0.9861 (1.1513)  acc1: 81.2500 (75.7143)  acc5: 93.7500 (91.4009)  time: 29.1747  data: 0.0026
Test:  [2850/3316]  eta: 3:46:35  loss: 1.1110 (1.1510)  acc1: 75.0000 (75.6927)  acc5: 87.5000 (91.4197)  time: 28.5769  data: 0.0026
Test:  [2810/3316]  eta: 4:09:44  loss: 1.1034 (1.1513)  acc1: 68.7500 (75.6915)  acc5: 93.7500 (91.4199)  time: 28.9472  data: 0.0025
Test:  [2780/3316]  eta: 4:27:25  loss: 1.2132 (1.1520)  acc1: 75.0000 (75.6877)  acc5: 93.7500 (91.3947)  time: 29.1496  data: 0.0026
Test:  [2850/3316]  eta: 3:46:47  loss: 1.1110 (1.1510)  acc1: 75.0000 (75.6927)  acc5: 87.5000 (91.4197)  time: 28.4431  data: 0.0026
Test:  [2780/3316]  eta: 4:27:29  loss: 1.2132 (1.1520)  acc1: 75.0000 (75.6877)  acc5: 93.7500 (91.3947)  time: 29.1223  data: 0.0027
Test:  [2780/3316]  eta: 4:27:30  loss: 1.2132 (1.1520)  acc1: 75.0000 (75.6877)  acc5: 93.7500 (91.3947)  time: 29.1623  data: 0.0025
Test:  [2790/3316]  eta: 4:21:45  loss: 0.9861 (1.1513)  acc1: 81.2500 (75.7143)  acc5: 93.7500 (91.4009)  time: 29.4109  data: 0.0025
Test:  [2800/3316]  eta: 4:16:10  loss: 0.9719 (1.1510)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4182)  time: 29.0178  data: 0.0026
Test:  [2860/3316]  eta: 3:41:43  loss: 1.2496 (1.1515)  acc1: 68.7500 (75.6794)  acc5: 93.7500 (91.4103)  time: 28.5468  data: 0.0025
Test:  [2820/3316]  eta: 4:04:47  loss: 1.0460 (1.1512)  acc1: 75.0000 (75.6890)  acc5: 93.7500 (91.4148)  time: 28.8835  data: 0.0025
Test:  [2860/3316]  eta: 3:41:54  loss: 1.2496 (1.1515)  acc1: 68.7500 (75.6794)  acc5: 93.7500 (91.4103)  time: 28.4823  data: 0.0025
Test:  [2790/3316]  eta: 4:22:24  loss: 0.9861 (1.1513)  acc1: 81.2500 (75.7143)  acc5: 93.7500 (91.4009)  time: 29.1207  data: 0.0026
Test:  [2790/3316]  eta: 4:22:29  loss: 0.9861 (1.1513)  acc1: 81.2500 (75.7143)  acc5: 93.7500 (91.4009)  time: 29.2570  data: 0.0026
Test:  [2790/3316]  eta: 4:22:30  loss: 0.9861 (1.1513)  acc1: 81.2500 (75.7143)  acc5: 93.7500 (91.4009)  time: 29.2789  data: 0.0025
Test:  [2800/3316]  eta: 4:16:45  loss: 0.9719 (1.1510)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4182)  time: 29.3390  data: 0.0024
Test:  [2810/3316]  eta: 4:11:11  loss: 1.1034 (1.1513)  acc1: 68.7500 (75.6915)  acc5: 93.7500 (91.4199)  time: 29.0320  data: 0.0027
Test:  [2870/3316]  eta: 3:36:50  loss: 1.2496 (1.1517)  acc1: 68.7500 (75.6727)  acc5: 93.7500 (91.4141)  time: 28.5913  data: 0.0026
Test:  [2870/3316]  eta: 3:37:01  loss: 1.2496 (1.1517)  acc1: 68.7500 (75.6727)  acc5: 93.7500 (91.4141)  time: 28.5021  data: 0.0025
Test:  [2830/3316]  eta: 3:59:50  loss: 1.0308 (1.1509)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4143)  time: 28.9711  data: 0.0027
Test:  [2800/3316]  eta: 4:17:23  loss: 0.9719 (1.1510)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4182)  time: 29.0700  data: 0.0025
Test:  [2800/3316]  eta: 4:17:28  loss: 0.9719 (1.1510)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4182)  time: 29.2214  data: 0.0026
Test:  [2800/3316]  eta: 4:17:29  loss: 0.9719 (1.1510)  acc1: 75.0000 (75.7096)  acc5: 93.7500 (91.4182)  time: 29.2258  data: 0.0026
Test:  [2810/3316]  eta: 4:11:45  loss: 1.1034 (1.1513)  acc1: 68.7500 (75.6915)  acc5: 93.7500 (91.4199)  time: 28.9798  data: 0.0026
Test:  [2820/3316]  eta: 4:06:11  loss: 1.0460 (1.1512)  acc1: 75.0000 (75.6890)  acc5: 93.7500 (91.4148)  time: 28.8941  data: 0.0026
Test:  [2880/3316]  eta: 3:31:57  loss: 1.1015 (1.1510)  acc1: 81.2500 (75.6942)  acc5: 93.7500 (91.4222)  time: 28.3882  data: 0.0025
Test:  [2880/3316]  eta: 3:32:07  loss: 1.1015 (1.1510)  acc1: 81.2500 (75.6942)  acc5: 93.7500 (91.4222)  time: 28.4291  data: 0.0025
Test:  [2840/3316]  eta: 3:54:53  loss: 1.0997 (1.1510)  acc1: 75.0000 (75.7018)  acc5: 87.5000 (91.4093)  time: 29.0265  data: 0.0027
Test:  [2810/3316]  eta: 4:12:22  loss: 1.1034 (1.1513)  acc1: 68.7500 (75.6915)  acc5: 93.7500 (91.4199)  time: 29.0163  data: 0.0025
Test:  [2810/3316]  eta: 4:12:27  loss: 1.1034 (1.1513)  acc1: 68.7500 (75.6915)  acc5: 93.7500 (91.4199)  time: 28.9801  data: 0.0025
Test:  [2810/3316]  eta: 4:12:27  loss: 1.1034 (1.1513)  acc1: 68.7500 (75.6915)  acc5: 93.7500 (91.4199)  time: 29.0151  data: 0.0030
Test:  [2820/3316]  eta: 4:06:45  loss: 1.0460 (1.1512)  acc1: 75.0000 (75.6890)  acc5: 93.7500 (91.4148)  time: 28.8613  data: 0.0026
Test:  [2830/3316]  eta: 4:01:11  loss: 1.0308 (1.1509)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4143)  time: 28.7599  data: 0.0024
Test:  [2890/3316]  eta: 3:27:04  loss: 0.9617 (1.1510)  acc1: 81.2500 (75.6983)  acc5: 93.7500 (91.4217)  time: 28.2400  data: 0.0024
Test:  [2890/3316]  eta: 3:27:14  loss: 0.9617 (1.1510)  acc1: 81.2500 (75.6983)  acc5: 93.7500 (91.4217)  time: 28.1990  data: 0.0024
Test:  [2850/3316]  eta: 3:49:55  loss: 1.1110 (1.1510)  acc1: 75.0000 (75.6927)  acc5: 87.5000 (91.4197)  time: 28.7030  data: 0.0027
Test:  [2820/3316]  eta: 4:07:20  loss: 1.0460 (1.1512)  acc1: 75.0000 (75.6890)  acc5: 93.7500 (91.4148)  time: 28.7876  data: 0.0025
Test:  [2820/3316]  eta: 4:07:25  loss: 1.0460 (1.1512)  acc1: 75.0000 (75.6890)  acc5: 93.7500 (91.4148)  time: 28.7368  data: 0.0024
Test:  [2820/3316]  eta: 4:07:26  loss: 1.0460 (1.1512)  acc1: 75.0000 (75.6890)  acc5: 93.7500 (91.4148)  time: 28.8216  data: 0.0029
Test:  [2830/3316]  eta: 4:01:44  loss: 1.0308 (1.1509)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4143)  time: 28.7877  data: 0.0025
Test:  [2840/3316]  eta: 3:56:13  loss: 1.0997 (1.1510)  acc1: 75.0000 (75.7018)  acc5: 87.5000 (91.4093)  time: 29.0582  data: 0.0029
Test:  [2900/3316]  eta: 3:22:12  loss: 0.9717 (1.1509)  acc1: 75.0000 (75.7023)  acc5: 93.7500 (91.4275)  time: 28.5620  data: 0.0030
Test:  [2900/3316]  eta: 3:22:21  loss: 0.9717 (1.1509)  acc1: 75.0000 (75.7023)  acc5: 93.7500 (91.4275)  time: 28.4097  data: 0.0040
Test:  [2860/3316]  eta: 3:44:58  loss: 1.2496 (1.1515)  acc1: 68.7500 (75.6794)  acc5: 93.7500 (91.4103)  time: 28.8154  data: 0.0034
Test:  [2830/3316]  eta: 4:02:20  loss: 1.0308 (1.1509)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4143)  time: 28.9652  data: 0.0040
Test:  [2830/3316]  eta: 4:02:24  loss: 1.0308 (1.1509)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4143)  time: 28.9001  data: 0.0039
Test:  [2830/3316]  eta: 4:02:26  loss: 1.0308 (1.1509)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4143)  time: 29.0166  data: 0.0044
Test:  [2840/3316]  eta: 3:56:45  loss: 1.0997 (1.1510)  acc1: 75.0000 (75.7018)  acc5: 87.5000 (91.4093)  time: 29.0535  data: 0.0044
Test:  [2850/3316]  eta: 3:51:13  loss: 1.1110 (1.1510)  acc1: 75.0000 (75.6927)  acc5: 87.5000 (91.4197)  time: 28.9520  data: 0.0041
Test:  [2910/3316]  eta: 3:17:18  loss: 0.9761 (1.1513)  acc1: 75.0000 (75.6956)  acc5: 93.7500 (91.4248)  time: 28.4812  data: 0.0042
Test:  [2910/3316]  eta: 3:17:28  loss: 0.9761 (1.1513)  acc1: 75.0000 (75.6956)  acc5: 93.7500 (91.4248)  time: 28.4351  data: 0.0047
Test:  [2870/3316]  eta: 3:40:00  loss: 1.2496 (1.1517)  acc1: 68.7500 (75.6727)  acc5: 93.7500 (91.4141)  time: 28.8042  data: 0.0038
Test:  [2840/3316]  eta: 3:57:19  loss: 1.0997 (1.1510)  acc1: 75.0000 (75.7018)  acc5: 87.5000 (91.4093)  time: 28.9931  data: 0.0045
Test:  [2840/3316]  eta: 3:57:23  loss: 1.0997 (1.1510)  acc1: 75.0000 (75.7018)  acc5: 87.5000 (91.4093)  time: 28.9018  data: 0.0042
Test:  [2840/3316]  eta: 3:57:24  loss: 1.0997 (1.1510)  acc1: 75.0000 (75.7018)  acc5: 87.5000 (91.4093)  time: 29.0188  data: 0.0049
Test:  [2850/3316]  eta: 3:51:45  loss: 1.1110 (1.1510)  acc1: 75.0000 (75.6927)  acc5: 87.5000 (91.4197)  time: 29.1443  data: 0.0046
Test:  [2860/3316]  eta: 3:46:13  loss: 1.2496 (1.1515)  acc1: 68.7500 (75.6794)  acc5: 93.7500 (91.4103)  time: 28.5841  data: 0.0037
Test:  [2920/3316]  eta: 3:12:25  loss: 1.0169 (1.1507)  acc1: 81.2500 (75.7082)  acc5: 93.7500 (91.4327)  time: 28.0645  data: 0.0036
Test:  [2920/3316]  eta: 3:12:35  loss: 1.0169 (1.1507)  acc1: 81.2500 (75.7082)  acc5: 93.7500 (91.4327)  time: 28.0638  data: 0.0030
Test:  [2880/3316]  eta: 3:35:02  loss: 1.1015 (1.1510)  acc1: 81.2500 (75.6942)  acc5: 93.7500 (91.4222)  time: 28.4357  data: 0.0029
Test:  [2850/3316]  eta: 3:52:17  loss: 1.1110 (1.1510)  acc1: 75.0000 (75.6927)  acc5: 87.5000 (91.4197)  time: 28.6077  data: 0.0029
Test:  [2850/3316]  eta: 3:52:21  loss: 1.1110 (1.1510)  acc1: 75.0000 (75.6927)  acc5: 87.5000 (91.4197)  time: 28.4959  data: 0.0027
Test:  [2850/3316]  eta: 3:52:23  loss: 1.1110 (1.1510)  acc1: 75.0000 (75.6927)  acc5: 87.5000 (91.4197)  time: 28.6533  data: 0.0028
Test:  [2860/3316]  eta: 3:46:45  loss: 1.2496 (1.1515)  acc1: 68.7500 (75.6794)  acc5: 93.7500 (91.4103)  time: 28.7415  data: 0.0026
Test:  [2930/3316]  eta: 3:07:32  loss: 1.1225 (1.1508)  acc1: 81.2500 (75.7101)  acc5: 93.7500 (91.4342)  time: 27.9791  data: 0.0023
Test:  [2870/3316]  eta: 3:41:14  loss: 1.2496 (1.1517)  acc1: 68.7500 (75.6727)  acc5: 93.7500 (91.4141)  time: 28.5834  data: 0.0024
Test:  [2930/3316]  eta: 3:07:41  loss: 1.1225 (1.1508)  acc1: 81.2500 (75.7101)  acc5: 93.7500 (91.4342)  time: 28.0187  data: 0.0023
Test:  [2890/3316]  eta: 3:30:05  loss: 0.9617 (1.1510)  acc1: 81.2500 (75.6983)  acc5: 93.7500 (91.4217)  time: 28.4570  data: 0.0023
Test:  [2860/3316]  eta: 3:47:16  loss: 1.2496 (1.1515)  acc1: 68.7500 (75.6794)  acc5: 93.7500 (91.4103)  time: 28.5776  data: 0.0023
Test:  [2860/3316]  eta: 3:47:20  loss: 1.2496 (1.1515)  acc1: 68.7500 (75.6794)  acc5: 93.7500 (91.4103)  time: 28.4591  data: 0.0022
Test:  [2860/3316]  eta: 3:47:22  loss: 1.2496 (1.1515)  acc1: 68.7500 (75.6794)  acc5: 93.7500 (91.4103)  time: 28.6619  data: 0.0023
Test:  [2870/3316]  eta: 3:41:44  loss: 1.2496 (1.1517)  acc1: 68.7500 (75.6727)  acc5: 93.7500 (91.4141)  time: 28.6590  data: 0.0024
Test:  [2940/3316]  eta: 3:02:39  loss: 1.1651 (1.1513)  acc1: 75.0000 (75.7055)  acc5: 87.5000 (91.4230)  time: 27.9858  data: 0.0022
Test:  [2880/3316]  eta: 3:36:14  loss: 1.1015 (1.1510)  acc1: 81.2500 (75.6942)  acc5: 93.7500 (91.4222)  time: 28.5674  data: 0.0023
Test:  [2940/3316]  eta: 3:02:48  loss: 1.1651 (1.1513)  acc1: 75.0000 (75.7055)  acc5: 87.5000 (91.4230)  time: 28.0752  data: 0.0023
Test:  [2900/3316]  eta: 3:25:08  loss: 0.9717 (1.1509)  acc1: 75.0000 (75.7023)  acc5: 93.7500 (91.4275)  time: 28.5949  data: 0.0024
Test:  [2870/3316]  eta: 3:42:15  loss: 1.2496 (1.1517)  acc1: 68.7500 (75.6727)  acc5: 93.7500 (91.4141)  time: 28.6726  data: 0.0023
Test:  [2870/3316]  eta: 3:42:19  loss: 1.2496 (1.1517)  acc1: 68.7500 (75.6727)  acc5: 93.7500 (91.4141)  time: 28.5493  data: 0.0023
Test:  [2870/3316]  eta: 3:42:21  loss: 1.2496 (1.1517)  acc1: 68.7500 (75.6727)  acc5: 93.7500 (91.4141)  time: 28.7261  data: 0.0023
Test:  [2880/3316]  eta: 3:36:44  loss: 1.1015 (1.1510)  acc1: 81.2500 (75.6942)  acc5: 93.7500 (91.4222)  time: 28.7022  data: 0.0023
Test:  [2950/3316]  eta: 2:57:47  loss: 1.2250 (1.1517)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4139)  time: 28.1476  data: 0.0024
Test:  [2890/3316]  eta: 3:31:16  loss: 0.9617 (1.1510)  acc1: 81.2500 (75.6983)  acc5: 93.7500 (91.4217)  time: 28.8488  data: 0.0022
Test:  [2950/3316]  eta: 2:57:56  loss: 1.2250 (1.1517)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4139)  time: 28.3303  data: 0.0027
Test:  [2910/3316]  eta: 3:20:11  loss: 0.9761 (1.1513)  acc1: 75.0000 (75.6956)  acc5: 93.7500 (91.4248)  time: 28.8416  data: 0.0033
Test:  [2880/3316]  eta: 3:37:15  loss: 1.1015 (1.1510)  acc1: 81.2500 (75.6942)  acc5: 93.7500 (91.4222)  time: 28.9213  data: 0.0033
Test:  [2880/3316]  eta: 3:37:18  loss: 1.1015 (1.1510)  acc1: 81.2500 (75.6942)  acc5: 93.7500 (91.4222)  time: 28.7583  data: 0.0031
Test:  [2880/3316]  eta: 3:37:20  loss: 1.1015 (1.1510)  acc1: 81.2500 (75.6942)  acc5: 93.7500 (91.4222)  time: 28.9825  data: 0.0032
Test:  [2890/3316]  eta: 3:31:45  loss: 0.9617 (1.1510)  acc1: 81.2500 (75.6983)  acc5: 93.7500 (91.4217)  time: 28.9831  data: 0.0039
Test:  [2960/3316]  eta: 2:52:54  loss: 1.1242 (1.1516)  acc1: 75.0000 (75.7092)  acc5: 93.7500 (91.4197)  time: 28.3871  data: 0.0038
Test:  [2900/3316]  eta: 3:26:17  loss: 0.9717 (1.1509)  acc1: 75.0000 (75.7023)  acc5: 93.7500 (91.4275)  time: 29.0074  data: 0.0039
Test:  [2960/3316]  eta: 2:53:03  loss: 1.1242 (1.1516)  acc1: 75.0000 (75.7092)  acc5: 93.7500 (91.4197)  time: 28.2947  data: 0.0036
Test:  [2920/3316]  eta: 3:15:14  loss: 1.0169 (1.1507)  acc1: 81.2500 (75.7082)  acc5: 93.7500 (91.4327)  time: 28.6987  data: 0.0041
Test:  [2890/3316]  eta: 3:32:14  loss: 0.9617 (1.1510)  acc1: 81.2500 (75.6983)  acc5: 93.7500 (91.4217)  time: 28.8708  data: 0.0040
Test:  [2890/3316]  eta: 3:32:17  loss: 0.9617 (1.1510)  acc1: 81.2500 (75.6983)  acc5: 93.7500 (91.4217)  time: 28.6758  data: 0.0037
Test:  [2890/3316]  eta: 3:32:20  loss: 0.9617 (1.1510)  acc1: 81.2500 (75.6983)  acc5: 93.7500 (91.4217)  time: 28.9532  data: 0.0036
Test:  [2900/3316]  eta: 3:26:45  loss: 0.9717 (1.1509)  acc1: 75.0000 (75.7023)  acc5: 93.7500 (91.4275)  time: 28.9696  data: 0.0043
Test:  [2970/3316]  eta: 2:48:02  loss: 1.0184 (1.1515)  acc1: 75.0000 (75.7131)  acc5: 93.7500 (91.4191)  time: 28.2889  data: 0.0038
Test:  [2910/3316]  eta: 3:21:18  loss: 0.9761 (1.1513)  acc1: 75.0000 (75.6956)  acc5: 93.7500 (91.4248)  time: 28.6534  data: 0.0040
Test:  [2970/3316]  eta: 2:48:10  loss: 1.0184 (1.1515)  acc1: 75.0000 (75.7131)  acc5: 93.7500 (91.4191)  time: 28.0392  data: 0.0033
Test:  [2930/3316]  eta: 3:10:16  loss: 1.1225 (1.1508)  acc1: 81.2500 (75.7101)  acc5: 93.7500 (91.4342)  time: 28.4154  data: 0.0033
Test:  [2900/3316]  eta: 3:27:13  loss: 0.9717 (1.1509)  acc1: 75.0000 (75.7023)  acc5: 93.7500 (91.4275)  time: 28.6546  data: 0.0032
Test:  [2900/3316]  eta: 3:27:16  loss: 0.9717 (1.1509)  acc1: 75.0000 (75.7023)  acc5: 93.7500 (91.4275)  time: 28.4361  data: 0.0030
Test:  [2900/3316]  eta: 3:27:19  loss: 0.9717 (1.1509)  acc1: 75.0000 (75.7023)  acc5: 93.7500 (91.4275)  time: 28.7471  data: 0.0028
Test:  [2910/3316]  eta: 3:21:46  loss: 0.9761 (1.1513)  acc1: 75.0000 (75.6956)  acc5: 93.7500 (91.4248)  time: 28.7217  data: 0.0027
Test:  [2980/3316]  eta: 2:43:09  loss: 1.0724 (1.1518)  acc1: 75.0000 (75.7066)  acc5: 93.7500 (91.4186)  time: 28.0858  data: 0.0024
Test:  [2920/3316]  eta: 3:16:19  loss: 1.0169 (1.1507)  acc1: 81.2500 (75.7082)  acc5: 93.7500 (91.4327)  time: 28.5624  data: 0.0023
Test:  [2980/3316]  eta: 2:43:17  loss: 1.0724 (1.1518)  acc1: 75.0000 (75.7066)  acc5: 93.7500 (91.4186)  time: 28.0264  data: 0.0023
Test:  [2940/3316]  eta: 3:05:19  loss: 1.1651 (1.1513)  acc1: 75.0000 (75.7055)  acc5: 87.5000 (91.4230)  time: 28.3718  data: 0.0024
Test:  [2910/3316]  eta: 3:22:13  loss: 0.9761 (1.1513)  acc1: 75.0000 (75.6956)  acc5: 93.7500 (91.4248)  time: 28.7975  data: 0.0024
Test:  [2910/3316]  eta: 3:22:15  loss: 0.9761 (1.1513)  acc1: 75.0000 (75.6956)  acc5: 93.7500 (91.4248)  time: 28.5482  data: 0.0026
Test:  [2910/3316]  eta: 3:22:19  loss: 0.9761 (1.1513)  acc1: 75.0000 (75.6956)  acc5: 93.7500 (91.4248)  time: 28.8584  data: 0.0024
Test:  [2920/3316]  eta: 3:16:46  loss: 1.0169 (1.1507)  acc1: 81.2500 (75.7082)  acc5: 93.7500 (91.4327)  time: 28.8091  data: 0.0028
Test:  [2990/3316]  eta: 2:38:17  loss: 1.0750 (1.1517)  acc1: 75.0000 (75.7042)  acc5: 93.7500 (91.4138)  time: 28.2124  data: 0.0024
Test:  [2930/3316]  eta: 3:11:20  loss: 1.1225 (1.1508)  acc1: 81.2500 (75.7101)  acc5: 93.7500 (91.4342)  time: 28.7346  data: 0.0026
Test:  [2990/3316]  eta: 2:38:24  loss: 1.0750 (1.1517)  acc1: 75.0000 (75.7042)  acc5: 93.7500 (91.4138)  time: 28.1987  data: 0.0031
Test:  [2950/3316]  eta: 3:00:22  loss: 1.2250 (1.1517)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4139)  time: 28.5226  data: 0.0030
Test:  [2920/3316]  eta: 3:17:13  loss: 1.0169 (1.1507)  acc1: 81.2500 (75.7082)  acc5: 93.7500 (91.4327)  time: 28.7886  data: 0.0030
Test:  [2920/3316]  eta: 3:17:14  loss: 1.0169 (1.1507)  acc1: 81.2500 (75.7082)  acc5: 93.7500 (91.4327)  time: 28.5521  data: 0.0027
Test:  [2920/3316]  eta: 3:17:18  loss: 1.0169 (1.1507)  acc1: 81.2500 (75.7082)  acc5: 93.7500 (91.4327)  time: 28.8857  data: 0.0027
Test:  [2930/3316]  eta: 3:11:46  loss: 1.1225 (1.1508)  acc1: 81.2500 (75.7101)  acc5: 93.7500 (91.4342)  time: 28.7549  data: 0.0031
Test:  [3000/3316]  eta: 2:33:24  loss: 0.9026 (1.1508)  acc1: 81.2500 (75.7227)  acc5: 93.7500 (91.4195)  time: 28.1958  data: 0.0027
Test:  [2940/3316]  eta: 3:06:21  loss: 1.1651 (1.1513)  acc1: 75.0000 (75.7055)  acc5: 87.5000 (91.4230)  time: 28.7655  data: 0.0030
Test:  [3000/3316]  eta: 2:33:32  loss: 0.9026 (1.1508)  acc1: 81.2500 (75.7227)  acc5: 93.7500 (91.4195)  time: 28.2597  data: 0.0032
Test:  [2960/3316]  eta: 2:55:25  loss: 1.1242 (1.1516)  acc1: 75.0000 (75.7092)  acc5: 93.7500 (91.4197)  time: 28.5866  data: 0.0030
Test:  [2930/3316]  eta: 3:12:13  loss: 1.1225 (1.1508)  acc1: 81.2500 (75.7101)  acc5: 93.7500 (91.4342)  time: 28.8025  data: 0.0030
Test:  [2930/3316]  eta: 3:12:14  loss: 1.1225 (1.1508)  acc1: 81.2500 (75.7101)  acc5: 93.7500 (91.4342)  time: 28.5461  data: 0.0026
Test:  [2930/3316]  eta: 3:12:18  loss: 1.1225 (1.1508)  acc1: 81.2500 (75.7101)  acc5: 93.7500 (91.4342)  time: 28.9701  data: 0.0028
Test:  [2940/3316]  eta: 3:06:47  loss: 1.1651 (1.1513)  acc1: 75.0000 (75.7055)  acc5: 87.5000 (91.4230)  time: 28.8124  data: 0.0029
Test:  [3010/3316]  eta: 2:28:33  loss: 0.7666 (1.1506)  acc1: 81.2500 (75.7307)  acc5: 93.7500 (91.4127)  time: 28.3040  data: 0.0027
Test:  [2950/3316]  eta: 3:01:24  loss: 1.2250 (1.1517)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4139)  time: 29.1601  data: 0.0034
Test:  [3010/3316]  eta: 2:28:40  loss: 0.7666 (1.1506)  acc1: 81.2500 (75.7307)  acc5: 93.7500 (91.4127)  time: 28.6339  data: 0.0038
Test:  [2970/3316]  eta: 2:50:29  loss: 1.0184 (1.1515)  acc1: 75.0000 (75.7131)  acc5: 93.7500 (91.4191)  time: 28.9781  data: 0.0038
Test:  [2940/3316]  eta: 3:07:14  loss: 1.1651 (1.1513)  acc1: 75.0000 (75.7055)  acc5: 87.5000 (91.4230)  time: 29.4298  data: 0.0040
Test:  [2940/3316]  eta: 3:07:15  loss: 1.1651 (1.1513)  acc1: 75.0000 (75.7055)  acc5: 87.5000 (91.4230)  time: 29.1596  data: 0.0034
Test:  [2940/3316]  eta: 3:07:19  loss: 1.1651 (1.1513)  acc1: 75.0000 (75.7055)  acc5: 87.5000 (91.4230)  time: 29.5183  data: 0.0037
Test:  [2950/3316]  eta: 3:01:49  loss: 1.2250 (1.1517)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4139)  time: 29.3858  data: 0.0046
Test:  [3020/3316]  eta: 2:23:41  loss: 1.1724 (1.1512)  acc1: 75.0000 (75.7158)  acc5: 87.5000 (91.4081)  time: 28.8962  data: 0.0038
Test:  [2960/3316]  eta: 2:56:26  loss: 1.1242 (1.1516)  acc1: 75.0000 (75.7092)  acc5: 93.7500 (91.4197)  time: 29.4489  data: 0.0046
Test:  [3020/3316]  eta: 2:23:48  loss: 1.1724 (1.1512)  acc1: 75.0000 (75.7158)  acc5: 87.5000 (91.4081)  time: 28.9314  data: 0.0053
Test:  [2980/3316]  eta: 2:45:33  loss: 1.0724 (1.1518)  acc1: 75.0000 (75.7066)  acc5: 93.7500 (91.4186)  time: 29.2478  data: 0.0049
Test:  [2950/3316]  eta: 3:02:15  loss: 1.2250 (1.1517)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4139)  time: 29.6828  data: 0.0051
Test:  [2950/3316]  eta: 3:02:15  loss: 1.2250 (1.1517)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4139)  time: 29.4128  data: 0.0040
Test:  [2950/3316]  eta: 3:02:20  loss: 1.2250 (1.1517)  acc1: 75.0000 (75.7032)  acc5: 87.5000 (91.4139)  time: 29.7292  data: 0.0044
Test:  [2960/3316]  eta: 2:56:50  loss: 1.1242 (1.1516)  acc1: 75.0000 (75.7092)  acc5: 93.7500 (91.4197)  time: 29.5083  data: 0.0048
Test:  [3030/3316]  eta: 2:18:50  loss: 1.2530 (1.1517)  acc1: 68.7500 (75.7011)  acc5: 93.7500 (91.4034)  time: 29.0292  data: 0.0042
Test:  [2970/3316]  eta: 2:51:28  loss: 1.0184 (1.1515)  acc1: 75.0000 (75.7131)  acc5: 93.7500 (91.4191)  time: 29.3303  data: 0.0043
Test:  [3030/3316]  eta: 2:18:56  loss: 1.2530 (1.1517)  acc1: 68.7500 (75.7011)  acc5: 93.7500 (91.4034)  time: 28.7691  data: 0.0045
Test:  [2990/3316]  eta: 2:40:37  loss: 1.0750 (1.1517)  acc1: 75.0000 (75.7042)  acc5: 93.7500 (91.4138)  time: 29.1500  data: 0.0037
Test:  [2960/3316]  eta: 2:57:16  loss: 1.1242 (1.1516)  acc1: 75.0000 (75.7092)  acc5: 93.7500 (91.4197)  time: 29.6517  data: 0.0031
Test:  [2960/3316]  eta: 2:57:16  loss: 1.1242 (1.1516)  acc1: 75.0000 (75.7092)  acc5: 93.7500 (91.4197)  time: 29.9339  data: 0.0037
Test:  [2960/3316]  eta: 2:57:21  loss: 1.1242 (1.1516)  acc1: 75.0000 (75.7092)  acc5: 93.7500 (91.4197)  time: 29.8755  data: 0.0033
Test:  [2970/3316]  eta: 2:51:54  loss: 1.0184 (1.1515)  acc1: 75.0000 (75.7131)  acc5: 93.7500 (91.4191)  time: 30.1966  data: 0.0030
Test:  [3040/3316]  eta: 2:14:01  loss: 1.1165 (1.1513)  acc1: 68.7500 (75.7070)  acc5: 93.7500 (91.4132)  time: 30.2718  data: 0.0031
Test:  [2980/3316]  eta: 2:46:38  loss: 1.0724 (1.1518)  acc1: 75.0000 (75.7066)  acc5: 93.7500 (91.4186)  time: 32.5511  data: 0.0051
Test:  [3040/3316]  eta: 2:14:10  loss: 1.1165 (1.1513)  acc1: 68.7500 (75.7070)  acc5: 93.7500 (91.4132)  time: 32.0056  data: 0.0041
Test:  [3000/3316]  eta: 2:35:48  loss: 0.9026 (1.1508)  acc1: 81.2500 (75.7227)  acc5: 93.7500 (91.4195)  time: 32.7018  data: 0.0061
Test:  [2970/3316]  eta: 2:52:24  loss: 1.0184 (1.1515)  acc1: 75.0000 (75.7131)  acc5: 93.7500 (91.4191)  time: 32.7086  data: 0.0036
Test:  [2970/3316]  eta: 2:52:24  loss: 1.0184 (1.1515)  acc1: 75.0000 (75.7131)  acc5: 93.7500 (91.4191)  time: 32.9827  data: 0.0039
Test:  [2970/3316]  eta: 2:52:29  loss: 1.0184 (1.1515)  acc1: 75.0000 (75.7131)  acc5: 93.7500 (91.4191)  time: 32.9112  data: 0.0056
Test:  [2980/3316]  eta: 2:47:01  loss: 1.0724 (1.1518)  acc1: 75.0000 (75.7066)  acc5: 93.7500 (91.4186)  time: 32.8928  data: 0.0040
Test:  [3050/3316]  eta: 2:09:13  loss: 1.0878 (1.1513)  acc1: 75.0000 (75.7088)  acc5: 93.7500 (91.4147)  time: 32.2031  data: 0.0031
Test:  [3050/3316]  eta: 2:09:19  loss: 1.0878 (1.1513)  acc1: 75.0000 (75.7088)  acc5: 93.7500 (91.4147)  time: 32.5083  data: 0.0038
Test:  [2990/3316]  eta: 2:41:41  loss: 1.0750 (1.1517)  acc1: 75.0000 (75.7042)  acc5: 93.7500 (91.4138)  time: 33.0479  data: 0.0052
Test:  [3010/3316]  eta: 2:30:53  loss: 0.7666 (1.1506)  acc1: 81.2500 (75.7307)  acc5: 93.7500 (91.4127)  time: 32.9921  data: 0.0066
Test:  [2980/3316]  eta: 2:47:25  loss: 1.0724 (1.1518)  acc1: 75.0000 (75.7066)  acc5: 93.7500 (91.4186)  time: 32.5410  data: 0.0043
Test:  [2980/3316]  eta: 2:47:25  loss: 1.0724 (1.1518)  acc1: 75.0000 (75.7066)  acc5: 93.7500 (91.4186)  time: 32.7723  data: 0.0047
Test:  [2980/3316]  eta: 2:47:30  loss: 1.0724 (1.1518)  acc1: 75.0000 (75.7066)  acc5: 93.7500 (91.4186)  time: 32.9397  data: 0.0065
Test:  [2990/3316]  eta: 2:42:03  loss: 1.0750 (1.1517)  acc1: 75.0000 (75.7042)  acc5: 93.7500 (91.4138)  time: 32.2214  data: 0.0050
Test:  [3060/3316]  eta: 2:04:21  loss: 1.0878 (1.1511)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4121)  time: 30.9594  data: 0.0040
Test:  [3060/3316]  eta: 2:04:27  loss: 1.0878 (1.1511)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4121)  time: 29.2412  data: 0.0038
Test:  [3000/3316]  eta: 2:36:43  loss: 0.9026 (1.1508)  acc1: 81.2500 (75.7227)  acc5: 93.7500 (91.4195)  time: 29.9214  data: 0.0042
Test:  [3020/3316]  eta: 2:25:57  loss: 1.1724 (1.1512)  acc1: 75.0000 (75.7158)  acc5: 87.5000 (91.4081)  time: 29.6277  data: 0.0042
Test:  [2990/3316]  eta: 2:42:25  loss: 1.0750 (1.1517)  acc1: 75.0000 (75.7042)  acc5: 93.7500 (91.4138)  time: 29.5184  data: 0.0038
Test:  [2990/3316]  eta: 2:42:26  loss: 1.0750 (1.1517)  acc1: 75.0000 (75.7042)  acc5: 93.7500 (91.4138)  time: 29.7273  data: 0.0039
Test:  [2990/3316]  eta: 2:42:31  loss: 1.0750 (1.1517)  acc1: 75.0000 (75.7042)  acc5: 93.7500 (91.4138)  time: 29.9860  data: 0.0041
Test:  [3000/3316]  eta: 2:37:04  loss: 0.9026 (1.1508)  acc1: 81.2500 (75.7227)  acc5: 93.7500 (91.4195)  time: 29.6660  data: 0.0039
Test:  [3070/3316]  eta: 1:59:29  loss: 1.0831 (1.1510)  acc1: 75.0000 (75.7123)  acc5: 93.7500 (91.4075)  time: 28.9767  data: 0.0040
Test:  [3070/3316]  eta: 1:59:35  loss: 1.0831 (1.1510)  acc1: 75.0000 (75.7123)  acc5: 93.7500 (91.4075)  time: 28.7186  data: 0.0039
Test:  [3010/3316]  eta: 2:31:45  loss: 0.7666 (1.1506)  acc1: 81.2500 (75.7307)  acc5: 93.7500 (91.4127)  time: 29.5291  data: 0.0041
Test:  [3030/3316]  eta: 2:21:00  loss: 1.2530 (1.1517)  acc1: 68.7500 (75.7011)  acc5: 93.7500 (91.4034)  time: 29.3229  data: 0.0037
Test:  [3000/3316]  eta: 2:37:26  loss: 0.9026 (1.1508)  acc1: 81.2500 (75.7227)  acc5: 93.7500 (91.4195)  time: 29.2464  data: 0.0032
Test:  [3000/3316]  eta: 2:37:27  loss: 0.9026 (1.1508)  acc1: 81.2500 (75.7227)  acc5: 93.7500 (91.4195)  time: 29.5321  data: 0.0032
Test:  [3000/3316]  eta: 2:37:32  loss: 0.9026 (1.1508)  acc1: 81.2500 (75.7227)  acc5: 93.7500 (91.4195)  time: 29.7413  data: 0.0031
Test:  [3080/3316]  eta: 1:54:38  loss: 1.0844 (1.1510)  acc1: 75.0000 (75.7100)  acc5: 93.7500 (91.4192)  time: 28.7369  data: 0.0029
Test:  [3010/3316]  eta: 2:32:06  loss: 0.7666 (1.1506)  acc1: 81.2500 (75.7307)  acc5: 93.7500 (91.4127)  time: 29.4214  data: 0.0028
Test:  [3080/3316]  eta: 1:54:43  loss: 1.0844 (1.1510)  acc1: 75.0000 (75.7100)  acc5: 93.7500 (91.4192)  time: 28.7338  data: 0.0031
Test:  [3020/3316]  eta: 2:26:47  loss: 1.1724 (1.1512)  acc1: 75.0000 (75.7158)  acc5: 87.5000 (91.4081)  time: 29.5376  data: 0.0029
Test:  [3040/3316]  eta: 2:16:04  loss: 1.1165 (1.1513)  acc1: 68.7500 (75.7070)  acc5: 93.7500 (91.4132)  time: 29.2280  data: 0.0029
Test:  [3010/3316]  eta: 2:32:26  loss: 0.7666 (1.1506)  acc1: 81.2500 (75.7307)  acc5: 93.7500 (91.4127)  time: 29.2370  data: 0.0028
Test:  [3010/3316]  eta: 2:32:27  loss: 0.7666 (1.1506)  acc1: 81.2500 (75.7307)  acc5: 93.7500 (91.4127)  time: 29.5152  data: 0.0030
Test:  [3010/3316]  eta: 2:32:32  loss: 0.7666 (1.1506)  acc1: 81.2500 (75.7307)  acc5: 93.7500 (91.4127)  time: 29.7553  data: 0.0028
Test:  [3090/3316]  eta: 1:49:46  loss: 1.0698 (1.1513)  acc1: 75.0000 (75.7037)  acc5: 93.7500 (91.4186)  time: 28.7329  data: 0.0029
Test:  [3020/3316]  eta: 2:27:07  loss: 1.1724 (1.1512)  acc1: 75.0000 (75.7158)  acc5: 87.5000 (91.4081)  time: 29.2563  data: 0.0026
Test:  [3090/3316]  eta: 1:49:51  loss: 1.0698 (1.1513)  acc1: 75.0000 (75.7037)  acc5: 93.7500 (91.4186)  time: 28.7676  data: 0.0029
Test:  [3030/3316]  eta: 2:21:49  loss: 1.2530 (1.1517)  acc1: 68.7500 (75.7011)  acc5: 93.7500 (91.4034)  time: 29.4890  data: 0.0028
Test:  [3050/3316]  eta: 2:11:08  loss: 1.0878 (1.1513)  acc1: 75.0000 (75.7088)  acc5: 93.7500 (91.4147)  time: 29.2365  data: 0.0028
Test:  [3020/3316]  eta: 2:27:27  loss: 1.1724 (1.1512)  acc1: 75.0000 (75.7158)  acc5: 87.5000 (91.4081)  time: 29.3112  data: 0.0028
Test:  [3020/3316]  eta: 2:27:28  loss: 1.1724 (1.1512)  acc1: 75.0000 (75.7158)  acc5: 87.5000 (91.4081)  time: 29.5762  data: 0.0031
Test:  [3020/3316]  eta: 2:27:33  loss: 1.1724 (1.1512)  acc1: 75.0000 (75.7158)  acc5: 87.5000 (91.4081)  time: 29.8150  data: 0.0029
Test:  [3100/3316]  eta: 1:44:54  loss: 1.3191 (1.1522)  acc1: 68.7500 (75.6792)  acc5: 87.5000 (91.4020)  time: 28.7977  data: 0.0032
Test:  [3030/3316]  eta: 2:22:08  loss: 1.2530 (1.1517)  acc1: 68.7500 (75.7011)  acc5: 93.7500 (91.4034)  time: 29.4356  data: 0.0027
Test:  [3100/3316]  eta: 1:44:59  loss: 1.3191 (1.1522)  acc1: 68.7500 (75.6792)  acc5: 87.5000 (91.4020)  time: 28.9573  data: 0.0029
Test:  [3040/3316]  eta: 2:16:52  loss: 1.1165 (1.1513)  acc1: 68.7500 (75.7070)  acc5: 93.7500 (91.4132)  time: 29.7992  data: 0.0029
Test:  [3060/3316]  eta: 2:06:13  loss: 1.0878 (1.1511)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4121)  time: 29.9451  data: 0.0033
Test:  [3030/3316]  eta: 2:22:28  loss: 1.2530 (1.1517)  acc1: 68.7500 (75.7011)  acc5: 93.7500 (91.4034)  time: 30.0437  data: 0.0035
Test:  [3030/3316]  eta: 2:22:30  loss: 1.2530 (1.1517)  acc1: 68.7500 (75.7011)  acc5: 93.7500 (91.4034)  time: 30.3871  data: 0.0035
Test:  [3030/3316]  eta: 2:22:35  loss: 1.2530 (1.1517)  acc1: 68.7500 (75.7011)  acc5: 93.7500 (91.4034)  time: 30.5598  data: 0.0035
Test:  [3110/3316]  eta: 1:40:04  loss: 1.1048 (1.1514)  acc1: 75.0000 (75.7072)  acc5: 93.7500 (91.4135)  time: 29.6564  data: 0.0035
Test:  [3040/3316]  eta: 2:17:12  loss: 1.1165 (1.1513)  acc1: 68.7500 (75.7070)  acc5: 93.7500 (91.4132)  time: 30.5629  data: 0.0033
Test:  [3110/3316]  eta: 1:40:09  loss: 1.1048 (1.1514)  acc1: 75.0000 (75.7072)  acc5: 93.7500 (91.4135)  time: 30.1478  data: 0.0037
Test:  [3050/3316]  eta: 2:11:56  loss: 1.0878 (1.1513)  acc1: 75.0000 (75.7088)  acc5: 93.7500 (91.4147)  time: 30.8603  data: 0.0038
Test:  [3070/3316]  eta: 2:01:19  loss: 1.0831 (1.1510)  acc1: 75.0000 (75.7123)  acc5: 93.7500 (91.4075)  time: 30.8418  data: 0.0039
Test:  [3040/3316]  eta: 2:17:31  loss: 1.1165 (1.1513)  acc1: 68.7500 (75.7070)  acc5: 93.7500 (91.4132)  time: 30.8590  data: 0.0039
Test:  [3040/3316]  eta: 2:17:33  loss: 1.1165 (1.1513)  acc1: 68.7500 (75.7070)  acc5: 93.7500 (91.4132)  time: 31.1726  data: 0.0037
Test:  [3120/3316]  eta: 1:35:13  loss: 0.8765 (1.1508)  acc1: 81.2500 (75.7169)  acc5: 93.7500 (91.4230)  time: 30.3940  data: 0.0039
Test:  [3040/3316]  eta: 2:17:37  loss: 1.1165 (1.1513)  acc1: 68.7500 (75.7070)  acc5: 93.7500 (91.4132)  time: 31.2306  data: 0.0038
Test:  [3050/3316]  eta: 2:12:14  loss: 1.0878 (1.1513)  acc1: 75.0000 (75.7088)  acc5: 93.7500 (91.4147)  time: 30.9810  data: 0.0034
Test:  [3120/3316]  eta: 1:35:17  loss: 0.8765 (1.1508)  acc1: 81.2500 (75.7169)  acc5: 93.7500 (91.4230)  time: 30.2851  data: 0.0037
Test:  [3060/3316]  eta: 2:06:59  loss: 1.0878 (1.1511)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4121)  time: 30.6561  data: 0.0038
Test:  [3080/3316]  eta: 1:56:22  loss: 1.0844 (1.1510)  acc1: 75.0000 (75.7100)  acc5: 93.7500 (91.4192)  time: 30.0780  data: 0.0033
Test:  [3050/3316]  eta: 2:12:32  loss: 1.0878 (1.1513)  acc1: 75.0000 (75.7088)  acc5: 93.7500 (91.4147)  time: 30.6276  data: 0.0033
Test:  [3050/3316]  eta: 2:12:34  loss: 1.0878 (1.1513)  acc1: 75.0000 (75.7088)  acc5: 93.7500 (91.4147)  time: 30.8704  data: 0.0031
Test:  [3130/3316]  eta: 1:30:22  loss: 0.9196 (1.1509)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4205)  time: 30.0083  data: 0.0033
Test:  [3050/3316]  eta: 2:12:39  loss: 1.0878 (1.1513)  acc1: 75.0000 (75.7088)  acc5: 93.7500 (91.4147)  time: 30.8484  data: 0.0031
Test:  [3060/3316]  eta: 2:07:16  loss: 1.0878 (1.1511)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4121)  time: 30.3577  data: 0.0030
Test:  [3130/3316]  eta: 1:30:26  loss: 0.9196 (1.1509)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4205)  time: 29.4364  data: 0.0028
Test:  [3070/3316]  eta: 2:02:01  loss: 1.0831 (1.1510)  acc1: 75.0000 (75.7123)  acc5: 93.7500 (91.4075)  time: 29.9366  data: 0.0028
Test:  [3090/3316]  eta: 1:51:27  loss: 1.0698 (1.1513)  acc1: 75.0000 (75.7037)  acc5: 93.7500 (91.4186)  time: 29.4813  data: 0.0025
Test:  [3060/3316]  eta: 2:07:32  loss: 1.0878 (1.1511)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4121)  time: 29.3982  data: 0.0026
Test:  [3060/3316]  eta: 2:07:34  loss: 1.0878 (1.1511)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4121)  time: 29.6596  data: 0.0025
Test:  [3140/3316]  eta: 1:25:30  loss: 1.0051 (1.1507)  acc1: 75.0000 (75.7124)  acc5: 87.5000 (91.4140)  time: 28.8636  data: 0.0024
Test:  [3060/3316]  eta: 2:07:39  loss: 1.0878 (1.1511)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4121)  time: 29.6909  data: 0.0024
Test:  [3070/3316]  eta: 2:02:17  loss: 1.0831 (1.1510)  acc1: 75.0000 (75.7123)  acc5: 93.7500 (91.4075)  time: 29.4334  data: 0.0025
Test:  [3140/3316]  eta: 1:25:34  loss: 1.0051 (1.1507)  acc1: 75.0000 (75.7124)  acc5: 87.5000 (91.4140)  time: 28.8192  data: 0.0025
Test:  [3080/3316]  eta: 1:57:03  loss: 1.0844 (1.1510)  acc1: 75.0000 (75.7100)  acc5: 93.7500 (91.4192)  time: 29.5159  data: 0.0025
Test:  [3100/3316]  eta: 1:46:30  loss: 1.3191 (1.1522)  acc1: 68.7500 (75.6792)  acc5: 87.5000 (91.4020)  time: 29.3464  data: 0.0025
Test:  [3070/3316]  eta: 2:02:32  loss: 1.0831 (1.1510)  acc1: 75.0000 (75.7123)  acc5: 93.7500 (91.4075)  time: 28.7080  data: 0.0024
Test:  [3070/3316]  eta: 2:02:35  loss: 1.0831 (1.1510)  acc1: 75.0000 (75.7123)  acc5: 93.7500 (91.4075)  time: 29.1002  data: 0.0025
Test:  [3150/3316]  eta: 1:20:38  loss: 1.1023 (1.1509)  acc1: 75.0000 (75.7041)  acc5: 87.5000 (91.4134)  time: 28.2738  data: 0.0024
Test:  [3070/3316]  eta: 2:02:39  loss: 1.0831 (1.1510)  acc1: 75.0000 (75.7123)  acc5: 93.7500 (91.4075)  time: 29.0763  data: 0.0024
Test:  [3080/3316]  eta: 1:57:18  loss: 1.0844 (1.1510)  acc1: 75.0000 (75.7100)  acc5: 93.7500 (91.4192)  time: 29.0728  data: 0.0024
Test:  [3150/3316]  eta: 1:20:42  loss: 1.1023 (1.1509)  acc1: 75.0000 (75.7041)  acc5: 87.5000 (91.4134)  time: 28.3761  data: 0.0026
Test:  [3090/3316]  eta: 1:52:05  loss: 1.0698 (1.1513)  acc1: 75.0000 (75.7037)  acc5: 93.7500 (91.4186)  time: 29.0467  data: 0.0028
Test:  [3110/3316]  eta: 1:41:34  loss: 1.1048 (1.1514)  acc1: 75.0000 (75.7072)  acc5: 93.7500 (91.4135)  time: 28.9268  data: 0.0028
Test:  [3080/3316]  eta: 1:57:32  loss: 1.0844 (1.1510)  acc1: 75.0000 (75.7100)  acc5: 93.7500 (91.4192)  time: 28.9121  data: 0.0026
Test:  [3080/3316]  eta: 1:57:35  loss: 1.0844 (1.1510)  acc1: 75.0000 (75.7100)  acc5: 93.7500 (91.4192)  time: 29.3544  data: 0.0026
Test:  [3160/3316]  eta: 1:15:46  loss: 1.3212 (1.1513)  acc1: 68.7500 (75.6861)  acc5: 93.7500 (91.4149)  time: 28.4804  data: 0.0027
Test:  [3080/3316]  eta: 1:57:39  loss: 1.0844 (1.1510)  acc1: 75.0000 (75.7100)  acc5: 93.7500 (91.4192)  time: 29.3393  data: 0.0028
Test:  [3090/3316]  eta: 1:52:20  loss: 1.0698 (1.1513)  acc1: 75.0000 (75.7037)  acc5: 93.7500 (91.4186)  time: 29.4475  data: 0.0028
Test:  [3160/3316]  eta: 1:15:50  loss: 1.3212 (1.1513)  acc1: 68.7500 (75.6861)  acc5: 93.7500 (91.4149)  time: 28.5696  data: 0.0028
Test:  [3100/3316]  eta: 1:47:07  loss: 1.3191 (1.1522)  acc1: 68.7500 (75.6792)  acc5: 87.5000 (91.4020)  time: 29.2175  data: 0.0029
Test:  [3120/3316]  eta: 1:36:38  loss: 0.8765 (1.1508)  acc1: 81.2500 (75.7169)  acc5: 93.7500 (91.4230)  time: 29.1863  data: 0.0028
Test:  [3090/3316]  eta: 1:52:33  loss: 1.0698 (1.1513)  acc1: 75.0000 (75.7037)  acc5: 93.7500 (91.4186)  time: 29.1125  data: 0.0028
Test:  [3090/3316]  eta: 1:52:36  loss: 1.0698 (1.1513)  acc1: 75.0000 (75.7037)  acc5: 93.7500 (91.4186)  time: 29.4791  data: 0.0029
Test:  [3170/3316]  eta: 1:10:54  loss: 1.1446 (1.1511)  acc1: 75.0000 (75.6977)  acc5: 93.7500 (91.4144)  time: 28.6429  data: 0.0028
Test:  [3090/3316]  eta: 1:52:40  loss: 1.0698 (1.1513)  acc1: 75.0000 (75.7037)  acc5: 93.7500 (91.4186)  time: 29.4971  data: 0.0028
Test:  [3100/3316]  eta: 1:47:21  loss: 1.3191 (1.1522)  acc1: 68.7500 (75.6792)  acc5: 87.5000 (91.4020)  time: 29.4597  data: 0.0031
Test:  [3170/3316]  eta: 1:10:58  loss: 1.1446 (1.1511)  acc1: 75.0000 (75.6977)  acc5: 93.7500 (91.4144)  time: 28.6701  data: 0.0029
Test:  [3110/3316]  eta: 1:42:09  loss: 1.1048 (1.1514)  acc1: 75.0000 (75.7072)  acc5: 93.7500 (91.4135)  time: 29.3175  data: 0.0027
Test:  [3130/3316]  eta: 1:31:42  loss: 0.9196 (1.1509)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4205)  time: 29.4017  data: 0.0028
Test:  [3100/3316]  eta: 1:47:34  loss: 1.3191 (1.1522)  acc1: 68.7500 (75.6792)  acc5: 87.5000 (91.4020)  time: 29.7089  data: 0.0028
Test:  [3180/3316]  eta: 1:06:03  loss: 0.9602 (1.1506)  acc1: 81.2500 (75.7093)  acc5: 93.7500 (91.4198)  time: 29.2328  data: 0.0028
Test:  [3100/3316]  eta: 1:47:37  loss: 1.3191 (1.1522)  acc1: 68.7500 (75.6792)  acc5: 87.5000 (91.4020)  time: 30.0268  data: 0.0033
Test:  [3100/3316]  eta: 1:47:41  loss: 1.3191 (1.1522)  acc1: 68.7500 (75.6792)  acc5: 87.5000 (91.4020)  time: 30.0026  data: 0.0029
Test:  [3180/3316]  eta: 1:06:06  loss: 0.9602 (1.1506)  acc1: 81.2500 (75.7093)  acc5: 93.7500 (91.4198)  time: 29.2134  data: 0.0031
Test:  [3110/3316]  eta: 1:42:23  loss: 1.1048 (1.1514)  acc1: 75.0000 (75.7072)  acc5: 93.7500 (91.4135)  time: 29.8734  data: 0.0032
Test:  [3120/3316]  eta: 1:37:12  loss: 0.8765 (1.1508)  acc1: 81.2500 (75.7169)  acc5: 93.7500 (91.4230)  time: 29.7499  data: 0.0028
Test:  [3140/3316]  eta: 1:26:47  loss: 1.0051 (1.1507)  acc1: 75.0000 (75.7124)  acc5: 87.5000 (91.4140)  time: 29.8012  data: 0.0029
Test:  [3110/3316]  eta: 1:42:35  loss: 1.1048 (1.1514)  acc1: 75.0000 (75.7072)  acc5: 93.7500 (91.4135)  time: 29.7132  data: 0.0029
Test:  [3190/3316]  eta: 1:01:12  loss: 1.1682 (1.1514)  acc1: 68.7500 (75.6934)  acc5: 87.5000 (91.4094)  time: 29.1604  data: 0.0028
Test:  [3110/3316]  eta: 1:42:38  loss: 1.1048 (1.1514)  acc1: 75.0000 (75.7072)  acc5: 93.7500 (91.4135)  time: 29.9972  data: 0.0029
Test:  [3110/3316]  eta: 1:42:42  loss: 1.1048 (1.1514)  acc1: 75.0000 (75.7072)  acc5: 93.7500 (91.4135)  time: 29.9709  data: 0.0029
Test:  [3190/3316]  eta: 1:01:14  loss: 1.1682 (1.1514)  acc1: 68.7500 (75.6934)  acc5: 87.5000 (91.4094)  time: 29.1459  data: 0.0030
Test:  [3120/3316]  eta: 1:37:25  loss: 0.8765 (1.1508)  acc1: 81.2500 (75.7169)  acc5: 93.7500 (91.4230)  time: 29.7974  data: 0.0030
Test:  [3130/3316]  eta: 1:32:14  loss: 0.9196 (1.1509)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4205)  time: 29.8213  data: 0.0028
Test:  [3150/3316]  eta: 1:21:51  loss: 1.1023 (1.1509)  acc1: 75.0000 (75.7041)  acc5: 87.5000 (91.4134)  time: 29.8142  data: 0.0028
Test:  [3120/3316]  eta: 1:37:36  loss: 0.8765 (1.1508)  acc1: 81.2500 (75.7169)  acc5: 93.7500 (91.4230)  time: 29.3071  data: 0.0029
Test:  [3200/3316]  eta: 0:56:20  loss: 1.1668 (1.1510)  acc1: 68.7500 (75.6990)  acc5: 87.5000 (91.4109)  time: 28.7156  data: 0.0027
Test:  [3120/3316]  eta: 1:37:39  loss: 0.8765 (1.1508)  acc1: 81.2500 (75.7169)  acc5: 93.7500 (91.4230)  time: 29.6404  data: 0.0027
Test:  [3200/3316]  eta: 0:56:23  loss: 1.1668 (1.1510)  acc1: 68.7500 (75.6990)  acc5: 87.5000 (91.4109)  time: 28.7724  data: 0.0028
Test:  [3120/3316]  eta: 1:37:43  loss: 0.8765 (1.1508)  acc1: 81.2500 (75.7169)  acc5: 93.7500 (91.4230)  time: 29.7138  data: 0.0028
Test:  [3130/3316]  eta: 1:32:26  loss: 0.9196 (1.1509)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4205)  time: 29.4470  data: 0.0028
Test:  [3140/3316]  eta: 1:27:16  loss: 1.0051 (1.1507)  acc1: 75.0000 (75.7124)  acc5: 87.5000 (91.4140)  time: 29.5796  data: 0.0027
Test:  [3160/3316]  eta: 1:16:55  loss: 1.3212 (1.1513)  acc1: 68.7500 (75.6861)  acc5: 93.7500 (91.4149)  time: 29.3667  data: 0.0027
Test:  [3130/3316]  eta: 1:32:37  loss: 0.9196 (1.1509)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4205)  time: 29.3168  data: 0.0028
Test:  [3210/3316]  eta: 0:51:28  loss: 1.0220 (1.1512)  acc1: 75.0000 (75.6949)  acc5: 93.7500 (91.4104)  time: 28.7671  data: 0.0027
Test:  [3130/3316]  eta: 1:32:40  loss: 0.9196 (1.1509)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4205)  time: 29.6428  data: 0.0027
Test:  [3210/3316]  eta: 0:51:31  loss: 1.0220 (1.1512)  acc1: 75.0000 (75.6949)  acc5: 93.7500 (91.4104)  time: 28.7822  data: 0.0027
Test:  [3130/3316]  eta: 1:32:43  loss: 0.9196 (1.1509)  acc1: 75.0000 (75.7126)  acc5: 93.7500 (91.4205)  time: 29.7384  data: 0.0027
Test:  [3140/3316]  eta: 1:27:28  loss: 1.0051 (1.1507)  acc1: 75.0000 (75.7124)  acc5: 87.5000 (91.4140)  time: 29.4067  data: 0.0027
Test:  [3150/3316]  eta: 1:22:19  loss: 1.1023 (1.1509)  acc1: 75.0000 (75.7041)  acc5: 87.5000 (91.4134)  time: 29.6262  data: 0.0028
Test:  [3170/3316]  eta: 1:11:59  loss: 1.1446 (1.1511)  acc1: 75.0000 (75.6977)  acc5: 93.7500 (91.4144)  time: 29.4924  data: 0.0029
Test:  [3140/3316]  eta: 1:27:38  loss: 1.0051 (1.1507)  acc1: 75.0000 (75.7124)  acc5: 87.5000 (91.4140)  time: 29.4725  data: 0.0029
Test:  [3220/3316]  eta: 0:46:37  loss: 1.2398 (1.1516)  acc1: 75.0000 (75.6908)  acc5: 93.7500 (91.4041)  time: 28.9757  data: 0.0028
Test:  [3140/3316]  eta: 1:27:41  loss: 1.0051 (1.1507)  acc1: 75.0000 (75.7124)  acc5: 87.5000 (91.4140)  time: 29.8317  data: 0.0029
Test:  [3220/3316]  eta: 0:46:39  loss: 1.2398 (1.1516)  acc1: 75.0000 (75.6908)  acc5: 93.7500 (91.4041)  time: 29.0611  data: 0.0028
Test:  [3140/3316]  eta: 1:27:44  loss: 1.0051 (1.1507)  acc1: 75.0000 (75.7124)  acc5: 87.5000 (91.4140)  time: 29.9026  data: 0.0026
Test:  [3150/3316]  eta: 1:22:30  loss: 1.1023 (1.1509)  acc1: 75.0000 (75.7041)  acc5: 87.5000 (91.4134)  time: 29.6682  data: 0.0027
Test:  [3160/3316]  eta: 1:17:22  loss: 1.3212 (1.1513)  acc1: 68.7500 (75.6861)  acc5: 93.7500 (91.4149)  time: 29.9816  data: 0.0031
Test:  [3180/3316]  eta: 1:07:03  loss: 0.9602 (1.1506)  acc1: 81.2500 (75.7093)  acc5: 93.7500 (91.4198)  time: 29.7997  data: 0.0033
Test:  [3150/3316]  eta: 1:22:40  loss: 1.1023 (1.1509)  acc1: 75.0000 (75.7041)  acc5: 87.5000 (91.4134)  time: 30.1451  data: 0.0030
Test:  [3230/3316]  eta: 0:41:46  loss: 1.1318 (1.1512)  acc1: 75.0000 (75.6964)  acc5: 93.7500 (91.4113)  time: 29.6669  data: 0.0031
Test:  [3150/3316]  eta: 1:22:43  loss: 1.1023 (1.1509)  acc1: 75.0000 (75.7041)  acc5: 87.5000 (91.4134)  time: 30.5382  data: 0.0033
Test:  [3230/3316]  eta: 0:41:48  loss: 1.1318 (1.1512)  acc1: 75.0000 (75.6964)  acc5: 93.7500 (91.4113)  time: 29.8128  data: 0.0032
Test:  [3150/3316]  eta: 1:22:46  loss: 1.1023 (1.1509)  acc1: 75.0000 (75.7041)  acc5: 87.5000 (91.4134)  time: 30.6548  data: 0.0028
Test:  [3160/3316]  eta: 1:17:32  loss: 1.3212 (1.1513)  acc1: 68.7500 (75.6861)  acc5: 93.7500 (91.4149)  time: 30.4556  data: 0.0031
Test:  [3170/3316]  eta: 1:12:25  loss: 1.1446 (1.1511)  acc1: 75.0000 (75.6977)  acc5: 93.7500 (91.4144)  time: 30.6648  data: 0.0033
Test:  [3190/3316]  eta: 1:02:08  loss: 1.1682 (1.1514)  acc1: 68.7500 (75.6934)  acc5: 87.5000 (91.4094)  time: 30.2113  data: 0.0033
Test:  [3160/3316]  eta: 1:17:41  loss: 1.3212 (1.1513)  acc1: 68.7500 (75.6861)  acc5: 93.7500 (91.4149)  time: 30.1034  data: 0.0031
Test:  [3240/3316]  eta: 0:36:54  loss: 1.1072 (1.1511)  acc1: 75.0000 (75.7039)  acc5: 93.7500 (91.4166)  time: 29.6056  data: 0.0029
Test:  [3160/3316]  eta: 1:17:44  loss: 1.3212 (1.1513)  acc1: 68.7500 (75.6861)  acc5: 93.7500 (91.4149)  time: 30.4028  data: 0.0032
Test:  [3240/3316]  eta: 0:36:56  loss: 1.1072 (1.1511)  acc1: 75.0000 (75.7039)  acc5: 93.7500 (91.4166)  time: 29.5310  data: 0.0030
Test:  [3160/3316]  eta: 1:17:47  loss: 1.3212 (1.1513)  acc1: 68.7500 (75.6861)  acc5: 93.7500 (91.4149)  time: 30.3670  data: 0.0029
Test:  [3170/3316]  eta: 1:12:34  loss: 1.1446 (1.1511)  acc1: 75.0000 (75.6977)  acc5: 93.7500 (91.4144)  time: 30.2075  data: 0.0030
Test:  [3180/3316]  eta: 1:07:27  loss: 0.9602 (1.1506)  acc1: 81.2500 (75.7093)  acc5: 93.7500 (91.4198)  time: 30.3235  data: 0.0030
Test:  [3200/3316]  eta: 0:57:12  loss: 1.1668 (1.1510)  acc1: 68.7500 (75.6990)  acc5: 87.5000 (91.4109)  time: 30.0079  data: 0.0031
Test:  [3170/3316]  eta: 1:12:42  loss: 1.1446 (1.1511)  acc1: 75.0000 (75.6977)  acc5: 93.7500 (91.4144)  time: 29.5188  data: 0.0029
Test:  [3250/3316]  eta: 0:32:03  loss: 1.2064 (1.1512)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4123)  time: 29.1194  data: 0.0026
Test:  [3170/3316]  eta: 1:12:45  loss: 1.1446 (1.1511)  acc1: 75.0000 (75.6977)  acc5: 93.7500 (91.4144)  time: 29.8456  data: 0.0029
Test:  [3250/3316]  eta: 0:32:04  loss: 1.2064 (1.1512)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4123)  time: 28.9303  data: 0.0027
Test:  [3170/3316]  eta: 1:12:47  loss: 1.1446 (1.1511)  acc1: 75.0000 (75.6977)  acc5: 93.7500 (91.4144)  time: 29.7514  data: 0.0026
Test:  [3180/3316]  eta: 1:07:35  loss: 0.9602 (1.1506)  acc1: 81.2500 (75.7093)  acc5: 93.7500 (91.4198)  time: 29.6249  data: 0.0027
Test:  [3190/3316]  eta: 1:02:29  loss: 1.1682 (1.1514)  acc1: 68.7500 (75.6934)  acc5: 87.5000 (91.4094)  time: 29.6769  data: 0.0028
Test:  [3210/3316]  eta: 0:52:16  loss: 1.0220 (1.1512)  acc1: 75.0000 (75.6949)  acc5: 93.7500 (91.4104)  time: 29.4164  data: 0.0029
Test:  [3180/3316]  eta: 1:07:43  loss: 0.9602 (1.1506)  acc1: 81.2500 (75.7093)  acc5: 93.7500 (91.4198)  time: 29.4787  data: 0.0030
Test:  [3260/3316]  eta: 0:27:12  loss: 1.1167 (1.1510)  acc1: 81.2500 (75.7053)  acc5: 93.7500 (91.4194)  time: 29.1392  data: 0.0026
Test:  [3260/3316]  eta: 0:27:13  loss: 1.1167 (1.1510)  acc1: 81.2500 (75.7053)  acc5: 93.7500 (91.4194)  time: 29.0188  data: 0.0029
Test:  [3180/3316]  eta: 1:07:46  loss: 0.9602 (1.1506)  acc1: 81.2500 (75.7093)  acc5: 93.7500 (91.4198)  time: 29.9082  data: 0.0027
Test:  [3190/3316]  eta: 1:02:37  loss: 1.1682 (1.1514)  acc1: 68.7500 (75.6934)  acc5: 87.5000 (91.4094)  time: 29.6742  data: 0.0028
Test:  [3180/3316]  eta: 1:07:48  loss: 0.9602 (1.1506)  acc1: 81.2500 (75.7093)  acc5: 93.7500 (91.4198)  time: 29.8577  data: 0.0027
Test:  [3200/3316]  eta: 0:57:32  loss: 1.1668 (1.1510)  acc1: 68.7500 (75.6990)  acc5: 87.5000 (91.4109)  time: 29.7215  data: 0.0028
Test:  [3220/3316]  eta: 0:47:20  loss: 1.2398 (1.1516)  acc1: 75.0000 (75.6908)  acc5: 93.7500 (91.4041)  time: 29.4530  data: 0.0031
Test:  [3270/3316]  eta: 0:22:20  loss: 0.9134 (1.1505)  acc1: 81.2500 (75.7242)  acc5: 93.7500 (91.4246)  time: 29.1176  data: 0.0025
Test:  [3190/3316]  eta: 1:02:44  loss: 1.1682 (1.1514)  acc1: 68.7500 (75.6934)  acc5: 87.5000 (91.4094)  time: 29.5029  data: 0.0029
Test:  [3270/3316]  eta: 0:22:21  loss: 0.9134 (1.1505)  acc1: 81.2500 (75.7242)  acc5: 93.7500 (91.4246)  time: 29.0183  data: 0.0029
Test:  [3190/3316]  eta: 1:02:47  loss: 1.1682 (1.1514)  acc1: 68.7500 (75.6934)  acc5: 87.5000 (91.4094)  time: 29.9181  data: 0.0029
Test:  [3200/3316]  eta: 0:57:39  loss: 1.1668 (1.1510)  acc1: 68.7500 (75.6990)  acc5: 87.5000 (91.4109)  time: 29.6614  data: 0.0029
Test:  [3190/3316]  eta: 1:02:49  loss: 1.1682 (1.1514)  acc1: 68.7500 (75.6934)  acc5: 87.5000 (91.4094)  time: 29.8341  data: 0.0028
Test:  [3210/3316]  eta: 0:52:34  loss: 1.0220 (1.1512)  acc1: 75.0000 (75.6949)  acc5: 93.7500 (91.4104)  time: 29.7494  data: 0.0028
Test:  [3230/3316]  eta: 0:42:24  loss: 1.1318 (1.1512)  acc1: 75.0000 (75.6964)  acc5: 93.7500 (91.4113)  time: 29.5758  data: 0.0030
Test:  [3280/3316]  eta: 0:17:29  loss: 0.8828 (1.1507)  acc1: 81.2500 (75.7162)  acc5: 93.7500 (91.4203)  time: 29.1471  data: 0.0027
Test:  [3200/3316]  eta: 0:57:45  loss: 1.1668 (1.1510)  acc1: 68.7500 (75.6990)  acc5: 87.5000 (91.4109)  time: 29.5384  data: 0.0028
Test:  [3280/3316]  eta: 0:17:29  loss: 0.8828 (1.1507)  acc1: 81.2500 (75.7162)  acc5: 93.7500 (91.4203)  time: 28.9967  data: 0.0030
Test:  [3200/3316]  eta: 0:57:48  loss: 1.1668 (1.1510)  acc1: 68.7500 (75.6990)  acc5: 87.5000 (91.4109)  time: 29.9019  data: 0.0029
Test:  [3210/3316]  eta: 0:52:41  loss: 1.0220 (1.1512)  acc1: 75.0000 (75.6949)  acc5: 93.7500 (91.4104)  time: 29.6563  data: 0.0029
Test:  [3200/3316]  eta: 0:57:50  loss: 1.1668 (1.1510)  acc1: 68.7500 (75.6990)  acc5: 87.5000 (91.4109)  time: 29.7855  data: 0.0028
Test:  [3220/3316]  eta: 0:47:36  loss: 1.2398 (1.1516)  acc1: 75.0000 (75.6908)  acc5: 93.7500 (91.4041)  time: 29.6480  data: 0.0029
Test:  [3240/3316]  eta: 0:37:28  loss: 1.1072 (1.1511)  acc1: 75.0000 (75.7039)  acc5: 93.7500 (91.4166)  time: 29.5000  data: 0.0026
Test:  [3290/3316]  eta: 0:12:37  loss: 1.1928 (1.1509)  acc1: 75.0000 (75.7160)  acc5: 87.5000 (91.4179)  time: 29.0319  data: 0.0029
Test:  [3210/3316]  eta: 0:52:46  loss: 1.0220 (1.1512)  acc1: 75.0000 (75.6949)  acc5: 93.7500 (91.4104)  time: 29.5221  data: 0.0027
Test:  [3290/3316]  eta: 0:12:38  loss: 1.1928 (1.1509)  acc1: 75.0000 (75.7160)  acc5: 87.5000 (91.4179)  time: 28.9329  data: 0.0028
Test:  [3210/3316]  eta: 0:52:49  loss: 1.0220 (1.1512)  acc1: 75.0000 (75.6949)  acc5: 93.7500 (91.4104)  time: 29.8456  data: 0.0026
Test:  [3220/3316]  eta: 0:47:42  loss: 1.2398 (1.1516)  acc1: 75.0000 (75.6908)  acc5: 93.7500 (91.4041)  time: 29.6761  data: 0.0028
Test:  [3210/3316]  eta: 0:52:50  loss: 1.0220 (1.1512)  acc1: 75.0000 (75.6949)  acc5: 93.7500 (91.4104)  time: 29.7732  data: 0.0027
Test:  [3230/3316]  eta: 0:42:39  loss: 1.1318 (1.1512)  acc1: 75.0000 (75.6964)  acc5: 93.7500 (91.4113)  time: 29.5435  data: 0.0029
Test:  [3250/3316]  eta: 0:32:32  loss: 1.2064 (1.1512)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4123)  time: 29.2542  data: 0.0025
Test:  [3300/3316]  eta: 0:07:46  loss: 0.9889 (1.1504)  acc1: 75.0000 (75.7308)  acc5: 93.7500 (91.4268)  time: 28.8652  data: 0.0031
Test:  [3220/3316]  eta: 0:47:47  loss: 1.2398 (1.1516)  acc1: 75.0000 (75.6908)  acc5: 93.7500 (91.4041)  time: 29.3977  data: 0.0027
Test:  [3300/3316]  eta: 0:07:46  loss: 0.9889 (1.1504)  acc1: 75.0000 (75.7308)  acc5: 93.7500 (91.4268)  time: 28.8531  data: 0.0031
Test:  [3220/3316]  eta: 0:47:50  loss: 1.2398 (1.1516)  acc1: 75.0000 (75.6908)  acc5: 93.7500 (91.4041)  time: 29.6710  data: 0.0027
Test:  [3230/3316]  eta: 0:42:44  loss: 1.1318 (1.1512)  acc1: 75.0000 (75.6964)  acc5: 93.7500 (91.4113)  time: 29.6698  data: 0.0027
Test:  [3220/3316]  eta: 0:47:51  loss: 1.2398 (1.1516)  acc1: 75.0000 (75.6908)  acc5: 93.7500 (91.4041)  time: 29.7211  data: 0.0027
Test:  [3240/3316]  eta: 0:37:41  loss: 1.1072 (1.1511)  acc1: 75.0000 (75.7039)  acc5: 93.7500 (91.4166)  time: 29.5338  data: 0.0027
Test:  [3260/3316]  eta: 0:27:36  loss: 1.1167 (1.1510)  acc1: 81.2500 (75.7053)  acc5: 93.7500 (91.4194)  time: 29.1717  data: 0.0027
Test:  [3310/3316]  eta: 0:02:54  loss: 0.9889 (1.1506)  acc1: 81.2500 (75.7267)  acc5: 93.7500 (91.4263)  time: 28.7889  data: 0.0030
Test:  [3230/3316]  eta: 0:42:48  loss: 1.1318 (1.1512)  acc1: 75.0000 (75.6964)  acc5: 93.7500 (91.4113)  time: 29.2853  data: 0.0027
Test:  [3310/3316]  eta: 0:02:54  loss: 0.9889 (1.1506)  acc1: 81.2500 (75.7267)  acc5: 93.7500 (91.4263)  time: 28.8333  data: 0.0030
Test:  [3230/3316]  eta: 0:42:51  loss: 1.1318 (1.1512)  acc1: 75.0000 (75.6964)  acc5: 93.7500 (91.4113)  time: 29.7721  data: 0.0028
Test:  [3315/3316]  eta: 0:00:29  loss: 0.9843 (1.1499)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4312)  time: 27.5866  data: 0.0029
Test: Total time: 1 day, 2:50:07 (29.1338 s / it)
* Acc@1 75.741 Acc@5 91.431 loss 1.150
Evaluation on 53041 test images- acc1: 75.74%, acc5: 91.43%
Test:  [3240/3316]  eta: 0:37:46  loss: 1.1072 (1.1511)  acc1: 75.0000 (75.7039)  acc5: 93.7500 (91.4166)  time: 29.6593  data: 0.0027
Test:  [3230/3316]  eta: 0:42:52  loss: 1.1318 (1.1512)  acc1: 75.0000 (75.6964)  acc5: 93.7500 (91.4113)  time: 29.6946  data: 0.0028
Test:  [3315/3316]  eta: 0:00:29  loss: 0.9843 (1.1499)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4312)  time: 27.6299  data: 0.0029
Test: Total time: 1 day, 2:51:14 (29.1539 s / it)
* Acc@1 75.741 Acc@5 91.431 loss 1.150
Evaluation on 53041 test images- acc1: 75.74%, acc5: 91.43%
Test:  [3250/3316]  eta: 0:32:43  loss: 1.2064 (1.1512)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4123)  time: 29.4984  data: 0.0026
Test:  [3270/3316]  eta: 0:22:40  loss: 0.9134 (1.1505)  acc1: 81.2500 (75.7242)  acc5: 93.7500 (91.4246)  time: 29.2788  data: 0.0029
Test:  [3240/3316]  eta: 0:37:49  loss: 1.1072 (1.1511)  acc1: 75.0000 (75.7039)  acc5: 93.7500 (91.4166)  time: 29.2676  data: 0.0027
Test:  [3240/3316]  eta: 0:37:51  loss: 1.1072 (1.1511)  acc1: 75.0000 (75.7039)  acc5: 93.7500 (91.4166)  time: 29.3958  data: 0.0027
Test:  [3250/3316]  eta: 0:32:47  loss: 1.2064 (1.1512)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4123)  time: 29.1583  data: 0.0025
Test:  [3240/3316]  eta: 0:37:53  loss: 1.1072 (1.1511)  acc1: 75.0000 (75.7039)  acc5: 93.7500 (91.4166)  time: 29.2133  data: 0.0026
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
job dir: /home/ada/satmae/SatMAEjob dir: /home/ada/satmae/SatMAE

Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=2,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='https://zenodo.org/record/7369797/files/fmow_pretrain.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=7,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='https://zenodo.org/record/7369797/files/fmow_pretrain.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=1,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='https://zenodo.org/record/7369797/files/fmow_pretrain.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='https://zenodo.org/record/7369797/files/fmow_pretrain.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=3,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='https://zenodo.org/record/7369797/files/fmow_pretrain.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=6,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='https://zenodo.org/record/7369797/files/fmow_pretrain.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=4,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='https://zenodo.org/record/7369797/files/fmow_pretrain.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
job dir: /home/ada/satmae/SatMAE
Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=16,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cpu',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
gpu=None,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=5,
log_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/temporal/evaluation/single_image_classification/pretrain',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='https://zenodo.org/record/7369797/files/fmow_pretrain.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/temporal/preprocessed/fmow/train/val_62classes.csv',
train_path='/home/ada/satmae/temporal/preprocessed/fmow/train/train_62classes.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
<util.datasets.CustomDatasetFromImages object at 0x7f8872efebd0>
<util.datasets.CustomDatasetFromImages object at 0x7f2aab121e10>
<util.datasets.CustomDatasetFromImages object at 0x7f2aab121e90>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f2aab121250>
<util.datasets.CustomDatasetFromImages object at 0x7f8872ef8cd0>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f8872ef8dd0>
<util.datasets.CustomDatasetFromImages object at 0x7fa67a610bd0>
<util.datasets.CustomDatasetFromImages object at 0x7f21f7008650>
<util.datasets.CustomDatasetFromImages object at 0x7f8d02384090>
<util.datasets.CustomDatasetFromImages object at 0x7fc5f3005dd0>
<util.datasets.CustomDatasetFromImages object at 0x7fa67a60ac90>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fa67a60a810>
<util.datasets.CustomDatasetFromImages object at 0x7f7a763e9110>
<util.datasets.CustomDatasetFromImages object at 0x7f05d8368e50>
<util.datasets.CustomDatasetFromImages object at 0x7f8d023753d0>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f8d09455350>
<util.datasets.CustomDatasetFromImages object at 0x7f21f6e7ad50>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f21f6e7ac50>
<util.datasets.CustomDatasetFromImages object at 0x7fc5eb266350>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fc5f247c4d0>
<util.datasets.CustomDatasetFromImages object at 0x7f7a763de390>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f7a763de1d0>
<util.datasets.CustomDatasetFromImages object at 0x7f05d83681d0>
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f05d8501690>
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
number of params (M): 304.33
base lr: 1.00e-03
actual lr: 6.25e-05
accumulate grad iterations: 1
effective batch size: 16
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
criterion = LabelSmoothingCrossEntropy()
Resume checkpoint https://zenodo.org/record/7369797/files/fmow_pretrain.pth
Resume checkpoint https://zenodo.org/record/7369797/files/fmow_pretrain.pth
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
Resume checkpoint https://zenodo.org/record/7369797/files/fmow_pretrain.pth
Resume checkpoint https://zenodo.org/record/7369797/files/fmow_pretrain.pth
Resume checkpoint https://zenodo.org/record/7369797/files/fmow_pretrain.pth
Resume checkpoint https://zenodo.org/record/7369797/files/fmow_pretrain.pth
Resume checkpoint https://zenodo.org/record/7369797/files/fmow_pretrain.pth
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
Resume checkpoint https://zenodo.org/record/7369797/files/fmow_pretrain.pth
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn('User provided device_type of \'cuda\', but CUDA is not available. Disabling')
Test:  [3260/3316]  eta: 0:27:46  loss: 1.1167 (1.1510)  acc1: 81.2500 (75.7053)  acc5: 93.7500 (91.4194)  time: 29.9899  data: 0.0024
Test:  [   0/3316]  eta: 1 day, 7:12:56  loss: 7.2022 (7.2022)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 33.8891  data: 0.6900
Test:  [   0/3316]  eta: 2 days, 5:14:29  loss: 7.2022 (7.2022)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 57.8015  data: 0.8434
Test:  [   0/3316]  eta: 2 days, 4:47:36  loss: 7.2022 (7.2022)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 57.3150  data: 0.6746
Test:  [   0/3316]  eta: 2 days, 4:21:11  loss: 7.2022 (7.2022)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 56.8369  data: 1.3221
Test:  [   0/3316]  eta: 2 days, 4:35:53  loss: 7.2022 (7.2022)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 57.1031  data: 1.2989
Test:  [   0/3316]  eta: 2 days, 4:36:37  loss: 7.2022 (7.2022)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 57.1162  data: 1.6039
Test:  [   0/3316]  eta: 2 days, 4:55:49  loss: 7.2022 (7.2022)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 57.4636  data: 1.5974
Test:  [   0/3316]  eta: 2 days, 4:39:56  loss: 7.2022 (7.2022)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 57.1761  data: 1.5435
Test:  [3280/3316]  eta: 0:17:45  loss: 0.8828 (1.1507)  acc1: 81.2500 (75.7162)  acc5: 93.7500 (91.4203)  time: 31.1055  data: 0.0026
Test:  [3250/3316]  eta: 0:32:52  loss: 1.2064 (1.1512)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4123)  time: 32.5772  data: 0.0025
Test:  [3250/3316]  eta: 0:32:56  loss: 1.2064 (1.1512)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4123)  time: 36.8632  data: 0.0027
Test:  [3260/3316]  eta: 0:27:51  loss: 1.1167 (1.1510)  acc1: 81.2500 (75.7053)  acc5: 93.7500 (91.4194)  time: 35.6076  data: 0.0027
Test:  [  10/3316]  eta: 1 day, 9:33:03  loss: 7.1826 (7.2122)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 36.5347  data: 0.1232
Test:  [3250/3316]  eta: 0:32:57  loss: 1.2064 (1.1512)  acc1: 75.0000 (75.6998)  acc5: 93.7500 (91.4123)  time: 38.9866  data: 0.0028
Test:  [  10/3316]  eta: 1 day, 14:00:37  loss: 7.1826 (7.2122)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 41.3907  data: 0.1203
Test:  [  10/3316]  eta: 1 day, 14:54:59  loss: 7.1826 (7.2122)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 42.3774  data: 0.0796
Test:  [  10/3316]  eta: 1 day, 21:15:50  loss: 7.1826 (7.2122)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 49.2894  data: 0.1472
Test:  [  10/3316]  eta: 1 day, 22:55:16  loss: 7.1826 (7.2122)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 51.0938  data: 0.0658
Test:  [  10/3316]  eta: 2 days, 0:11:15  loss: 7.1826 (7.2122)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 52.4729  data: 0.1428
Test:  [3290/3316]  eta: 0:12:51  loss: 1.1928 (1.1509)  acc1: 75.0000 (75.7160)  acc5: 87.5000 (91.4179)  time: 42.1985  data: 0.0029
Test:  [3270/3316]  eta: 0:22:52  loss: 0.9134 (1.1505)  acc1: 81.2500 (75.7242)  acc5: 93.7500 (91.4246)  time: 43.6567  data: 0.0031
Test:  [  10/3316]  eta: 2 days, 4:07:26  loss: 7.1826 (7.2122)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 56.7593  data: 0.1479
Test:  [  10/3316]  eta: 2 days, 4:14:31  loss: 7.1826 (7.2122)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 56.8879  data: 0.0642
Test:  [3260/3316]  eta: 0:27:58  loss: 1.1167 (1.1510)  acc1: 81.2500 (75.7053)  acc5: 93.7500 (91.4194)  time: 46.2391  data: 0.0027
Test:  [  20/3316]  eta: 1 day, 12:19:30  loss: 7.1653 (7.1900)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 38.8176  data: 0.0029
Test:  [3260/3316]  eta: 0:28:00  loss: 1.1167 (1.1510)  acc1: 81.2500 (75.7053)  acc5: 93.7500 (91.4194)  time: 49.0693  data: 0.0029
Test:  [3260/3316]  eta: 0:28:01  loss: 1.1167 (1.1510)  acc1: 81.2500 (75.7053)  acc5: 93.7500 (91.4194)  time: 47.7497  data: 0.0033
Test:  [3270/3316]  eta: 0:22:56  loss: 0.9134 (1.1505)  acc1: 81.2500 (75.7242)  acc5: 93.7500 (91.4246)  time: 48.4750  data: 0.0033
Test:  [  20/3316]  eta: 1 day, 18:41:20  loss: 7.1653 (7.1900)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 46.0677  data: 0.0026
Test:  [  20/3316]  eta: 1 day, 19:13:58  loss: 7.1653 (7.1900)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 46.7082  data: 0.0025
Test:  [  20/3316]  eta: 1 day, 20:17:02  loss: 7.1653 (7.1900)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 47.9279  data: 0.0026
Test:  [  20/3316]  eta: 1 day, 20:37:30  loss: 7.1653 (7.1900)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 48.3231  data: 0.0025
Test:  [3280/3316]  eta: 0:17:56  loss: 0.8828 (1.1507)  acc1: 81.2500 (75.7162)  acc5: 93.7500 (91.4203)  time: 51.5677  data: 0.0038
Test:  [  20/3316]  eta: 1 day, 22:23:36  loss: 7.1653 (7.1900)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 50.3502  data: 0.0021
Test:  [  20/3316]  eta: 2 days, 0:39:48  loss: 7.1653 (7.1900)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 54.1149  data: 0.0029
Test:  [3300/3316]  eta: 0:07:55  loss: 0.9889 (1.1504)  acc1: 75.0000 (75.7308)  acc5: 93.7500 (91.4268)  time: 53.5163  data: 0.0037
Test:  [  20/3316]  eta: 2 days, 3:23:13  loss: 7.1653 (7.1900)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 56.0672  data: 0.0033
Test:  [3270/3316]  eta: 0:23:01  loss: 0.9134 (1.1505)  acc1: 81.2500 (75.7242)  acc5: 93.7500 (91.4246)  time: 53.0958  data: 0.0030
Test:  [  30/3316]  eta: 1 day, 14:30:53  loss: 7.1653 (7.1916)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 45.3085  data: 0.0027
Test:  [3270/3316]  eta: 0:23:03  loss: 0.9134 (1.1505)  acc1: 81.2500 (75.7242)  acc5: 93.7500 (91.4246)  time: 45.7129  data: 0.0031
Test:  [  30/3316]  eta: 1 day, 16:40:14  loss: 7.1653 (7.1916)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 37.8458  data: 0.0022
Test:  [3270/3316]  eta: 0:23:04  loss: 0.9134 (1.1505)  acc1: 81.2500 (75.7242)  acc5: 93.7500 (91.4246)  time: 54.3800  data: 0.0037
Test:  [  30/3316]  eta: 1 day, 17:27:08  loss: 7.1653 (7.1916)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 41.5306  data: 0.0026
Test:  [3280/3316]  eta: 0:18:00  loss: 0.8828 (1.1507)  acc1: 81.2500 (75.7162)  acc5: 93.7500 (91.4203)  time: 54.1055  data: 0.0033
Test:  [  30/3316]  eta: 1 day, 19:04:42  loss: 7.1653 (7.1916)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 46.0431  data: 0.0029
Test:  [  30/3316]  eta: 1 day, 20:36:03  loss: 7.1653 (7.1916)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 52.4301  data: 0.0025
Test:  [  30/3316]  eta: 1 day, 22:16:30  loss: 7.1653 (7.1916)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 55.8156  data: 0.0028
Test:  [3280/3316]  eta: 0:18:01  loss: 0.8828 (1.1507)  acc1: 81.2500 (75.7162)  acc5: 93.7500 (91.4203)  time: 43.8390  data: 0.0028
Test:  [3290/3316]  eta: 0:12:59  loss: 1.1928 (1.1509)  acc1: 75.0000 (75.7160)  acc5: 87.5000 (91.4179)  time: 50.8049  data: 0.0032
Test:  [  30/3316]  eta: 2 days, 1:03:42  loss: 7.1653 (7.1916)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 55.2110  data: 0.0026
Test:  [3310/3316]  eta: 0:02:58  loss: 0.9889 (1.1506)  acc1: 81.2500 (75.7267)  acc5: 93.7500 (91.4263)  time: 55.2545  data: 0.0034
Test:  [  30/3316]  eta: 2 days, 2:49:14  loss: 7.1653 (7.1916)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 55.0108  data: 0.0034
Test:  [3280/3316]  eta: 0:18:04  loss: 0.8828 (1.1507)  acc1: 81.2500 (75.7162)  acc5: 93.7500 (91.4203)  time: 46.8349  data: 0.0036
Test:  [  40/3316]  eta: 1 day, 16:28:13  loss: 7.2647 (7.2183)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 49.5103  data: 0.0029
Test:  [  40/3316]  eta: 1 day, 16:35:51  loss: 7.2647 (7.2183)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 38.2504  data: 0.0031
Test:  [  40/3316]  eta: 1 day, 16:43:48  loss: 7.2647 (7.2183)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 42.1732  data: 0.0033
Test:  [3280/3316]  eta: 0:18:05  loss: 0.8828 (1.1507)  acc1: 81.2500 (75.7162)  acc5: 93.7500 (91.4203)  time: 50.6433  data: 0.0031
Test:  [3315/3316]  eta: 0:00:29  loss: 0.9843 (1.1499)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4312)  time: 51.2163  data: 0.0034
Test: Total time: 1 day, 3:28:34 (29.8296 s / it)
* Acc@1 75.741 Acc@5 91.431 loss 1.150
Evaluation on 53041 test images- acc1: 75.74%, acc5: 91.43%
Test:  [  40/3316]  eta: 1 day, 19:49:47  loss: 7.2647 (7.2183)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 47.9510  data: 0.0028
Test:  [3290/3316]  eta: 0:13:02  loss: 1.1928 (1.1509)  acc1: 75.0000 (75.7160)  acc5: 87.5000 (91.4179)  time: 55.3461  data: 0.0034
Test:  [3300/3316]  eta: 0:08:00  loss: 0.9889 (1.1504)  acc1: 75.0000 (75.7308)  acc5: 93.7500 (91.4268)  time: 47.9338  data: 0.0031
Test:  [  40/3316]  eta: 1 day, 22:10:05  loss: 7.2647 (7.2183)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 55.0475  data: 0.0035
Test:  [  40/3316]  eta: 1 day, 22:33:23  loss: 7.2647 (7.2183)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 45.9475  data: 0.0030
Test:  [  40/3316]  eta: 1 day, 22:57:00  loss: 7.2647 (7.2183)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 54.5887  data: 0.0030
Test:  [  40/3316]  eta: 1 day, 23:31:56  loss: 7.2647 (7.2183)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 51.2693  data: 0.0030
Test:  [3290/3316]  eta: 0:13:03  loss: 1.1928 (1.1509)  acc1: 75.0000 (75.7160)  acc5: 87.5000 (91.4179)  time: 47.3213  data: 0.0028
Test:  [3290/3316]  eta: 0:13:03  loss: 1.1928 (1.1509)  acc1: 75.0000 (75.7160)  acc5: 87.5000 (91.4179)  time: 39.4363  data: 0.0029
Test:  [3310/3316]  eta: 0:02:59  loss: 0.9889 (1.1506)  acc1: 81.2500 (75.7267)  acc5: 93.7500 (91.4263)  time: 34.8702  data: 0.0028
Test:  [  50/3316]  eta: 1 day, 17:08:48  loss: 7.2643 (7.2153)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 50.2520  data: 0.0028
Test:  [  50/3316]  eta: 1 day, 17:51:13  loss: 7.2643 (7.2153)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 44.4897  data: 0.0032
Test:  [  50/3316]  eta: 1 day, 18:12:53  loss: 7.2643 (7.2153)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 49.5933  data: 0.0033
Test:  [3290/3316]  eta: 0:13:05  loss: 1.1928 (1.1509)  acc1: 75.0000 (75.7160)  acc5: 87.5000 (91.4179)  time: 53.2806  data: 0.0031
Test:  [3315/3316]  eta: 0:00:29  loss: 0.9843 (1.1499)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4312)  time: 27.8041  data: 0.0026
Test: Total time: 1 day, 3:37:26 (29.9900 s / it)
* Acc@1 75.741 Acc@5 91.431 loss 1.150
Evaluation on 53041 test images- acc1: 75.74%, acc5: 91.43%
Test:  [  50/3316]  eta: 1 day, 19:14:24  loss: 7.2643 (7.2153)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 38.2256  data: 0.0027
Test:  [3300/3316]  eta: 0:08:02  loss: 0.9889 (1.1504)  acc1: 75.0000 (75.7308)  acc5: 93.7500 (91.4268)  time: 37.0051  data: 0.0030
Test:  [  50/3316]  eta: 1 day, 20:50:38  loss: 7.2643 (7.2153)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 55.6559  data: 0.0029
Test:  [3300/3316]  eta: 0:08:02  loss: 0.9889 (1.1504)  acc1: 75.0000 (75.7308)  acc5: 93.7500 (91.4268)  time: 55.7561  data: 0.0038
Test:  [  50/3316]  eta: 1 day, 21:17:39  loss: 7.2643 (7.2153)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 48.7314  data: 0.0028
Test:  [  50/3316]  eta: 1 day, 22:30:24  loss: 7.2643 (7.2153)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 44.4212  data: 0.0028
Test:  [  50/3316]  eta: 1 day, 22:43:28  loss: 7.2643 (7.2153)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 55.5947  data: 0.0035
Test:  [3300/3316]  eta: 0:08:03  loss: 0.9889 (1.1504)  acc1: 75.0000 (75.7308)  acc5: 93.7500 (91.4268)  time: 39.8350  data: 0.0030
Test:  [3300/3316]  eta: 0:08:03  loss: 0.9889 (1.1504)  acc1: 75.0000 (75.7308)  acc5: 93.7500 (91.4268)  time: 55.5892  data: 0.0035
Test:  [  60/3316]  eta: 1 day, 16:17:58  loss: 7.2197 (7.2160)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8209  data: 0.0024
Test:  [3310/3316]  eta: 0:03:00  loss: 0.9889 (1.1506)  acc1: 81.2500 (75.7267)  acc5: 93.7500 (91.4263)  time: 31.1298  data: 0.0028
Test:  [  60/3316]  eta: 1 day, 18:00:45  loss: 7.2197 (7.2160)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 35.9097  data: 0.0025
Test:  [  60/3316]  eta: 1 day, 18:20:35  loss: 7.2197 (7.2160)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 51.6213  data: 0.0028
Test:  [  60/3316]  eta: 1 day, 18:57:04  loss: 7.2197 (7.2160)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 53.0871  data: 0.0027
Test:  [  60/3316]  eta: 1 day, 19:14:04  loss: 7.2197 (7.2160)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 54.3411  data: 0.0028
Test:  [3315/3316]  eta: 0:00:30  loss: 0.9843 (1.1499)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4312)  time: 29.6254  data: 0.0027
Test: Total time: 1 day, 3:45:53 (30.1428 s / it)
* Acc@1 75.741 Acc@5 91.431 loss 1.150
Evaluation on 53041 test images- acc1: 75.74%, acc5: 91.43%
Test:  [3310/3316]  eta: 0:03:01  loss: 0.9889 (1.1506)  acc1: 81.2500 (75.7267)  acc5: 93.7500 (91.4263)  time: 29.0256  data: 0.0028
Test:  [  70/3316]  eta: 1 day, 14:07:52  loss: 7.2332 (7.2246)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5902  data: 0.0025
Test:  [  60/3316]  eta: 1 day, 21:22:52  loss: 7.2197 (7.2160)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 54.2988  data: 0.0028
Test:  [3310/3316]  eta: 0:03:01  loss: 0.9889 (1.1506)  acc1: 81.2500 (75.7267)  acc5: 93.7500 (91.4263)  time: 54.4755  data: 0.0033
Test:  [3310/3316]  eta: 0:03:01  loss: 0.9889 (1.1506)  acc1: 81.2500 (75.7267)  acc5: 93.7500 (91.4263)  time: 47.1969  data: 0.0032
Test:  [3315/3316]  eta: 0:00:30  loss: 0.9843 (1.1499)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4312)  time: 28.4641  data: 0.0028
Test: Total time: 1 day, 3:48:47 (30.1952 s / it)
* Acc@1 75.741 Acc@5 91.431 loss 1.150
Evaluation on 53041 test images- acc1: 75.74%, acc5: 91.43%
Test:  [  70/3316]  eta: 1 day, 15:35:50  loss: 7.2332 (7.2246)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5894  data: 0.0024
Test:  [  60/3316]  eta: 1 day, 22:49:48  loss: 7.2197 (7.2160)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 53.0423  data: 0.0028
Test:  [  60/3316]  eta: 1 day, 23:00:38  loss: 7.2197 (7.2160)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 54.5257  data: 0.0028
Test:  [3315/3316]  eta: 0:00:30  loss: 0.9843 (1.1499)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4312)  time: 39.3258  data: 0.0030
Test: Total time: 1 day, 3:50:29 (30.2261 s / it)
* Acc@1 75.741 Acc@5 91.431 loss 1.150
Evaluation on 53041 test images- acc1: 75.74%, acc5: 91.43%
Test:  [  70/3316]  eta: 1 day, 16:40:26  loss: 7.2332 (7.2246)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 41.4826  data: 0.0026
Test:  [  70/3316]  eta: 1 day, 17:45:59  loss: 7.2332 (7.2246)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 46.7994  data: 0.0033
Test:  [  80/3316]  eta: 1 day, 12:35:45  loss: 7.2486 (7.2257)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9856  data: 0.0025
Test:  [3315/3316]  eta: 0:00:30  loss: 0.9843 (1.1499)  acc1: 81.2500 (75.7414)  acc5: 93.7500 (91.4312)  time: 52.4978  data: 0.0033
Test: Total time: 1 day, 3:52:12 (30.2571 s / it)
* Acc@1 75.741 Acc@5 91.431 loss 1.150
Evaluation on 53041 test images- acc1: 75.74%, acc5: 91.43%
Test:  [  70/3316]  eta: 1 day, 18:36:59  loss: 7.2332 (7.2246)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 41.7416  data: 0.0026
Test:  [  70/3316]  eta: 1 day, 18:39:17  loss: 7.2332 (7.2246)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 52.2838  data: 0.0030
Test:  [  80/3316]  eta: 1 day, 13:52:58  loss: 7.2486 (7.2257)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0071  data: 0.0025
Test:  [  70/3316]  eta: 1 day, 20:08:26  loss: 7.2332 (7.2246)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 43.0689  data: 0.0027
Test:  [  70/3316]  eta: 1 day, 20:14:31  loss: 7.2332 (7.2246)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 42.8551  data: 0.0026
Test:  [  80/3316]  eta: 1 day, 14:49:15  loss: 7.2486 (7.2257)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1131  data: 0.0025
Test:  [  80/3316]  eta: 1 day, 15:38:53  loss: 7.2486 (7.2257)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 33.7962  data: 0.0031
Test:  [  90/3316]  eta: 1 day, 11:15:53  loss: 7.2151 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9291  data: 0.0024
Test:  [  80/3316]  eta: 1 day, 16:23:23  loss: 7.2486 (7.2257)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9431  data: 0.0025
Test:  [  80/3316]  eta: 1 day, 16:24:41  loss: 7.2486 (7.2257)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 39.2854  data: 0.0027
Test:  [  90/3316]  eta: 1 day, 12:25:00  loss: 7.2151 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0049  data: 0.0024
Test:  [  80/3316]  eta: 1 day, 17:42:05  loss: 7.2486 (7.2257)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9658  data: 0.0025
Test:  [  80/3316]  eta: 1 day, 17:48:46  loss: 7.2486 (7.2257)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8596  data: 0.0023
Test:  [  90/3316]  eta: 1 day, 13:14:44  loss: 7.2151 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9750  data: 0.0024
Test:  [  90/3316]  eta: 1 day, 13:59:23  loss: 7.2151 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4523  data: 0.0023
Test:  [ 100/3316]  eta: 1 day, 10:11:24  loss: 7.1864 (7.2263)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3911  data: 0.0023
Test:  [  90/3316]  eta: 1 day, 14:39:21  loss: 7.2151 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4876  data: 0.0024
Test:  [  90/3316]  eta: 1 day, 14:40:05  loss: 7.2151 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3988  data: 0.0024
Test:  [ 100/3316]  eta: 1 day, 11:14:40  loss: 7.1864 (7.2263)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5536  data: 0.0024
Test:  [  90/3316]  eta: 1 day, 15:47:35  loss: 7.2151 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2601  data: 0.0024
Test:  [  90/3316]  eta: 1 day, 15:55:28  loss: 7.2151 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5287  data: 0.0024
Test:  [ 100/3316]  eta: 1 day, 11:58:36  loss: 7.1864 (7.2263)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4655  data: 0.0023
Test:  [ 100/3316]  eta: 1 day, 12:39:19  loss: 7.1864 (7.2263)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5739  data: 0.0023
Test:  [ 110/3316]  eta: 1 day, 9:18:10  loss: 7.1866 (7.2270)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4880  data: 0.0023
Test:  [ 100/3316]  eta: 1 day, 13:15:07  loss: 7.1864 (7.2263)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5093  data: 0.0024
Test:  [ 100/3316]  eta: 1 day, 13:15:10  loss: 7.1864 (7.2263)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6110  data: 0.0024
Test:  [ 110/3316]  eta: 1 day, 10:15:31  loss: 7.1866 (7.2270)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5965  data: 0.0024
Test:  [ 100/3316]  eta: 1 day, 14:14:52  loss: 7.1864 (7.2263)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3258  data: 0.0024
Test:  [ 100/3316]  eta: 1 day, 14:22:49  loss: 7.1864 (7.2263)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5739  data: 0.0025
Test:  [ 110/3316]  eta: 1 day, 10:54:55  loss: 7.1866 (7.2270)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4810  data: 0.0023
Test:  [ 110/3316]  eta: 1 day, 11:32:26  loss: 7.1866 (7.2270)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5990  data: 0.0023
Test:  [ 120/3316]  eta: 1 day, 8:32:55  loss: 7.1866 (7.2237)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5366  data: 0.0024
Test:  [ 110/3316]  eta: 1 day, 12:04:25  loss: 7.1866 (7.2270)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4773  data: 0.0023
Test:  [ 110/3316]  eta: 1 day, 12:04:33  loss: 7.1866 (7.2270)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5515  data: 0.0024
Test:  [ 120/3316]  eta: 1 day, 9:25:53  loss: 7.1866 (7.2237)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5915  data: 0.0023
Test:  [ 110/3316]  eta: 1 day, 12:58:03  loss: 7.1866 (7.2270)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3354  data: 0.0024
Test:  [ 110/3316]  eta: 1 day, 13:06:17  loss: 7.1866 (7.2270)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5229  data: 0.0024
Test:  [ 120/3316]  eta: 1 day, 10:01:13  loss: 7.1866 (7.2237)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4655  data: 0.0023
Test:  [ 120/3316]  eta: 1 day, 10:37:44  loss: 7.1866 (7.2237)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7777  data: 0.0024
Test:  [ 130/3316]  eta: 1 day, 7:56:09  loss: 7.1718 (7.2229)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8172  data: 0.0027
Test:  [ 120/3316]  eta: 1 day, 11:09:21  loss: 7.1866 (7.2237)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9935  data: 0.0025
Test:  [ 120/3316]  eta: 1 day, 11:10:17  loss: 7.1866 (7.2237)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0959  data: 0.0025
Test:  [ 130/3316]  eta: 1 day, 8:47:32  loss: 7.1718 (7.2229)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1971  data: 0.0025
Test:  [ 120/3316]  eta: 1 day, 11:58:05  loss: 7.1866 (7.2237)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9010  data: 0.0025
Test:  [ 120/3316]  eta: 1 day, 12:07:13  loss: 7.1866 (7.2237)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1875  data: 0.0025
Test:  [ 130/3316]  eta: 1 day, 9:20:09  loss: 7.1718 (7.2229)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1291  data: 0.0027
Test:  [ 130/3316]  eta: 1 day, 9:52:57  loss: 7.1718 (7.2229)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2776  data: 0.0028
Test:  [ 140/3316]  eta: 1 day, 7:25:06  loss: 7.2281 (7.2308)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2573  data: 0.0028
Test:  [ 130/3316]  eta: 1 day, 10:19:44  loss: 7.1718 (7.2229)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2582  data: 0.0028
Test:  [ 130/3316]  eta: 1 day, 10:21:09  loss: 7.1718 (7.2229)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4213  data: 0.0027
Test:  [ 140/3316]  eta: 1 day, 8:10:48  loss: 7.2281 (7.2308)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3290  data: 0.0026
Test:  [ 130/3316]  eta: 1 day, 11:03:45  loss: 7.1718 (7.2229)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1182  data: 0.0027
Test:  [ 130/3316]  eta: 1 day, 11:13:24  loss: 7.1718 (7.2229)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4536  data: 0.0026
Test:  [ 140/3316]  eta: 1 day, 8:41:06  loss: 7.2281 (7.2308)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3516  data: 0.0028
Test:  [ 140/3316]  eta: 1 day, 9:11:55  loss: 7.2281 (7.2308)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3077  data: 0.0030
Test:  [ 150/3316]  eta: 1 day, 6:56:10  loss: 7.2513 (7.2307)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2235  data: 0.0027
Test:  [ 140/3316]  eta: 1 day, 9:36:56  loss: 7.2281 (7.2308)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0493  data: 0.0027
Test:  [ 140/3316]  eta: 1 day, 9:38:36  loss: 7.2281 (7.2308)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1638  data: 0.0026
Test:  [ 150/3316]  eta: 1 day, 7:37:45  loss: 7.2513 (7.2307)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8371  data: 0.0025
Test:  [ 140/3316]  eta: 1 day, 10:16:48  loss: 7.2281 (7.2308)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8236  data: 0.0026
Test:  [ 140/3316]  eta: 1 day, 10:26:15  loss: 7.2281 (7.2308)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0463  data: 0.0026
Test:  [ 150/3316]  eta: 1 day, 8:06:49  loss: 7.2513 (7.2307)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9715  data: 0.0027
Test:  [ 150/3316]  eta: 1 day, 8:35:23  loss: 7.2513 (7.2307)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0118  data: 0.0028
Test:  [ 160/3316]  eta: 1 day, 6:30:02  loss: 7.2247 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0035  data: 0.0026
Test:  [ 150/3316]  eta: 1 day, 8:59:16  loss: 7.2513 (7.2307)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1271  data: 0.0027
Test:  [ 150/3316]  eta: 1 day, 9:00:50  loss: 7.2513 (7.2307)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1749  data: 0.0025
Test:  [ 160/3316]  eta: 1 day, 7:08:46  loss: 7.2247 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8419  data: 0.0027
Test:  [ 150/3316]  eta: 1 day, 9:36:18  loss: 7.2513 (7.2307)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9968  data: 0.0026
Test:  [ 150/3316]  eta: 1 day, 9:45:34  loss: 7.2513 (7.2307)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1328  data: 0.0026
Test:  [ 160/3316]  eta: 1 day, 7:37:10  loss: 7.2247 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1541  data: 0.0028
Test:  [ 160/3316]  eta: 1 day, 8:04:07  loss: 7.2247 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1755  data: 0.0028
Test:  [ 170/3316]  eta: 1 day, 6:07:34  loss: 7.1987 (7.2298)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1649  data: 0.0027
Test:  [ 160/3316]  eta: 1 day, 8:29:37  loss: 7.2247 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7470  data: 0.0027
Test:  [ 160/3316]  eta: 1 day, 8:30:41  loss: 7.2247 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6883  data: 0.0026
Test:  [ 170/3316]  eta: 1 day, 6:46:28  loss: 7.1987 (7.2298)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5559  data: 0.0030
Test:  [ 160/3316]  eta: 1 day, 9:03:48  loss: 7.2247 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6663  data: 0.0031
Test:  [ 160/3316]  eta: 1 day, 9:12:51  loss: 7.2247 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7932  data: 0.0032
Test:  [ 170/3316]  eta: 1 day, 7:13:23  loss: 7.1987 (7.2298)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7861  data: 0.0032
Test:  [ 170/3316]  eta: 1 day, 7:38:50  loss: 7.1987 (7.2298)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8501  data: 0.0036
Test:  [ 180/3316]  eta: 1 day, 5:49:49  loss: 7.1855 (7.2288)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8365  data: 0.0033
Test:  [ 170/3316]  eta: 1 day, 8:00:26  loss: 7.1987 (7.2298)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9559  data: 0.0033
Test:  [ 170/3316]  eta: 1 day, 8:01:31  loss: 7.1987 (7.2298)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9083  data: 0.0032
Test:  [ 180/3316]  eta: 1 day, 6:23:53  loss: 7.1855 (7.2288)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8079  data: 0.0033
Test:  [ 170/3316]  eta: 1 day, 8:32:10  loss: 7.1987 (7.2298)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8315  data: 0.0034
Test:  [ 170/3316]  eta: 1 day, 8:40:24  loss: 7.1987 (7.2298)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8458  data: 0.0036
Test:  [ 180/3316]  eta: 1 day, 6:49:31  loss: 7.1855 (7.2288)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8979  data: 0.0034
Test:  [ 180/3316]  eta: 1 day, 7:13:31  loss: 7.1855 (7.2288)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9282  data: 0.0037
Test:  [ 190/3316]  eta: 1 day, 5:31:13  loss: 7.2030 (7.2284)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9182  data: 0.0037
Test:  [ 180/3316]  eta: 1 day, 7:33:27  loss: 7.1855 (7.2288)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4778  data: 0.0035
Test:  [ 180/3316]  eta: 1 day, 7:34:28  loss: 7.1855 (7.2288)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4899  data: 0.0033
Test:  [ 190/3316]  eta: 1 day, 6:02:35  loss: 7.2030 (7.2284)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3250  data: 0.0031
Test:  [ 180/3316]  eta: 1 day, 8:02:33  loss: 7.1855 (7.2288)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2858  data: 0.0031
Test:  [ 180/3316]  eta: 1 day, 8:09:48  loss: 7.1855 (7.2288)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1549  data: 0.0030
Test:  [ 190/3316]  eta: 1 day, 6:26:41  loss: 7.2030 (7.2284)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3542  data: 0.0031
Test:  [ 190/3316]  eta: 1 day, 6:49:35  loss: 7.2030 (7.2284)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4019  data: 0.0031
Test:  [ 200/3316]  eta: 1 day, 5:13:03  loss: 7.2698 (7.2286)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3370  data: 0.0031
Test:  [ 190/3316]  eta: 1 day, 7:07:56  loss: 7.2030 (7.2284)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2409  data: 0.0029
Test:  [ 190/3316]  eta: 1 day, 7:09:08  loss: 7.2030 (7.2284)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2839  data: 0.0029
Test:  [ 200/3316]  eta: 1 day, 5:42:07  loss: 7.2698 (7.2286)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0651  data: 0.0027
Test:  [ 190/3316]  eta: 1 day, 7:35:05  loss: 7.2030 (7.2284)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0404  data: 0.0027
Test:  [ 190/3316]  eta: 1 day, 7:41:39  loss: 7.2030 (7.2284)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9061  data: 0.0027
Test:  [ 200/3316]  eta: 1 day, 6:05:29  loss: 7.2698 (7.2286)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1493  data: 0.0027
Test:  [ 210/3316]  eta: 1 day, 4:56:07  loss: 7.2252 (7.2264)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1517  data: 0.0028
Test:  [ 200/3316]  eta: 1 day, 6:27:24  loss: 7.2698 (7.2286)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2357  data: 0.0028
Test:  [ 200/3316]  eta: 1 day, 6:44:29  loss: 7.2698 (7.2286)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0889  data: 0.0027
Test:  [ 200/3316]  eta: 1 day, 6:45:42  loss: 7.2698 (7.2286)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1495  data: 0.0027
Test:  [ 210/3316]  eta: 1 day, 5:25:27  loss: 7.2252 (7.2264)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3812  data: 0.0027
Test:  [ 200/3316]  eta: 1 day, 7:12:16  loss: 7.2698 (7.2286)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4278  data: 0.0028
Test:  [ 200/3316]  eta: 1 day, 7:17:58  loss: 7.2698 (7.2286)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2728  data: 0.0028
Test:  [ 210/3316]  eta: 1 day, 5:47:55  loss: 7.2252 (7.2264)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5429  data: 0.0029
Test:  [ 220/3316]  eta: 1 day, 4:42:06  loss: 7.1849 (7.2258)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5386  data: 0.0027
Test:  [ 210/3316]  eta: 1 day, 6:08:47  loss: 7.2252 (7.2264)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5944  data: 0.0028
Test:  [ 210/3316]  eta: 1 day, 6:24:34  loss: 7.2252 (7.2264)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4493  data: 0.0030
Test:  [ 210/3316]  eta: 1 day, 6:26:03  loss: 7.2252 (7.2264)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5317  data: 0.0028
Test:  [ 220/3316]  eta: 1 day, 5:07:26  loss: 7.1849 (7.2258)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3389  data: 0.0030
Test:  [ 210/3316]  eta: 1 day, 6:48:38  loss: 7.2252 (7.2264)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3803  data: 0.0031
Test:  [ 210/3316]  eta: 1 day, 6:53:26  loss: 7.2252 (7.2264)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1513  data: 0.0028
Test:  [ 220/3316]  eta: 1 day, 5:28:50  loss: 7.1849 (7.2258)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3997  data: 0.0030
Test:  [ 230/3316]  eta: 1 day, 4:26:39  loss: 7.1850 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4300  data: 0.0027
Test:  [ 220/3316]  eta: 1 day, 5:48:55  loss: 7.1849 (7.2258)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4563  data: 0.0028
Test:  [ 220/3316]  eta: 1 day, 6:03:43  loss: 7.1849 (7.2258)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3201  data: 0.0030
Test:  [ 220/3316]  eta: 1 day, 6:05:21  loss: 7.1849 (7.2258)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4342  data: 0.0028
Test:  [ 230/3316]  eta: 1 day, 4:50:21  loss: 7.1850 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7746  data: 0.0028
Test:  [ 220/3316]  eta: 1 day, 6:26:34  loss: 7.1849 (7.2258)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8390  data: 0.0029
Test:  [ 220/3316]  eta: 1 day, 6:30:35  loss: 7.1849 (7.2258)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5973  data: 0.0026
Test:  [ 230/3316]  eta: 1 day, 5:10:26  loss: 7.1850 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7043  data: 0.0027
Test:  [ 240/3316]  eta: 1 day, 4:11:25  loss: 7.1950 (7.2262)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7771  data: 0.0026
Test:  [ 230/3316]  eta: 1 day, 5:29:42  loss: 7.1850 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7812  data: 0.0024
Test:  [ 230/3316]  eta: 1 day, 5:43:15  loss: 7.1850 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6081  data: 0.0024
Test:  [ 230/3316]  eta: 1 day, 5:45:11  loss: 7.1850 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7393  data: 0.0024
Test:  [ 240/3316]  eta: 1 day, 4:33:07  loss: 7.1950 (7.2262)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4490  data: 0.0025
Test:  [ 230/3316]  eta: 1 day, 6:04:36  loss: 7.1850 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4947  data: 0.0025
Test:  [ 230/3316]  eta: 1 day, 6:08:16  loss: 7.1850 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3411  data: 0.0024
Test:  [ 240/3316]  eta: 1 day, 4:52:21  loss: 7.1950 (7.2262)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3883  data: 0.0024
Test:  [ 250/3316]  eta: 1 day, 3:58:35  loss: 7.2311 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0073  data: 0.0023
Test:  [ 240/3316]  eta: 1 day, 5:13:20  loss: 7.1950 (7.2262)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0213  data: 0.0023
Test:  [ 240/3316]  eta: 1 day, 5:26:03  loss: 7.1950 (7.2262)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8414  data: 0.0024
Test:  [ 240/3316]  eta: 1 day, 5:28:15  loss: 7.1950 (7.2262)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0075  data: 0.0024
Test:  [ 250/3316]  eta: 1 day, 4:19:04  loss: 7.2311 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7154  data: 0.0024
Test:  [ 240/3316]  eta: 1 day, 5:46:27  loss: 7.1950 (7.2262)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7417  data: 0.0024
Test:  [ 240/3316]  eta: 1 day, 5:49:38  loss: 7.1950 (7.2262)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6321  data: 0.0023
Test:  [ 250/3316]  eta: 1 day, 4:37:33  loss: 7.2311 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7438  data: 0.0023
Test:  [ 260/3316]  eta: 1 day, 3:44:43  loss: 7.1843 (7.2260)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9699  data: 0.0023
Test:  [ 250/3316]  eta: 1 day, 4:56:09  loss: 7.2311 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9873  data: 0.0024
Test:  [ 250/3316]  eta: 1 day, 5:08:12  loss: 7.2311 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9019  data: 0.0025
Test:  [ 250/3316]  eta: 1 day, 5:10:30  loss: 7.2311 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0327  data: 0.0024
Test:  [ 260/3316]  eta: 1 day, 4:04:02  loss: 7.1843 (7.2260)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8186  data: 0.0024
Test:  [ 250/3316]  eta: 1 day, 5:27:36  loss: 7.2311 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8743  data: 0.0025
Test:  [ 250/3316]  eta: 1 day, 5:30:18  loss: 7.2311 (7.2250)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7141  data: 0.0024
Test:  [ 260/3316]  eta: 1 day, 4:21:40  loss: 7.1843 (7.2260)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8156  data: 0.0024
Test:  [ 260/3316]  eta: 1 day, 4:39:47  loss: 7.1843 (7.2260)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5274  data: 0.0026
Test:  [ 270/3316]  eta: 1 day, 3:31:42  loss: 7.2627 (7.2275)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5940  data: 0.0024
Test:  [ 260/3316]  eta: 1 day, 4:51:30  loss: 7.1843 (7.2260)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5368  data: 0.0024
Test:  [ 260/3316]  eta: 1 day, 4:53:48  loss: 7.1843 (7.2260)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6110  data: 0.0024
Test:  [ 270/3316]  eta: 1 day, 3:49:51  loss: 7.2627 (7.2275)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4085  data: 0.0024
Test:  [ 260/3316]  eta: 1 day, 5:09:58  loss: 7.1843 (7.2260)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4759  data: 0.0027
Test:  [ 260/3316]  eta: 1 day, 5:12:09  loss: 7.1843 (7.2260)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2865  data: 0.0025
Test:  [ 270/3316]  eta: 1 day, 4:06:33  loss: 7.2627 (7.2275)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3261  data: 0.0023
Test:  [ 270/3316]  eta: 1 day, 4:23:55  loss: 7.2627 (7.2275)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3932  data: 0.0025
Test:  [ 280/3316]  eta: 1 day, 3:19:12  loss: 7.2697 (7.2297)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6244  data: 0.0024
Test:  [ 270/3316]  eta: 1 day, 4:35:35  loss: 7.2627 (7.2275)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5462  data: 0.0023
Test:  [ 270/3316]  eta: 1 day, 4:37:55  loss: 7.2627 (7.2275)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6074  data: 0.0025
Test:  [ 280/3316]  eta: 1 day, 3:36:18  loss: 7.2697 (7.2297)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4239  data: 0.0023
Test:  [ 270/3316]  eta: 1 day, 4:52:53  loss: 7.2627 (7.2275)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3987  data: 0.0026
Test:  [ 270/3316]  eta: 1 day, 4:54:57  loss: 7.2627 (7.2275)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2897  data: 0.0025
Test:  [ 280/3316]  eta: 1 day, 3:52:21  loss: 7.2697 (7.2297)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3615  data: 0.0023
Test:  [ 280/3316]  eta: 1 day, 4:09:25  loss: 7.2697 (7.2297)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4593  data: 0.0023
Test:  [ 290/3316]  eta: 1 day, 3:08:38  loss: 7.2151 (7.2274)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0115  data: 0.0023
Test:  [ 280/3316]  eta: 1 day, 4:23:02  loss: 7.2697 (7.2297)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2381  data: 0.0025
Test:  [ 280/3316]  eta: 1 day, 4:25:30  loss: 7.2697 (7.2297)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3354  data: 0.0024
Test:  [ 290/3316]  eta: 1 day, 3:25:47  loss: 7.2151 (7.2274)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1127  data: 0.0027
Test:  [ 280/3316]  eta: 1 day, 4:39:17  loss: 7.2697 (7.2297)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0150  data: 0.0025
Test:  [ 280/3316]  eta: 1 day, 4:41:11  loss: 7.2697 (7.2297)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9867  data: 0.0026
Test:  [ 290/3316]  eta: 1 day, 3:41:20  loss: 7.2151 (7.2274)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1438  data: 0.0025
Test:  [ 290/3316]  eta: 1 day, 3:57:17  loss: 7.2151 (7.2274)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1057  data: 0.0026
Test:  [ 300/3316]  eta: 1 day, 2:58:30  loss: 7.2058 (7.2282)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4310  data: 0.0025
Test:  [ 290/3316]  eta: 1 day, 4:08:37  loss: 7.2151 (7.2274)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2662  data: 0.0026
Test:  [ 290/3316]  eta: 1 day, 4:11:23  loss: 7.2151 (7.2274)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4393  data: 0.0025
Test:  [ 300/3316]  eta: 1 day, 3:13:20  loss: 7.2058 (7.2282)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1241  data: 0.0028
Test:  [ 290/3316]  eta: 1 day, 4:24:19  loss: 7.2151 (7.2274)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1782  data: 0.0027
Test:  [ 290/3316]  eta: 1 day, 4:26:09  loss: 7.2151 (7.2274)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1514  data: 0.0027
Test:  [ 300/3316]  eta: 1 day, 3:28:57  loss: 7.2058 (7.2282)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3421  data: 0.0026
Test:  [ 300/3316]  eta: 1 day, 3:44:33  loss: 7.2058 (7.2282)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2709  data: 0.0028
Test:  [ 310/3316]  eta: 1 day, 2:48:00  loss: 7.2748 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2305  data: 0.0027
Test:  [ 300/3316]  eta: 1 day, 3:55:38  loss: 7.2058 (7.2282)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8121  data: 0.0027
Test:  [ 300/3316]  eta: 1 day, 3:58:34  loss: 7.2058 (7.2282)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0021  data: 0.0026
Test:  [ 310/3316]  eta: 1 day, 3:02:26  loss: 7.2748 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7612  data: 0.0027
Test:  [ 300/3316]  eta: 1 day, 4:10:30  loss: 7.2058 (7.2282)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7522  data: 0.0026
Test:  [ 300/3316]  eta: 1 day, 4:12:17  loss: 7.2058 (7.2282)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7540  data: 0.0026
Test:  [ 310/3316]  eta: 1 day, 3:17:26  loss: 7.2748 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9308  data: 0.0028
Test:  [ 310/3316]  eta: 1 day, 3:32:05  loss: 7.2748 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8781  data: 0.0027
Test:  [ 320/3316]  eta: 1 day, 2:37:49  loss: 7.3139 (7.2320)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9960  data: 0.0030
Test:  [ 310/3316]  eta: 1 day, 3:42:57  loss: 7.2748 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9782  data: 0.0027
Test:  [ 310/3316]  eta: 1 day, 3:45:55  loss: 7.2748 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1027  data: 0.0027
Test:  [ 320/3316]  eta: 1 day, 2:51:26  loss: 7.3139 (7.2320)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9361  data: 0.0029
Test:  [ 310/3316]  eta: 1 day, 3:57:01  loss: 7.2748 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8163  data: 0.0026
Test:  [ 310/3316]  eta: 1 day, 3:58:47  loss: 7.2748 (7.2305)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8347  data: 0.0026
Test:  [ 320/3316]  eta: 1 day, 3:05:54  loss: 7.3139 (7.2320)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9129  data: 0.0028
Test:  [ 320/3316]  eta: 1 day, 3:19:57  loss: 7.3139 (7.2320)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7580  data: 0.0028
Test:  [ 330/3316]  eta: 1 day, 2:27:52  loss: 7.2598 (7.2325)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9543  data: 0.0030
Test:  [ 320/3316]  eta: 1 day, 3:30:31  loss: 7.3139 (7.2320)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8358  data: 0.0027
Test:  [ 320/3316]  eta: 1 day, 3:35:33  loss: 7.3139 (7.2320)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5759  data: 0.0028
Test:  [ 330/3316]  eta: 1 day, 2:43:47  loss: 7.2598 (7.2325)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7751  data: 0.0030
Test:  [ 320/3316]  eta: 1 day, 3:47:27  loss: 7.3139 (7.2320)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8267  data: 0.0032
Test:  [ 320/3316]  eta: 1 day, 3:49:14  loss: 7.3139 (7.2320)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8655  data: 0.0034
Test:  [ 330/3316]  eta: 1 day, 2:58:03  loss: 7.2598 (7.2325)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8629  data: 0.0034
Test:  [ 330/3316]  eta: 1 day, 3:11:40  loss: 7.2598 (7.2325)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8439  data: 0.0043
Test:  [ 340/3316]  eta: 1 day, 2:21:32  loss: 7.2514 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.0685  data: 0.0047
Test:  [ 330/3316]  eta: 1 day, 3:21:57  loss: 7.2598 (7.2325)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8881  data: 0.0042
Test:  [ 330/3316]  eta: 1 day, 3:25:36  loss: 7.2598 (7.2325)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.1819  data: 0.0042
Test:  [ 340/3316]  eta: 1 day, 2:33:56  loss: 7.2514 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9518  data: 0.0050
Test:  [ 330/3316]  eta: 1 day, 3:35:42  loss: 7.2598 (7.2325)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.0972  data: 0.0044
Test:  [ 330/3316]  eta: 1 day, 3:37:35  loss: 7.2598 (7.2325)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.1735  data: 0.0046
Test:  [ 340/3316]  eta: 1 day, 2:48:12  loss: 7.2514 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.2078  data: 0.0043
Test:  [ 340/3316]  eta: 1 day, 3:01:26  loss: 7.2514 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.2410  data: 0.0045
Test:  [ 350/3316]  eta: 1 day, 2:13:02  loss: 7.1781 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.4155  data: 0.0051
Test:  [ 340/3316]  eta: 1 day, 3:11:20  loss: 7.2514 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.2463  data: 0.0045
Test:  [ 340/3316]  eta: 1 day, 3:14:56  loss: 7.2514 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8676  data: 0.0044
Test:  [ 350/3316]  eta: 1 day, 2:24:47  loss: 7.1781 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2942  data: 0.0052
Test:  [ 340/3316]  eta: 1 day, 3:24:19  loss: 7.2514 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2668  data: 0.0040
Test:  [ 340/3316]  eta: 1 day, 3:26:19  loss: 7.2514 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3804  data: 0.0041
Test:  [ 350/3316]  eta: 1 day, 2:38:25  loss: 7.1781 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3885  data: 0.0039
Test:  [ 350/3316]  eta: 1 day, 2:51:08  loss: 7.1781 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3739  data: 0.0032
Test:  [ 360/3316]  eta: 1 day, 2:04:19  loss: 7.1784 (7.2332)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4758  data: 0.0032
Test:  [ 350/3316]  eta: 1 day, 3:00:36  loss: 7.1781 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3182  data: 0.0031
Test:  [ 350/3316]  eta: 1 day, 3:04:05  loss: 7.1781 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3406  data: 0.0034
Test:  [ 360/3316]  eta: 1 day, 2:15:25  loss: 7.1784 (7.2332)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2794  data: 0.0033
Test:  [ 350/3316]  eta: 1 day, 3:12:48  loss: 7.1781 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0718  data: 0.0029
Test:  [ 350/3316]  eta: 1 day, 3:14:39  loss: 7.1781 (7.2335)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1070  data: 0.0030
Test:  [ 360/3316]  eta: 1 day, 2:27:59  loss: 7.1784 (7.2332)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9728  data: 0.0028
Test:  [ 360/3316]  eta: 1 day, 2:40:00  loss: 7.1784 (7.2332)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8281  data: 0.0027
Test:  [ 370/3316]  eta: 1 day, 1:54:47  loss: 7.2659 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9382  data: 0.0028
Test:  [ 360/3316]  eta: 1 day, 2:49:07  loss: 7.1784 (7.2332)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7653  data: 0.0028
Test:  [ 360/3316]  eta: 1 day, 2:52:29  loss: 7.1784 (7.2332)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7650  data: 0.0029
Test:  [ 370/3316]  eta: 1 day, 2:05:17  loss: 7.2659 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7287  data: 0.0028
Test:  [ 360/3316]  eta: 1 day, 3:00:41  loss: 7.1784 (7.2332)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5445  data: 0.0026
Test:  [ 360/3316]  eta: 1 day, 3:02:25  loss: 7.1784 (7.2332)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4916  data: 0.0027
Test:  [ 370/3316]  eta: 1 day, 2:17:14  loss: 7.2659 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4062  data: 0.0025
Test:  [ 370/3316]  eta: 1 day, 2:28:52  loss: 7.2659 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2868  data: 0.0023
Test:  [ 380/3316]  eta: 1 day, 1:45:20  loss: 7.2305 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4799  data: 0.0024
Test:  [ 370/3316]  eta: 1 day, 2:37:45  loss: 7.2659 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2811  data: 0.0025
Test:  [ 370/3316]  eta: 1 day, 2:41:05  loss: 7.2659 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3030  data: 0.0023
Test:  [ 380/3316]  eta: 1 day, 1:55:16  loss: 7.2305 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2831  data: 0.0026
Test:  [ 370/3316]  eta: 1 day, 2:49:56  loss: 7.2659 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5538  data: 0.0026
Test:  [ 370/3316]  eta: 1 day, 2:51:42  loss: 7.2659 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5627  data: 0.0027
Test:  [ 380/3316]  eta: 1 day, 2:07:48  loss: 7.2305 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5605  data: 0.0028
Test:  [ 380/3316]  eta: 1 day, 2:19:04  loss: 7.2305 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5444  data: 0.0033
Test:  [ 390/3316]  eta: 1 day, 1:37:10  loss: 7.2305 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8327  data: 0.0034
Test:  [ 380/3316]  eta: 1 day, 2:27:49  loss: 7.2305 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6101  data: 0.0037
Test:  [ 380/3316]  eta: 1 day, 2:31:03  loss: 7.2305 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6313  data: 0.0035
Test:  [ 390/3316]  eta: 1 day, 1:46:30  loss: 7.2305 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6094  data: 0.0035
Test:  [ 380/3316]  eta: 1 day, 2:38:28  loss: 7.2305 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5114  data: 0.0035
Test:  [ 380/3316]  eta: 1 day, 2:40:12  loss: 7.2305 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5582  data: 0.0036
Test:  [ 390/3316]  eta: 1 day, 1:57:37  loss: 7.2305 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5457  data: 0.0035
Test:  [ 390/3316]  eta: 1 day, 2:08:38  loss: 7.2305 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5707  data: 0.0037
Test:  [ 400/3316]  eta: 1 day, 1:28:07  loss: 7.2209 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8233  data: 0.0035
Test:  [ 390/3316]  eta: 1 day, 2:17:12  loss: 7.2305 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6503  data: 0.0037
Test:  [ 390/3316]  eta: 1 day, 2:20:21  loss: 7.2305 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6545  data: 0.0035
Test:  [ 400/3316]  eta: 1 day, 1:37:02  loss: 7.2209 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6292  data: 0.0034
Test:  [ 390/3316]  eta: 1 day, 2:27:32  loss: 7.2305 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1875  data: 0.0034
Test:  [ 390/3316]  eta: 1 day, 2:29:08  loss: 7.2305 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1651  data: 0.0033
Test:  [ 400/3316]  eta: 1 day, 1:47:44  loss: 7.2209 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1672  data: 0.0031
Test:  [ 400/3316]  eta: 1 day, 1:58:29  loss: 7.2209 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2145  data: 0.0026
Test:  [ 410/3316]  eta: 1 day, 1:19:06  loss: 7.1985 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3148  data: 0.0024
Test:  [ 400/3316]  eta: 1 day, 2:06:48  loss: 7.2209 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2435  data: 0.0023
Test:  [ 400/3316]  eta: 1 day, 2:10:06  loss: 7.2209 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3423  data: 0.0023
Test:  [ 410/3316]  eta: 1 day, 1:27:47  loss: 7.1985 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2542  data: 0.0024
Test:  [ 400/3316]  eta: 1 day, 2:16:47  loss: 7.2209 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2187  data: 0.0024
Test:  [ 400/3316]  eta: 1 day, 2:18:26  loss: 7.2209 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2148  data: 0.0023
Test:  [ 410/3316]  eta: 1 day, 1:38:16  loss: 7.1985 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2516  data: 0.0023
Test:  [ 410/3316]  eta: 1 day, 1:49:12  loss: 7.1985 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4691  data: 0.0027
Test:  [ 420/3316]  eta: 1 day, 1:10:55  loss: 7.2621 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5132  data: 0.0026
Test:  [ 410/3316]  eta: 1 day, 1:57:21  loss: 7.1985 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4971  data: 0.0029
Test:  [ 410/3316]  eta: 1 day, 2:00:42  loss: 7.1985 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6480  data: 0.0032
Test:  [ 420/3316]  eta: 1 day, 1:19:22  loss: 7.2621 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5216  data: 0.0031
Test:  [ 410/3316]  eta: 1 day, 2:07:07  loss: 7.1985 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4986  data: 0.0033
Test:  [ 410/3316]  eta: 1 day, 2:08:49  loss: 7.1985 (7.2337)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5746  data: 0.0033
Test:  [ 420/3316]  eta: 1 day, 1:30:00  loss: 7.2621 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7452  data: 0.0033
Test:  [ 420/3316]  eta: 1 day, 1:40:05  loss: 7.2621 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7030  data: 0.0035
Test:  [ 430/3316]  eta: 1 day, 1:02:53  loss: 7.2976 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7827  data: 0.0035
Test:  [ 420/3316]  eta: 1 day, 1:48:13  loss: 7.2621 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8188  data: 0.0037
Test:  [ 420/3316]  eta: 1 day, 1:51:32  loss: 7.2621 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9032  data: 0.0038
Test:  [ 430/3316]  eta: 1 day, 1:11:14  loss: 7.2976 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8445  data: 0.0037
Test:  [ 420/3316]  eta: 1 day, 1:57:55  loss: 7.2621 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9262  data: 0.0036
Test:  [ 420/3316]  eta: 1 day, 1:59:52  loss: 7.2621 (7.2342)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1021  data: 0.0036
Test:  [ 430/3316]  eta: 1 day, 1:22:06  loss: 7.2976 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2603  data: 0.0037
Test:  [ 430/3316]  eta: 1 day, 1:32:07  loss: 7.2976 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1137  data: 0.0034
Test:  [ 440/3316]  eta: 1 day, 0:55:52  loss: 7.2332 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1797  data: 0.0035
Test:  [ 430/3316]  eta: 1 day, 1:40:12  loss: 7.2976 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2706  data: 0.0034
Test:  [ 430/3316]  eta: 1 day, 1:43:28  loss: 7.2976 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3195  data: 0.0032
Test:  [ 440/3316]  eta: 1 day, 1:04:11  loss: 7.2332 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3199  data: 0.0033
Test:  [ 430/3316]  eta: 1 day, 1:49:36  loss: 7.2976 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3418  data: 0.0030
Test:  [ 430/3316]  eta: 1 day, 1:51:43  loss: 7.2976 (7.2351)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5706  data: 0.0031
Test:  [ 440/3316]  eta: 1 day, 1:14:51  loss: 7.2332 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5912  data: 0.0029
Test:  [ 440/3316]  eta: 1 day, 1:24:31  loss: 7.2332 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6426  data: 0.0029
Test:  [ 450/3316]  eta: 1 day, 0:49:09  loss: 7.1936 (7.2341)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6761  data: 0.0030
Test:  [ 440/3316]  eta: 1 day, 1:33:51  loss: 7.2332 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.3830  data: 0.0031
Test:  [ 440/3316]  eta: 1 day, 1:37:03  loss: 7.2332 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.4185  data: 0.0034
Test:  [ 450/3316]  eta: 1 day, 0:58:40  loss: 7.1936 (7.2341)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.4256  data: 0.0038
Test:  [ 440/3316]  eta: 1 day, 1:43:00  loss: 7.2332 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.3697  data: 0.0037
Test:  [ 440/3316]  eta: 1 day, 1:45:05  loss: 7.2332 (7.2343)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.4730  data: 0.0043
Test:  [ 450/3316]  eta: 1 day, 1:09:03  loss: 7.1936 (7.2341)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.4624  data: 0.0043
Test:  [ 450/3316]  eta: 1 day, 1:18:35  loss: 7.1936 (7.2341)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.4714  data: 0.0050
Test:  [ 460/3316]  eta: 1 day, 0:44:01  loss: 7.1814 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.5073  data: 0.0048
Test:  [ 450/3316]  eta: 1 day, 1:26:19  loss: 7.1936 (7.2341)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.5044  data: 0.0049
Test:  [ 450/3316]  eta: 1 day, 1:29:31  loss: 7.1936 (7.2341)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.5508  data: 0.0047
Test:  [ 460/3316]  eta: 1 day, 0:51:57  loss: 7.1814 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.5225  data: 0.0050
Test:  [ 450/3316]  eta: 1 day, 1:35:12  loss: 7.1936 (7.2341)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.4898  data: 0.0045
Test:  [ 450/3316]  eta: 1 day, 1:37:27  loss: 7.1936 (7.2341)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.5987  data: 0.0052
Test:  [ 460/3316]  eta: 1 day, 1:02:04  loss: 7.1814 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.5081  data: 0.0049
Test:  [ 460/3316]  eta: 1 day, 1:11:30  loss: 7.1814 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.6346  data: 0.0051
Test:  [ 470/3316]  eta: 1 day, 0:37:40  loss: 7.2670 (7.2344)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.6371  data: 0.0049
Test:  [ 460/3316]  eta: 1 day, 1:18:50  loss: 7.1814 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8969  data: 0.0048
Test:  [ 460/3316]  eta: 1 day, 1:21:56  loss: 7.1814 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9211  data: 0.0043
Test:  [ 470/3316]  eta: 1 day, 0:45:12  loss: 7.2670 (7.2344)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8805  data: 0.0041
Test:  [ 460/3316]  eta: 1 day, 1:27:17  loss: 7.1814 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7752  data: 0.0037
Test:  [ 460/3316]  eta: 1 day, 1:29:31  loss: 7.1814 (7.2345)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9026  data: 0.0039
Test:  [ 470/3316]  eta: 1 day, 0:54:37  loss: 7.2670 (7.2344)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6576  data: 0.0035
Test:  [ 470/3316]  eta: 1 day, 1:03:37  loss: 7.2670 (7.2344)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6250  data: 0.0030
Test:  [ 480/3316]  eta: 1 day, 0:30:23  loss: 7.2670 (7.2360)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5230  data: 0.0030
Test:  [ 470/3316]  eta: 1 day, 1:10:37  loss: 7.2670 (7.2344)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4505  data: 0.0030
Test:  [ 470/3316]  eta: 1 day, 1:13:42  loss: 7.2670 (7.2344)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4713  data: 0.0030
Test:  [ 480/3316]  eta: 1 day, 0:37:50  loss: 7.2670 (7.2360)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4762  data: 0.0029
Test:  [ 470/3316]  eta: 1 day, 1:18:55  loss: 7.2670 (7.2344)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3697  data: 0.0029
Test:  [ 470/3316]  eta: 1 day, 1:21:09  loss: 7.2670 (7.2344)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4213  data: 0.0029
Test:  [ 480/3316]  eta: 1 day, 0:46:58  loss: 7.2670 (7.2360)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2258  data: 0.0028
Test:  [ 480/3316]  eta: 1 day, 0:55:46  loss: 7.2670 (7.2360)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1344  data: 0.0028
Test:  [ 490/3316]  eta: 1 day, 0:23:16  loss: 7.2637 (7.2357)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0555  data: 0.0027
Test:  [ 480/3316]  eta: 1 day, 1:02:37  loss: 7.2670 (7.2360)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0681  data: 0.0029
Test:  [ 480/3316]  eta: 1 day, 1:06:08  loss: 7.2670 (7.2360)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3542  data: 0.0027
Test:  [ 490/3316]  eta: 1 day, 0:31:07  loss: 7.2637 (7.2357)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4090  data: 0.0029
Test:  [ 480/3316]  eta: 1 day, 1:11:12  loss: 7.2670 (7.2360)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3406  data: 0.0035
Test:  [ 480/3316]  eta: 1 day, 1:13:30  loss: 7.2670 (7.2360)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4296  data: 0.0034
Test:  [ 490/3316]  eta: 1 day, 0:39:59  loss: 7.2637 (7.2357)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3561  data: 0.0038
Test:  [ 490/3316]  eta: 1 day, 0:48:41  loss: 7.2637 (7.2357)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4281  data: 0.0045
Test:  [ 500/3316]  eta: 1 day, 0:16:47  loss: 7.2356 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3910  data: 0.0043
Test:  [ 490/3316]  eta: 1 day, 0:55:17  loss: 7.2637 (7.2357)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3858  data: 0.0043
Test:  [ 490/3316]  eta: 1 day, 0:58:19  loss: 7.2637 (7.2357)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4326  data: 0.0042
Test:  [ 500/3316]  eta: 1 day, 0:24:03  loss: 7.2356 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4768  data: 0.0042
Test:  [ 490/3316]  eta: 1 day, 1:03:10  loss: 7.2637 (7.2357)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3796  data: 0.0045
Test:  [ 490/3316]  eta: 1 day, 1:05:26  loss: 7.2637 (7.2357)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4529  data: 0.0040
Test:  [ 500/3316]  eta: 1 day, 0:32:38  loss: 7.2356 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4096  data: 0.0043
Test:  [ 500/3316]  eta: 1 day, 0:41:00  loss: 7.2356 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3978  data: 0.0045
Test:  [ 510/3316]  eta: 1 day, 0:09:52  loss: 7.2568 (7.2379)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4055  data: 0.0045
Test:  [ 500/3316]  eta: 1 day, 0:47:27  loss: 7.2356 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3531  data: 0.0045
Test:  [ 500/3316]  eta: 1 day, 0:50:24  loss: 7.2356 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1390  data: 0.0043
Test:  [ 510/3316]  eta: 1 day, 0:16:47  loss: 7.2568 (7.2379)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0936  data: 0.0043
Test:  [ 500/3316]  eta: 1 day, 0:55:00  loss: 7.2356 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0205  data: 0.0037
Test:  [ 500/3316]  eta: 1 day, 0:57:22  loss: 7.2356 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1104  data: 0.0036
Test:  [ 510/3316]  eta: 1 day, 0:25:07  loss: 7.2568 (7.2379)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0208  data: 0.0034
Test:  [ 510/3316]  eta: 1 day, 0:33:36  loss: 7.2568 (7.2379)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1155  data: 0.0029
Test:  [ 520/3316]  eta: 1 day, 0:03:01  loss: 7.2765 (7.2386)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1222  data: 0.0029
Test:  [ 510/3316]  eta: 1 day, 0:39:49  loss: 7.2568 (7.2379)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0811  data: 0.0032
Test:  [ 510/3316]  eta: 1 day, 0:42:46  loss: 7.2568 (7.2379)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1147  data: 0.0029
Test:  [ 520/3316]  eta: 1 day, 0:09:46  loss: 7.2765 (7.2386)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0180  data: 0.0028
Test:  [ 510/3316]  eta: 1 day, 0:47:09  loss: 7.2568 (7.2379)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9707  data: 0.0033
Test:  [ 510/3316]  eta: 1 day, 0:49:35  loss: 7.2568 (7.2379)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1204  data: 0.0031
Test:  [ 520/3316]  eta: 1 day, 0:17:53  loss: 7.2765 (7.2386)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9785  data: 0.0027
Test:  [ 520/3316]  eta: 1 day, 0:26:01  loss: 7.2765 (7.2386)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0601  data: 0.0028
Test:  [ 530/3316]  eta: 23:56:01  loss: 7.2260 (7.2385)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9949  data: 0.0026
Test:  [ 520/3316]  eta: 1 day, 0:32:07  loss: 7.2765 (7.2386)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0284  data: 0.0032
Test:  [ 520/3316]  eta: 1 day, 0:34:56  loss: 7.2765 (7.2386)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0254  data: 0.0029
Test:  [ 530/3316]  eta: 1 day, 0:02:35  loss: 7.2260 (7.2385)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9679  data: 0.0027
Test:  [ 520/3316]  eta: 1 day, 0:39:29  loss: 7.2765 (7.2386)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1134  data: 0.0033
Test:  [ 520/3316]  eta: 1 day, 0:41:59  loss: 7.2765 (7.2386)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2465  data: 0.0029
Test:  [ 530/3316]  eta: 1 day, 0:10:49  loss: 7.2260 (7.2385)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1206  data: 0.0026
Test:  [ 530/3316]  eta: 1 day, 0:18:53  loss: 7.2260 (7.2385)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1007  data: 0.0026
Test:  [ 540/3316]  eta: 23:49:22  loss: 7.1737 (7.2370)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0234  data: 0.0027
Test:  [ 530/3316]  eta: 1 day, 0:24:45  loss: 7.2260 (7.2385)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0604  data: 0.0031
Test:  [ 530/3316]  eta: 1 day, 0:27:28  loss: 7.2260 (7.2385)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9992  data: 0.0028
Test:  [ 540/3316]  eta: 23:55:44  loss: 7.1737 (7.2370)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9733  data: 0.0032
Test:  [ 530/3316]  eta: 1 day, 0:31:36  loss: 7.2260 (7.2385)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9703  data: 0.0030
Test:  [ 530/3316]  eta: 1 day, 0:34:12  loss: 7.2260 (7.2385)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1235  data: 0.0028
Test:  [ 540/3316]  eta: 1 day, 0:03:37  loss: 7.1737 (7.2370)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0380  data: 0.0027
Test:  [ 540/3316]  eta: 1 day, 0:11:34  loss: 7.1737 (7.2370)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1389  data: 0.0027
Test:  [ 550/3316]  eta: 23:42:32  loss: 7.2057 (7.2374)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0304  data: 0.0027
Test:  [ 540/3316]  eta: 1 day, 0:17:13  loss: 7.1737 (7.2370)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0306  data: 0.0029
Test:  [ 540/3316]  eta: 1 day, 0:19:53  loss: 7.1737 (7.2370)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0161  data: 0.0029
Test:  [ 550/3316]  eta: 23:48:43  loss: 7.2057 (7.2374)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9705  data: 0.0031
Test:  [ 540/3316]  eta: 1 day, 0:23:51  loss: 7.1737 (7.2370)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7899  data: 0.0030
Test:  [ 540/3316]  eta: 1 day, 0:26:32  loss: 7.1737 (7.2370)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9647  data: 0.0029
Test:  [ 550/3316]  eta: 23:56:32  loss: 7.2057 (7.2374)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9381  data: 0.0028
Test:  [ 550/3316]  eta: 1 day, 0:04:48  loss: 7.2057 (7.2374)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2690  data: 0.0027
Test:  [ 560/3316]  eta: 23:36:25  loss: 7.2118 (7.2371)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2785  data: 0.0028
Test:  [ 550/3316]  eta: 1 day, 0:10:26  loss: 7.2057 (7.2374)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2721  data: 0.0034
Test:  [ 550/3316]  eta: 1 day, 0:13:05  loss: 7.2057 (7.2374)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3070  data: 0.0036
Test:  [ 560/3316]  eta: 23:42:27  loss: 7.2118 (7.2371)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2292  data: 0.0035
Test:  [ 550/3316]  eta: 1 day, 0:16:52  loss: 7.2057 (7.2374)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2078  data: 0.0041
Test:  [ 550/3316]  eta: 1 day, 0:19:40  loss: 7.2057 (7.2374)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3876  data: 0.0037
Test:  [ 560/3316]  eta: 23:50:08  loss: 7.2118 (7.2371)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3198  data: 0.0037
Test:  [ 560/3316]  eta: 23:57:47  loss: 7.2118 (7.2371)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3418  data: 0.0039
Test:  [ 570/3316]  eta: 23:29:40  loss: 7.2087 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2430  data: 0.0040
Test:  [ 560/3316]  eta: 1 day, 0:03:09  loss: 7.2118 (7.2371)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3150  data: 0.0044
Test:  [ 560/3316]  eta: 1 day, 0:05:41  loss: 7.2118 (7.2371)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3085  data: 0.0044
Test:  [ 570/3316]  eta: 23:35:39  loss: 7.2087 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2825  data: 0.0044
Test:  [ 560/3316]  eta: 1 day, 0:09:19  loss: 7.2118 (7.2371)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2075  data: 0.0045
Test:  [ 560/3316]  eta: 1 day, 0:12:14  loss: 7.2118 (7.2371)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4171  data: 0.0041
Test:  [ 570/3316]  eta: 23:43:07  loss: 7.2087 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2720  data: 0.0040
Test:  [ 570/3316]  eta: 23:50:35  loss: 7.2087 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9858  data: 0.0041
Test:  [ 580/3316]  eta: 23:22:59  loss: 7.2614 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8221  data: 0.0038
Test:  [ 570/3316]  eta: 23:55:52  loss: 7.2087 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9184  data: 0.0038
Test:  [ 570/3316]  eta: 23:58:25  loss: 7.2087 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9221  data: 0.0037
Test:  [ 580/3316]  eta: 23:28:53  loss: 7.2614 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9081  data: 0.0037
Test:  [ 570/3316]  eta: 1 day, 0:01:52  loss: 7.2087 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8096  data: 0.0032
Test:  [ 570/3316]  eta: 1 day, 0:04:50  loss: 7.2087 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0019  data: 0.0031
Test:  [ 580/3316]  eta: 23:36:13  loss: 7.2614 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8783  data: 0.0029
Test:  [ 580/3316]  eta: 23:43:28  loss: 7.2614 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8345  data: 0.0029
Test:  [ 590/3316]  eta: 23:16:22  loss: 7.2614 (7.2375)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8330  data: 0.0027
Test:  [ 580/3316]  eta: 23:48:41  loss: 7.2614 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8761  data: 0.0028
Test:  [ 580/3316]  eta: 23:51:22  loss: 7.2614 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0426  data: 0.0029
Test:  [ 590/3316]  eta: 23:22:19  loss: 7.2614 (7.2375)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9718  data: 0.0027
Test:  [ 580/3316]  eta: 23:54:40  loss: 7.2614 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9246  data: 0.0027
Test:  [ 580/3316]  eta: 23:57:42  loss: 7.2614 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0755  data: 0.0028
Test:  [ 590/3316]  eta: 23:29:35  loss: 7.2614 (7.2375)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0352  data: 0.0026
Test:  [ 590/3316]  eta: 23:36:41  loss: 7.2614 (7.2375)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9964  data: 0.0030
Test:  [ 600/3316]  eta: 23:10:07  loss: 7.2441 (7.2364)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0332  data: 0.0027
Test:  [ 590/3316]  eta: 23:41:44  loss: 7.2614 (7.2375)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9803  data: 0.0030
Test:  [ 590/3316]  eta: 23:44:20  loss: 7.2614 (7.2375)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0783  data: 0.0030
Test:  [ 600/3316]  eta: 23:15:41  loss: 7.2441 (7.2364)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9640  data: 0.0030
Test:  [ 590/3316]  eta: 23:47:23  loss: 7.2614 (7.2375)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9178  data: 0.0026
Test:  [ 590/3316]  eta: 23:50:28  loss: 7.2614 (7.2375)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0701  data: 0.0027
Test:  [ 600/3316]  eta: 23:22:46  loss: 7.2441 (7.2364)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0049  data: 0.0027
Test:  [ 600/3316]  eta: 23:29:44  loss: 7.2441 (7.2364)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9988  data: 0.0029
Test:  [ 610/3316]  eta: 23:03:57  loss: 7.2362 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2656  data: 0.0025
Test:  [ 600/3316]  eta: 23:34:57  loss: 7.2441 (7.2364)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1516  data: 0.0032
Test:  [ 600/3316]  eta: 23:37:29  loss: 7.2441 (7.2364)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1148  data: 0.0030
Test:  [ 610/3316]  eta: 23:09:17  loss: 7.2362 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0023  data: 0.0034
Test:  [ 600/3316]  eta: 23:40:27  loss: 7.2441 (7.2364)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9825  data: 0.0028
Test:  [ 600/3316]  eta: 23:43:36  loss: 7.2441 (7.2364)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1426  data: 0.0027
Test:  [ 610/3316]  eta: 23:16:17  loss: 7.2362 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0166  data: 0.0028
Test:  [ 610/3316]  eta: 23:23:11  loss: 7.2362 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0765  data: 0.0029
Test:  [ 620/3316]  eta: 22:57:40  loss: 7.2537 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1798  data: 0.0027
Test:  [ 610/3316]  eta: 23:28:06  loss: 7.2362 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1212  data: 0.0031
Test:  [ 610/3316]  eta: 23:30:34  loss: 7.2362 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1116  data: 0.0030
Test:  [ 620/3316]  eta: 23:02:52  loss: 7.2537 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0866  data: 0.0033
Test:  [ 610/3316]  eta: 23:33:30  loss: 7.2362 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1053  data: 0.0031
Test:  [ 610/3316]  eta: 23:36:33  loss: 7.2362 (7.2365)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1619  data: 0.0030
Test:  [ 620/3316]  eta: 23:09:44  loss: 7.2537 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1191  data: 0.0030
Test:  [ 620/3316]  eta: 23:16:34  loss: 7.2537 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2198  data: 0.0030
Test:  [ 630/3316]  eta: 22:51:32  loss: 7.2673 (7.2369)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1360  data: 0.0028
Test:  [ 620/3316]  eta: 23:21:17  loss: 7.2537 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0156  data: 0.0029
Test:  [ 630/3316]  eta: 22:56:28  loss: 7.2673 (7.2369)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0064  data: 0.0032
Test:  [ 620/3316]  eta: 23:23:50  loss: 7.2537 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0928  data: 0.0030
Test:  [ 620/3316]  eta: 23:26:35  loss: 7.2537 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0223  data: 0.0030
Test:  [ 620/3316]  eta: 23:29:57  loss: 7.2537 (7.2372)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2661  data: 0.0032
Test:  [ 630/3316]  eta: 23:03:44  loss: 7.2673 (7.2369)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3882  data: 0.0033
Test:  [ 630/3316]  eta: 23:10:17  loss: 7.2673 (7.2369)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3345  data: 0.0035
Test:  [ 640/3316]  eta: 22:45:42  loss: 7.1681 (7.2366)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4048  data: 0.0035
Test:  [ 630/3316]  eta: 23:14:51  loss: 7.2673 (7.2369)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2164  data: 0.0044
Test:  [ 640/3316]  eta: 22:50:26  loss: 7.1681 (7.2366)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2128  data: 0.0047
Test:  [ 630/3316]  eta: 23:17:28  loss: 7.2673 (7.2369)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3936  data: 0.0042
Test:  [ 630/3316]  eta: 23:20:02  loss: 7.2673 (7.2369)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2168  data: 0.0044
Test:  [ 630/3316]  eta: 23:22:57  loss: 7.2673 (7.2369)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2080  data: 0.0043
Test:  [ 640/3316]  eta: 22:57:14  loss: 7.1681 (7.2366)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3635  data: 0.0043
Test:  [ 640/3316]  eta: 23:03:38  loss: 7.1681 (7.2366)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2359  data: 0.0041
Test:  [ 650/3316]  eta: 22:39:27  loss: 7.2437 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2786  data: 0.0040
Test:  [ 640/3316]  eta: 23:08:07  loss: 7.1681 (7.2366)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1973  data: 0.0046
Test:  [ 650/3316]  eta: 22:44:03  loss: 7.2437 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1686  data: 0.0047
Test:  [ 640/3316]  eta: 23:10:44  loss: 7.1681 (7.2366)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3182  data: 0.0044
Test:  [ 640/3316]  eta: 23:13:12  loss: 7.1681 (7.2366)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1926  data: 0.0046
Test:  [ 640/3316]  eta: 23:16:03  loss: 7.1681 (7.2366)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9003  data: 0.0043
Test:  [ 650/3316]  eta: 22:50:46  loss: 7.2437 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9579  data: 0.0041
Test:  [ 650/3316]  eta: 22:57:14  loss: 7.2437 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0751  data: 0.0035
Test:  [ 660/3316]  eta: 22:33:32  loss: 7.2475 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1564  data: 0.0033
Test:  [ 650/3316]  eta: 23:01:52  loss: 7.2437 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2665  data: 0.0031
Test:  [ 660/3316]  eta: 22:38:17  loss: 7.2475 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3025  data: 0.0031
Test:  [ 650/3316]  eta: 23:04:36  loss: 7.2437 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4132  data: 0.0033
Test:  [ 650/3316]  eta: 23:06:55  loss: 7.2437 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3036  data: 0.0032
Test:  [ 650/3316]  eta: 23:09:42  loss: 7.2437 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2885  data: 0.0032
Test:  [ 660/3316]  eta: 22:44:54  loss: 7.2475 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3517  data: 0.0031
Test:  [ 660/3316]  eta: 22:51:43  loss: 7.2475 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8470  data: 0.0034
Test:  [ 670/3316]  eta: 22:28:16  loss: 7.2475 (7.2383)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.8441  data: 0.0036
Test:  [ 660/3316]  eta: 22:56:07  loss: 7.2475 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9100  data: 0.0038
Test:  [ 670/3316]  eta: 22:32:56  loss: 7.2475 (7.2383)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.0171  data: 0.0046
Test:  [ 660/3316]  eta: 22:58:53  loss: 7.2475 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.0913  data: 0.0044
Test:  [ 660/3316]  eta: 23:01:08  loss: 7.2475 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9954  data: 0.0052
Test:  [ 660/3316]  eta: 23:03:47  loss: 7.2475 (7.2377)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9435  data: 0.0047
Test:  [ 670/3316]  eta: 22:39:26  loss: 7.2475 (7.2383)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 30.0499  data: 0.0047
Test:  [ 670/3316]  eta: 22:45:32  loss: 7.2475 (7.2383)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.9569  data: 0.0050
Test:  [ 680/3316]  eta: 22:22:18  loss: 7.3036 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7781  data: 0.0047
Test:  [ 670/3316]  eta: 22:49:39  loss: 7.2475 (7.2383)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6918  data: 0.0044
Test:  [ 680/3316]  eta: 22:26:41  loss: 7.3036 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6231  data: 0.0049
Test:  [ 670/3316]  eta: 22:52:20  loss: 7.2475 (7.2383)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7212  data: 0.0047
Test:  [ 670/3316]  eta: 22:54:23  loss: 7.2475 (7.2383)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5911  data: 0.0053
Test:  [ 670/3316]  eta: 22:56:57  loss: 7.2475 (7.2383)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4985  data: 0.0046
Test:  [ 680/3316]  eta: 22:33:02  loss: 7.3036 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6023  data: 0.0043
Test:  [ 680/3316]  eta: 22:39:03  loss: 7.3036 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1699  data: 0.0046
Test:  [ 690/3316]  eta: 22:16:09  loss: 7.2945 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0534  data: 0.0038
Test:  [ 680/3316]  eta: 22:43:03  loss: 7.3036 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9859  data: 0.0035
Test:  [ 690/3316]  eta: 22:20:26  loss: 7.2945 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8900  data: 0.0032
Test:  [ 680/3316]  eta: 22:45:45  loss: 7.3036 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0199  data: 0.0033
Test:  [ 680/3316]  eta: 22:47:42  loss: 7.3036 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8517  data: 0.0030
Test:  [ 680/3316]  eta: 22:50:10  loss: 7.3036 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7758  data: 0.0027
Test:  [ 690/3316]  eta: 22:26:39  loss: 7.2945 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8545  data: 0.0026
Test:  [ 690/3316]  eta: 22:32:36  loss: 7.2945 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9069  data: 0.0028
Test:  [ 700/3316]  eta: 22:10:01  loss: 7.2682 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8655  data: 0.0027
Test:  [ 690/3316]  eta: 22:36:45  loss: 7.2945 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0484  data: 0.0026
Test:  [ 700/3316]  eta: 22:14:28  loss: 7.2682 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0370  data: 0.0029
Test:  [ 690/3316]  eta: 22:39:32  loss: 7.2945 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2066  data: 0.0029
Test:  [ 690/3316]  eta: 22:41:20  loss: 7.2945 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0694  data: 0.0029
Test:  [ 690/3316]  eta: 22:43:43  loss: 7.2945 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0015  data: 0.0032
Test:  [ 700/3316]  eta: 22:20:34  loss: 7.2682 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0264  data: 0.0037
Test:  [ 700/3316]  eta: 22:26:28  loss: 7.2682 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1165  data: 0.0045
Test:  [ 710/3316]  eta: 22:04:10  loss: 7.3321 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0415  data: 0.0046
Test:  [ 700/3316]  eta: 22:30:15  loss: 7.2682 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0530  data: 0.0040
Test:  [ 710/3316]  eta: 22:08:18  loss: 7.3321 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0457  data: 0.0038
Test:  [ 700/3316]  eta: 22:33:05  loss: 7.2682 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2349  data: 0.0040
Test:  [ 700/3316]  eta: 22:34:48  loss: 7.2682 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1129  data: 0.0038
Test:  [ 700/3316]  eta: 22:37:05  loss: 7.2682 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0291  data: 0.0040
Test:  [ 710/3316]  eta: 22:14:19  loss: 7.3321 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0630  data: 0.0041
Test:  [ 710/3316]  eta: 22:20:05  loss: 7.3321 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0809  data: 0.0045
Test:  [ 720/3316]  eta: 21:58:07  loss: 7.2838 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0560  data: 0.0044
Test:  [ 710/3316]  eta: 22:23:48  loss: 7.3321 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8455  data: 0.0041
Test:  [ 720/3316]  eta: 22:02:10  loss: 7.2838 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8667  data: 0.0036
Test:  [ 710/3316]  eta: 22:26:39  loss: 7.3321 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9919  data: 0.0038
Test:  [ 710/3316]  eta: 22:28:17  loss: 7.3321 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9058  data: 0.0035
Test:  [ 710/3316]  eta: 22:30:29  loss: 7.3321 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8268  data: 0.0033
Test:  [ 720/3316]  eta: 22:08:04  loss: 7.2838 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8792  data: 0.0029
Test:  [ 720/3316]  eta: 22:13:49  loss: 7.2838 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9150  data: 0.0026
Test:  [ 730/3316]  eta: 21:52:06  loss: 7.2838 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8815  data: 0.0027
Test:  [ 720/3316]  eta: 22:17:26  loss: 7.2838 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8930  data: 0.0027
Test:  [ 730/3316]  eta: 21:56:10  loss: 7.2838 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9473  data: 0.0027
Test:  [ 720/3316]  eta: 22:20:17  loss: 7.2838 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9899  data: 0.0026
Test:  [ 720/3316]  eta: 22:21:49  loss: 7.2838 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8838  data: 0.0028
Test:  [ 720/3316]  eta: 22:24:19  loss: 7.2838 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1310  data: 0.0027
Test:  [ 730/3316]  eta: 22:02:16  loss: 7.2838 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1831  data: 0.0030
Test:  [ 730/3316]  eta: 22:07:57  loss: 7.2838 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2956  data: 0.0034
Test:  [ 740/3316]  eta: 21:46:29  loss: 7.2851 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1870  data: 0.0037
Test:  [ 730/3316]  eta: 22:11:28  loss: 7.2838 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2298  data: 0.0040
Test:  [ 740/3316]  eta: 21:50:34  loss: 7.2851 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3306  data: 0.0043
Test:  [ 730/3316]  eta: 22:14:19  loss: 7.2838 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3154  data: 0.0045
Test:  [ 730/3316]  eta: 22:15:47  loss: 7.2838 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2160  data: 0.0045
Test:  [ 730/3316]  eta: 22:17:52  loss: 7.2838 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1861  data: 0.0039
Test:  [ 740/3316]  eta: 21:56:11  loss: 7.2851 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2685  data: 0.0042
Test:  [ 740/3316]  eta: 22:01:50  loss: 7.2851 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3519  data: 0.0040
Test:  [ 750/3316]  eta: 21:40:37  loss: 7.2404 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2646  data: 0.0040
Test:  [ 740/3316]  eta: 22:05:12  loss: 7.2851 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2441  data: 0.0045
Test:  [ 750/3316]  eta: 21:44:38  loss: 7.2404 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3383  data: 0.0044
Test:  [ 740/3316]  eta: 22:08:07  loss: 7.2851 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3912  data: 0.0047
Test:  [ 740/3316]  eta: 22:09:27  loss: 7.2851 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2593  data: 0.0045
Test:  [ 740/3316]  eta: 22:11:31  loss: 7.2851 (7.2422)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9679  data: 0.0042
Test:  [ 750/3316]  eta: 21:50:07  loss: 7.2404 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0037  data: 0.0039
Test:  [ 750/3316]  eta: 21:55:41  loss: 7.2404 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0560  data: 0.0033
Test:  [ 760/3316]  eta: 21:34:47  loss: 7.2283 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0485  data: 0.0032
Test:  [ 750/3316]  eta: 21:58:56  loss: 7.2404 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9317  data: 0.0032
Test:  [ 760/3316]  eta: 21:38:40  loss: 7.2283 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9947  data: 0.0030
Test:  [ 750/3316]  eta: 22:01:55  loss: 7.2404 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1321  data: 0.0029
Test:  [ 750/3316]  eta: 22:03:08  loss: 7.2404 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9559  data: 0.0029
Test:  [ 750/3316]  eta: 22:05:10  loss: 7.2404 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9669  data: 0.0029
Test:  [ 760/3316]  eta: 21:44:04  loss: 7.2283 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9591  data: 0.0029
Test:  [ 760/3316]  eta: 21:49:52  loss: 7.2283 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.2639  data: 0.0033
Test:  [ 770/3316]  eta: 21:29:16  loss: 7.1891 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3139  data: 0.0032
Test:  [ 760/3316]  eta: 21:53:10  loss: 7.2283 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3076  data: 0.0040
Test:  [ 770/3316]  eta: 21:33:16  loss: 7.1891 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4215  data: 0.0039
Test:  [ 760/3316]  eta: 21:56:14  loss: 7.2283 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5225  data: 0.0043
Test:  [ 760/3316]  eta: 21:57:23  loss: 7.2283 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4053  data: 0.0040
Test:  [ 760/3316]  eta: 21:59:22  loss: 7.2283 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3875  data: 0.0044
Test:  [ 770/3316]  eta: 21:38:33  loss: 7.1891 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3909  data: 0.0044
Test:  [ 770/3316]  eta: 21:44:04  loss: 7.1891 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.5314  data: 0.0047
Test:  [ 780/3316]  eta: 21:23:49  loss: 7.1867 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6232  data: 0.0044
Test:  [ 770/3316]  eta: 21:47:19  loss: 7.1891 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.6226  data: 0.0050
Test:  [ 780/3316]  eta: 21:27:43  loss: 7.1867 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7531  data: 0.0047
Test:  [ 770/3316]  eta: 21:50:20  loss: 7.1891 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7535  data: 0.0050
Test:  [ 770/3316]  eta: 21:51:28  loss: 7.1891 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7091  data: 0.0048
Test:  [ 770/3316]  eta: 21:53:29  loss: 7.1891 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7548  data: 0.0051
Test:  [ 780/3316]  eta: 21:32:54  loss: 7.1867 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.7220  data: 0.0048
Test:  [ 780/3316]  eta: 21:38:10  loss: 7.1867 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.4181  data: 0.0045
Test:  [ 790/3316]  eta: 21:18:04  loss: 7.2586 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.3719  data: 0.0042
Test:  [ 780/3316]  eta: 21:41:04  loss: 7.1867 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1336  data: 0.0040
Test:  [ 790/3316]  eta: 21:21:38  loss: 7.2586 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0980  data: 0.0038
Test:  [ 780/3316]  eta: 21:43:54  loss: 7.1867 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0233  data: 0.0033
Test:  [ 780/3316]  eta: 21:45:00  loss: 7.1867 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0085  data: 0.0034
Test:  [ 780/3316]  eta: 21:47:00  loss: 7.1867 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0942  data: 0.0033
Test:  [ 790/3316]  eta: 21:26:35  loss: 7.2586 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9508  data: 0.0031
Test:  [ 790/3316]  eta: 21:31:41  loss: 7.2586 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7083  data: 0.0028
Test:  [ 800/3316]  eta: 21:11:50  loss: 7.2602 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5999  data: 0.0027
Test:  [ 790/3316]  eta: 21:34:29  loss: 7.2586 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4010  data: 0.0025
Test:  [ 800/3316]  eta: 21:15:35  loss: 7.2602 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5762  data: 0.0028
Test:  [ 790/3316]  eta: 21:37:33  loss: 7.2586 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5350  data: 0.0027
Test:  [ 790/3316]  eta: 21:38:37  loss: 7.2586 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5120  data: 0.0027
Test:  [ 790/3316]  eta: 21:40:37  loss: 7.2586 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5609  data: 0.0031
Test:  [ 800/3316]  eta: 21:20:31  loss: 7.2602 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5008  data: 0.0032
Test:  [ 800/3316]  eta: 21:25:28  loss: 7.2602 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3518  data: 0.0033
Test:  [ 810/3316]  eta: 21:05:52  loss: 7.2376 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3330  data: 0.0038
Test:  [ 800/3316]  eta: 21:28:16  loss: 7.2602 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3601  data: 0.0038
Test:  [ 810/3316]  eta: 21:09:19  loss: 7.2376 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3351  data: 0.0036
Test:  [ 800/3316]  eta: 21:31:00  loss: 7.2602 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3505  data: 0.0038
Test:  [ 800/3316]  eta: 21:32:05  loss: 7.2602 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3703  data: 0.0036
Test:  [ 800/3316]  eta: 21:34:03  loss: 7.2602 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3876  data: 0.0038
Test:  [ 810/3316]  eta: 21:14:11  loss: 7.2376 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4052  data: 0.0037
Test:  [ 810/3316]  eta: 21:19:02  loss: 7.2376 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3383  data: 0.0035
Test:  [ 820/3316]  eta: 20:59:41  loss: 7.2504 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3177  data: 0.0038
Test:  [ 810/3316]  eta: 21:21:53  loss: 7.2376 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4693  data: 0.0039
Test:  [ 820/3316]  eta: 21:03:06  loss: 7.2504 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1160  data: 0.0036
Test:  [ 810/3316]  eta: 21:24:31  loss: 7.2376 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1465  data: 0.0034
Test:  [ 810/3316]  eta: 21:25:38  loss: 7.2376 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2188  data: 0.0034
Test:  [ 810/3316]  eta: 21:27:30  loss: 7.2376 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1658  data: 0.0031
Test:  [ 820/3316]  eta: 21:07:51  loss: 7.2504 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0831  data: 0.0031
Test:  [ 820/3316]  eta: 21:12:40  loss: 7.2504 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1114  data: 0.0025
Test:  [ 830/3316]  eta: 20:53:33  loss: 7.2635 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0895  data: 0.0024
Test:  [ 820/3316]  eta: 21:15:27  loss: 7.2504 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1892  data: 0.0024
Test:  [ 830/3316]  eta: 20:56:56  loss: 7.2635 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1395  data: 0.0025
Test:  [ 820/3316]  eta: 21:18:06  loss: 7.2504 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1820  data: 0.0023
Test:  [ 820/3316]  eta: 21:19:10  loss: 7.2504 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2020  data: 0.0025
Test:  [ 820/3316]  eta: 21:21:01  loss: 7.2504 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1600  data: 0.0024
Test:  [ 830/3316]  eta: 21:01:35  loss: 7.2635 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0784  data: 0.0024
Test:  [ 830/3316]  eta: 21:06:33  loss: 7.2635 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3343  data: 0.0030
Test:  [ 840/3316]  eta: 20:47:39  loss: 7.2181 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3014  data: 0.0028
Test:  [ 830/3316]  eta: 21:09:20  loss: 7.2635 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3592  data: 0.0037
Test:  [ 840/3316]  eta: 20:51:02  loss: 7.2181 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3755  data: 0.0032
Test:  [ 830/3316]  eta: 21:11:55  loss: 7.2635 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3897  data: 0.0034
Test:  [ 830/3316]  eta: 21:12:59  loss: 7.2635 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3977  data: 0.0034
Test:  [ 830/3316]  eta: 21:14:48  loss: 7.2635 (7.2427)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3858  data: 0.0041
Test:  [ 840/3316]  eta: 20:55:36  loss: 7.2181 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3512  data: 0.0038
Test:  [ 840/3316]  eta: 21:00:15  loss: 7.2181 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3313  data: 0.0037
Test:  [ 850/3316]  eta: 20:41:36  loss: 7.2314 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3297  data: 0.0037
Test:  [ 840/3316]  eta: 21:02:58  loss: 7.2181 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3513  data: 0.0041
Test:  [ 850/3316]  eta: 20:44:54  loss: 7.2314 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3380  data: 0.0035
Test:  [ 840/3316]  eta: 21:05:35  loss: 7.2181 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4114  data: 0.0035
Test:  [ 840/3316]  eta: 21:06:34  loss: 7.2181 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3560  data: 0.0034
Test:  [ 840/3316]  eta: 21:08:24  loss: 7.2181 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3945  data: 0.0040
Test:  [ 850/3316]  eta: 20:49:26  loss: 7.2314 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3815  data: 0.0039
Test:  [ 850/3316]  eta: 20:53:59  loss: 7.2314 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1178  data: 0.0031
Test:  [ 860/3316]  eta: 20:35:33  loss: 7.2503 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1071  data: 0.0032
Test:  [ 850/3316]  eta: 20:56:40  loss: 7.2314 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1007  data: 0.0028
Test:  [ 860/3316]  eta: 20:38:48  loss: 7.2503 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0800  data: 0.0027
Test:  [ 850/3316]  eta: 20:59:16  loss: 7.2314 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1974  data: 0.0024
Test:  [ 850/3316]  eta: 21:00:11  loss: 7.2314 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0667  data: 0.0025
Test:  [ 850/3316]  eta: 21:02:02  loss: 7.2314 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1749  data: 0.0024
Test:  [ 860/3316]  eta: 20:43:19  loss: 7.2503 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1705  data: 0.0024
Test:  [ 860/3316]  eta: 20:47:47  loss: 7.2503 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1415  data: 0.0023
Test:  [ 870/3316]  eta: 20:29:34  loss: 7.2316 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1062  data: 0.0023
Test:  [ 860/3316]  eta: 20:50:35  loss: 7.2503 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3053  data: 0.0024
Test:  [ 870/3316]  eta: 20:32:54  loss: 7.2316 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2628  data: 0.0025
Test:  [ 860/3316]  eta: 20:53:10  loss: 7.2503 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3476  data: 0.0026
Test:  [ 860/3316]  eta: 20:54:01  loss: 7.2503 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2563  data: 0.0028
Test:  [ 860/3316]  eta: 20:55:54  loss: 7.2503 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3692  data: 0.0032
Test:  [ 870/3316]  eta: 20:37:22  loss: 7.2316 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3252  data: 0.0034
Test:  [ 870/3316]  eta: 20:41:45  loss: 7.2316 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3106  data: 0.0034
Test:  [ 880/3316]  eta: 20:23:48  loss: 7.2452 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3535  data: 0.0036
Test:  [ 870/3316]  eta: 20:44:23  loss: 7.2316 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3507  data: 0.0034
Test:  [ 880/3316]  eta: 20:26:54  loss: 7.2452 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2988  data: 0.0034
Test:  [ 870/3316]  eta: 20:46:56  loss: 7.2316 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3702  data: 0.0034
Test:  [ 870/3316]  eta: 20:47:44  loss: 7.2316 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2779  data: 0.0034
Test:  [ 870/3316]  eta: 20:49:38  loss: 7.2316 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3866  data: 0.0038
Test:  [ 880/3316]  eta: 20:31:15  loss: 7.2452 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2743  data: 0.0039
Test:  [ 880/3316]  eta: 20:35:35  loss: 7.2452 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2674  data: 0.0034
Test:  [ 890/3316]  eta: 20:17:54  loss: 7.2489 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3881  data: 0.0038
Test:  [ 880/3316]  eta: 20:38:12  loss: 7.2452 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1705  data: 0.0036
Test:  [ 890/3316]  eta: 20:20:53  loss: 7.2489 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1094  data: 0.0033
Test:  [ 880/3316]  eta: 20:40:42  loss: 7.2452 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1772  data: 0.0031
Test:  [ 880/3316]  eta: 20:41:27  loss: 7.2452 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0750  data: 0.0030
Test:  [ 880/3316]  eta: 20:43:21  loss: 7.2452 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1633  data: 0.0029
Test:  [ 890/3316]  eta: 20:25:11  loss: 7.2489 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0710  data: 0.0030
Test:  [ 890/3316]  eta: 20:29:33  loss: 7.2489 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2023  data: 0.0024
Test:  [ 900/3316]  eta: 20:12:04  loss: 7.2489 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2588  data: 0.0025
Test:  [ 890/3316]  eta: 20:32:10  loss: 7.2489 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2674  data: 0.0025
Test:  [ 900/3316]  eta: 20:15:00  loss: 7.2489 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1719  data: 0.0025
Test:  [ 890/3316]  eta: 20:34:38  loss: 7.2489 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2817  data: 0.0023
Test:  [ 890/3316]  eta: 20:35:19  loss: 7.2489 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1675  data: 0.0026
Test:  [ 890/3316]  eta: 20:37:15  loss: 7.2489 (7.2429)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2748  data: 0.0024
Test:  [ 900/3316]  eta: 20:19:15  loss: 7.2489 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2013  data: 0.0028
Test:  [ 900/3316]  eta: 20:23:27  loss: 7.2489 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2056  data: 0.0024
Test:  [ 910/3316]  eta: 20:06:10  loss: 7.2406 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1984  data: 0.0023
Test:  [ 900/3316]  eta: 20:26:02  loss: 7.2489 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2539  data: 0.0024
Test:  [ 910/3316]  eta: 20:09:04  loss: 7.2406 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1942  data: 0.0023
Test:  [ 900/3316]  eta: 20:28:29  loss: 7.2489 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2701  data: 0.0024
Test:  [ 900/3316]  eta: 20:29:12  loss: 7.2489 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2832  data: 0.0026
Test:  [ 900/3316]  eta: 20:31:13  loss: 7.2489 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4625  data: 0.0026
Test:  [ 910/3316]  eta: 20:13:23  loss: 7.2406 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3811  data: 0.0029
Test:  [ 910/3316]  eta: 20:17:30  loss: 7.2406 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2295  data: 0.0023
Test:  [ 920/3316]  eta: 20:00:26  loss: 7.2595 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2452  data: 0.0026
Test:  [ 910/3316]  eta: 20:20:02  loss: 7.2406 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2257  data: 0.0026
Test:  [ 920/3316]  eta: 20:03:15  loss: 7.2595 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2308  data: 0.0024
Test:  [ 910/3316]  eta: 20:22:27  loss: 7.2406 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2493  data: 0.0025
Test:  [ 910/3316]  eta: 20:23:02  loss: 7.2406 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1736  data: 0.0025
Test:  [ 910/3316]  eta: 20:25:03  loss: 7.2406 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3326  data: 0.0028
Test:  [ 920/3316]  eta: 20:07:26  loss: 7.2595 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2878  data: 0.0027
Test:  [ 920/3316]  eta: 20:11:28  loss: 7.2595 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2404  data: 0.0024
Test:  [ 930/3316]  eta: 19:54:36  loss: 7.2595 (7.2441)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2805  data: 0.0027
Test:  [ 920/3316]  eta: 20:13:58  loss: 7.2595 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2330  data: 0.0026
Test:  [ 930/3316]  eta: 19:57:21  loss: 7.2595 (7.2441)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1999  data: 0.0024
Test:  [ 920/3316]  eta: 20:16:21  loss: 7.2595 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2522  data: 0.0024
Test:  [ 920/3316]  eta: 20:16:52  loss: 7.2595 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0396  data: 0.0024
Test:  [ 920/3316]  eta: 20:18:56  loss: 7.2595 (7.2440)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1736  data: 0.0026
Test:  [ 930/3316]  eta: 20:01:31  loss: 7.2595 (7.2441)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1676  data: 0.0025
Test:  [ 930/3316]  eta: 20:05:27  loss: 7.2595 (7.2441)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1102  data: 0.0024
Test:  [ 940/3316]  eta: 19:48:59  loss: 7.1925 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3603  data: 0.0023
Test:  [ 930/3316]  eta: 20:08:07  loss: 7.2595 (7.2441)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3473  data: 0.0024
Test:  [ 940/3316]  eta: 19:51:40  loss: 7.1925 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2927  data: 0.0029
Test:  [ 930/3316]  eta: 20:10:31  loss: 7.2595 (7.2441)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4155  data: 0.0027
Test:  [ 930/3316]  eta: 20:10:59  loss: 7.2595 (7.2441)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2865  data: 0.0030
Test:  [ 930/3316]  eta: 20:13:04  loss: 7.2595 (7.2441)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4314  data: 0.0031
Test:  [ 940/3316]  eta: 19:55:49  loss: 7.1925 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4174  data: 0.0035
Test:  [ 940/3316]  eta: 19:59:41  loss: 7.1925 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3602  data: 0.0037
Test:  [ 950/3316]  eta: 19:43:11  loss: 7.1925 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3401  data: 0.0041
Test:  [ 940/3316]  eta: 20:02:05  loss: 7.1925 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3297  data: 0.0033
Test:  [ 950/3316]  eta: 19:45:50  loss: 7.1925 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3264  data: 0.0038
Test:  [ 940/3316]  eta: 20:04:29  loss: 7.1925 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4311  data: 0.0035
Test:  [ 940/3316]  eta: 20:04:54  loss: 7.1925 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3198  data: 0.0037
Test:  [ 940/3316]  eta: 20:06:59  loss: 7.1925 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4094  data: 0.0035
Test:  [ 950/3316]  eta: 19:49:55  loss: 7.1925 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3707  data: 0.0040
Test:  [ 950/3316]  eta: 19:53:44  loss: 7.1925 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3677  data: 0.0039
Test:  [ 960/3316]  eta: 19:37:25  loss: 7.2516 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1187  data: 0.0042
Test:  [ 950/3316]  eta: 19:56:07  loss: 7.1925 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1348  data: 0.0033
Test:  [ 960/3316]  eta: 19:40:03  loss: 7.2516 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1470  data: 0.0032
Test:  [ 950/3316]  eta: 19:58:30  loss: 7.1925 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1859  data: 0.0031
Test:  [ 950/3316]  eta: 19:58:55  loss: 7.1925 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1524  data: 0.0029
Test:  [ 950/3316]  eta: 20:00:57  loss: 7.1925 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1576  data: 0.0027
Test:  [ 960/3316]  eta: 19:44:03  loss: 7.2516 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1061  data: 0.0027
Test:  [ 960/3316]  eta: 19:47:49  loss: 7.2516 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1325  data: 0.0025
Test:  [ 970/3316]  eta: 19:31:42  loss: 7.2275 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1694  data: 0.0024
Test:  [ 960/3316]  eta: 19:50:09  loss: 7.2516 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1325  data: 0.0025
Test:  [ 970/3316]  eta: 19:34:14  loss: 7.2275 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1193  data: 0.0024
Test:  [ 960/3316]  eta: 19:52:31  loss: 7.2516 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1710  data: 0.0023
Test:  [ 960/3316]  eta: 19:52:56  loss: 7.2516 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1917  data: 0.0023
Test:  [ 960/3316]  eta: 19:55:04  loss: 7.2516 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3303  data: 0.0026
Test:  [ 970/3316]  eta: 19:38:21  loss: 7.2275 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3006  data: 0.0031
Test:  [ 970/3316]  eta: 19:42:05  loss: 7.2275 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3242  data: 0.0035
Test:  [ 980/3316]  eta: 19:26:08  loss: 7.2032 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3747  data: 0.0034
Test:  [ 970/3316]  eta: 19:44:23  loss: 7.2275 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3027  data: 0.0036
Test:  [ 980/3316]  eta: 19:28:36  loss: 7.2032 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2564  data: 0.0033
Test:  [ 970/3316]  eta: 19:46:42  loss: 7.2275 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3095  data: 0.0036
Test:  [ 970/3316]  eta: 19:47:08  loss: 7.2275 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3464  data: 0.0037
Test:  [ 970/3316]  eta: 19:49:06  loss: 7.2275 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3439  data: 0.0035
Test:  [ 980/3316]  eta: 19:32:31  loss: 7.2032 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3022  data: 0.0041
Test:  [ 980/3316]  eta: 19:36:12  loss: 7.2032 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3101  data: 0.0038
Test:  [ 990/3316]  eta: 19:20:28  loss: 7.2738 (7.2442)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3747  data: 0.0037
Test:  [ 980/3316]  eta: 19:38:27  loss: 7.2032 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3057  data: 0.0038
Test:  [ 990/3316]  eta: 19:22:50  loss: 7.2738 (7.2442)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2517  data: 0.0033
Test:  [ 980/3316]  eta: 19:40:44  loss: 7.2032 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2879  data: 0.0037
Test:  [ 980/3316]  eta: 19:41:09  loss: 7.2032 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3003  data: 0.0037
Test:  [ 980/3316]  eta: 19:43:04  loss: 7.2032 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1057  data: 0.0033
Test:  [ 990/3316]  eta: 19:26:40  loss: 7.2738 (7.2442)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0470  data: 0.0032
Test:  [ 990/3316]  eta: 19:30:17  loss: 7.2738 (7.2442)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0375  data: 0.0026
Test:  [1000/3316]  eta: 19:14:49  loss: 7.1550 (7.2431)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2254  data: 0.0026
Test:  [ 990/3316]  eta: 19:32:38  loss: 7.2738 (7.2442)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1759  data: 0.0027
Test:  [1000/3316]  eta: 19:17:08  loss: 7.1550 (7.2431)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1082  data: 0.0024
Test:  [ 990/3316]  eta: 19:34:51  loss: 7.2738 (7.2442)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1441  data: 0.0023
Test:  [ 990/3316]  eta: 19:35:16  loss: 7.2738 (7.2442)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1304  data: 0.0025
Test:  [ 990/3316]  eta: 19:37:09  loss: 7.2738 (7.2442)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0937  data: 0.0026
Test:  [1000/3316]  eta: 19:20:56  loss: 7.1550 (7.2431)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0955  data: 0.0023
Test:  [1000/3316]  eta: 19:24:29  loss: 7.1550 (7.2431)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0738  data: 0.0028
Test:  [1010/3316]  eta: 19:09:05  loss: 7.1767 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0978  data: 0.0027
Test:  [1000/3316]  eta: 19:26:42  loss: 7.1550 (7.2431)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1070  data: 0.0028
Test:  [1010/3316]  eta: 19:11:21  loss: 7.1767 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0442  data: 0.0026
Test:  [1000/3316]  eta: 19:28:54  loss: 7.1550 (7.2431)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0904  data: 0.0024
Test:  [1000/3316]  eta: 19:29:17  loss: 7.1550 (7.2431)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0458  data: 0.0025
Test:  [1000/3316]  eta: 19:31:11  loss: 7.1550 (7.2431)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1047  data: 0.0027
Test:  [1010/3316]  eta: 19:15:09  loss: 7.1767 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1442  data: 0.0024
Test:  [1010/3316]  eta: 19:18:36  loss: 7.1767 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0544  data: 0.0029
Test:  [1020/3316]  eta: 19:03:21  loss: 7.2597 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9257  data: 0.0027
Test:  [1010/3316]  eta: 19:20:47  loss: 7.1767 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9362  data: 0.0027
Test:  [1020/3316]  eta: 19:05:36  loss: 7.2597 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9267  data: 0.0025
Test:  [1010/3316]  eta: 19:22:58  loss: 7.1767 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9651  data: 0.0024
Test:  [1010/3316]  eta: 19:23:19  loss: 7.1767 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8883  data: 0.0024
Test:  [1010/3316]  eta: 19:25:15  loss: 7.1767 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0364  data: 0.0025
Test:  [1020/3316]  eta: 19:09:28  loss: 7.2597 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1579  data: 0.0022
Test:  [1020/3316]  eta: 19:12:53  loss: 7.2597 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1160  data: 0.0030
Test:  [1030/3316]  eta: 18:57:48  loss: 7.1772 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1150  data: 0.0033
Test:  [1020/3316]  eta: 19:15:06  loss: 7.2597 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1812  data: 0.0034
Test:  [1030/3316]  eta: 19:00:01  loss: 7.1772 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1419  data: 0.0030
Test:  [1020/3316]  eta: 19:17:12  loss: 7.2597 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1458  data: 0.0035
Test:  [1020/3316]  eta: 19:17:33  loss: 7.2597 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1109  data: 0.0035
Test:  [1020/3316]  eta: 19:19:28  loss: 7.2597 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1994  data: 0.0037
Test:  [1030/3316]  eta: 19:03:41  loss: 7.1772 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0883  data: 0.0032
Test:  [1030/3316]  eta: 19:07:04  loss: 7.1772 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1458  data: 0.0037
Test:  [1040/3316]  eta: 18:52:08  loss: 7.2451 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1375  data: 0.0040
Test:  [1030/3316]  eta: 19:09:15  loss: 7.1772 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2188  data: 0.0039
Test:  [1040/3316]  eta: 18:54:20  loss: 7.2451 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1677  data: 0.0035
Test:  [1030/3316]  eta: 19:11:20  loss: 7.1772 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1696  data: 0.0038
Test:  [1030/3316]  eta: 19:11:39  loss: 7.1772 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1352  data: 0.0034
Test:  [1030/3316]  eta: 19:13:34  loss: 7.1772 (7.2428)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1853  data: 0.0036
Test:  [1040/3316]  eta: 18:57:54  loss: 7.2451 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9101  data: 0.0033
Test:  [1040/3316]  eta: 19:01:16  loss: 7.2451 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9754  data: 0.0030
Test:  [1050/3316]  eta: 18:46:28  loss: 7.2518 (7.2436)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9448  data: 0.0029
Test:  [1040/3316]  eta: 19:03:26  loss: 7.2451 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9844  data: 0.0028
Test:  [1050/3316]  eta: 18:48:43  loss: 7.2518 (7.2436)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0700  data: 0.0027
Test:  [1040/3316]  eta: 19:05:34  loss: 7.2451 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1076  data: 0.0025
Test:  [1040/3316]  eta: 19:05:50  loss: 7.2451 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0122  data: 0.0023
Test:  [1040/3316]  eta: 19:07:45  loss: 7.2451 (7.2430)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0901  data: 0.0023
Test:  [1050/3316]  eta: 18:52:14  loss: 7.2518 (7.2436)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0123  data: 0.0022
Test:  [1050/3316]  eta: 18:55:34  loss: 7.2518 (7.2436)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0746  data: 0.0023
Test:  [1060/3316]  eta: 18:40:56  loss: 7.2582 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0872  data: 0.0025
Test:  [1050/3316]  eta: 18:57:41  loss: 7.2518 (7.2436)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0642  data: 0.0023
Test:  [1060/3316]  eta: 18:43:05  loss: 7.2582 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0929  data: 0.0023
Test:  [1050/3316]  eta: 18:59:45  loss: 7.2518 (7.2436)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1066  data: 0.0023
Test:  [1050/3316]  eta: 19:00:00  loss: 7.2518 (7.2436)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0379  data: 0.0023
Test:  [1050/3316]  eta: 19:01:56  loss: 7.2518 (7.2436)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1211  data: 0.0022
Test:  [1060/3316]  eta: 18:46:30  loss: 7.2582 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0309  data: 0.0024
Test:  [1060/3316]  eta: 18:49:55  loss: 7.2582 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2173  data: 0.0023
Test:  [1070/3316]  eta: 18:35:26  loss: 7.2432 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2494  data: 0.0029
Test:  [1060/3316]  eta: 18:52:00  loss: 7.2582 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1927  data: 0.0026
Test:  [1070/3316]  eta: 18:37:33  loss: 7.2432 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1699  data: 0.0023
Test:  [1060/3316]  eta: 18:54:02  loss: 7.2582 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1282  data: 0.0027
Test:  [1060/3316]  eta: 18:54:15  loss: 7.2582 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0745  data: 0.0023
Test:  [1060/3316]  eta: 18:56:12  loss: 7.2582 (7.2432)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1934  data: 0.0027
Test:  [1070/3316]  eta: 18:40:53  loss: 7.2432 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0531  data: 0.0028
Test:  [1070/3316]  eta: 18:44:11  loss: 7.2432 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0998  data: 0.0029
Test:  [1080/3316]  eta: 18:29:50  loss: 7.2432 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1329  data: 0.0030
Test:  [1070/3316]  eta: 18:46:14  loss: 7.2432 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1032  data: 0.0028
Test:  [1080/3316]  eta: 18:31:58  loss: 7.2432 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1948  data: 0.0025
Test:  [1070/3316]  eta: 18:48:14  loss: 7.2432 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1037  data: 0.0029
Test:  [1070/3316]  eta: 18:48:25  loss: 7.2432 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0232  data: 0.0026
Test:  [1070/3316]  eta: 18:50:23  loss: 7.2432 (7.2438)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1397  data: 0.0028
Test:  [1080/3316]  eta: 18:35:11  loss: 7.2432 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0323  data: 0.0029
Test:  [1080/3316]  eta: 18:38:27  loss: 7.2432 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9466  data: 0.0031
Test:  [1090/3316]  eta: 18:24:16  loss: 7.2037 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9907  data: 0.0027
Test:  [1080/3316]  eta: 18:40:29  loss: 7.2432 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9545  data: 0.0026
Test:  [1090/3316]  eta: 18:26:21  loss: 7.2037 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0078  data: 0.0024
Test:  [1080/3316]  eta: 18:42:28  loss: 7.2432 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9692  data: 0.0026
Test:  [1080/3316]  eta: 18:42:37  loss: 7.2432 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8812  data: 0.0027
Test:  [1080/3316]  eta: 18:44:43  loss: 7.2432 (7.2439)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1639  data: 0.0025
Test:  [1090/3316]  eta: 18:29:41  loss: 7.2037 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1428  data: 0.0025
Test:  [1090/3316]  eta: 18:32:53  loss: 7.2037 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1427  data: 0.0034
Test:  [1100/3316]  eta: 18:18:51  loss: 7.2050 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2048  data: 0.0034
Test:  [1090/3316]  eta: 18:34:53  loss: 7.2037 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1334  data: 0.0035
Test:  [1100/3316]  eta: 18:20:53  loss: 7.2050 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1385  data: 0.0032
Test:  [1090/3316]  eta: 18:36:52  loss: 7.2037 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1839  data: 0.0039
Test:  [1090/3316]  eta: 18:36:59  loss: 7.2037 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0932  data: 0.0035
Test:  [1090/3316]  eta: 18:38:57  loss: 7.2037 (7.2435)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1809  data: 0.0039
Test:  [1100/3316]  eta: 18:24:05  loss: 7.2050 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2509  data: 0.0033
Test:  [1100/3316]  eta: 18:27:11  loss: 7.2050 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1376  data: 0.0039
Test:  [1110/3316]  eta: 18:13:18  loss: 7.1857 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1890  data: 0.0038
Test:  [1100/3316]  eta: 18:29:11  loss: 7.2050 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1706  data: 0.0038
Test:  [1110/3316]  eta: 18:15:18  loss: 7.1857 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1636  data: 0.0034
Test:  [1100/3316]  eta: 18:31:08  loss: 7.2050 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1752  data: 0.0038
Test:  [1100/3316]  eta: 18:31:13  loss: 7.2050 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1043  data: 0.0034
Test:  [1100/3316]  eta: 18:33:11  loss: 7.2050 (7.2434)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9944  data: 0.0038
Test:  [1110/3316]  eta: 18:18:30  loss: 7.1857 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0736  data: 0.0033
Test:  [1110/3316]  eta: 18:21:31  loss: 7.1857 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9358  data: 0.0029
Test:  [1120/3316]  eta: 18:07:47  loss: 7.1857 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9943  data: 0.0026
Test:  [1110/3316]  eta: 18:23:30  loss: 7.1857 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0029  data: 0.0025
Test:  [1120/3316]  eta: 18:09:49  loss: 7.1857 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0761  data: 0.0024
Test:  [1110/3316]  eta: 18:25:29  loss: 7.1857 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0777  data: 0.0022
Test:  [1110/3316]  eta: 18:25:33  loss: 7.1857 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0002  data: 0.0023
Test:  [1110/3316]  eta: 18:27:31  loss: 7.1857 (7.2424)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0859  data: 0.0023
Test:  [1120/3316]  eta: 18:12:59  loss: 7.1857 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1349  data: 0.0023
Test:  [1120/3316]  eta: 18:16:00  loss: 7.1857 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1497  data: 0.0024
Test:  [1130/3316]  eta: 18:02:34  loss: 7.2000 (7.2417)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4536  data: 0.0023
Test:  [1120/3316]  eta: 18:18:06  loss: 7.1857 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3900  data: 0.0026
Test:  [1130/3316]  eta: 18:04:33  loss: 7.2000 (7.2417)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5172  data: 0.0030
Test:  [1120/3316]  eta: 18:20:05  loss: 7.1857 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5202  data: 0.0034
Test:  [1120/3316]  eta: 18:20:07  loss: 7.1857 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4401  data: 0.0030
Test:  [1120/3316]  eta: 18:22:05  loss: 7.1857 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5274  data: 0.0039
Test:  [1130/3316]  eta: 18:07:41  loss: 7.2000 (7.2417)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5620  data: 0.0034
Test:  [1130/3316]  eta: 18:10:40  loss: 7.2000 (7.2417)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6173  data: 0.0041
Test:  [1140/3316]  eta: 17:57:10  loss: 7.2213 (7.2419)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6092  data: 0.0038
Test:  [1130/3316]  eta: 18:12:34  loss: 7.2000 (7.2417)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5629  data: 0.0038
Test:  [1140/3316]  eta: 17:59:08  loss: 7.2213 (7.2419)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5942  data: 0.0039
Test:  [1130/3316]  eta: 18:14:31  loss: 7.2000 (7.2417)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5156  data: 0.0036
Test:  [1130/3316]  eta: 18:14:32  loss: 7.2000 (7.2417)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6294  data: 0.0042
Test:  [1130/3316]  eta: 18:16:31  loss: 7.2000 (7.2417)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6435  data: 0.0043
Test:  [1140/3316]  eta: 18:02:14  loss: 7.2213 (7.2419)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6120  data: 0.0041
Test:  [1140/3316]  eta: 18:05:10  loss: 7.2213 (7.2419)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5976  data: 0.0041
Test:  [1150/3316]  eta: 17:51:47  loss: 7.2467 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3098  data: 0.0040
Test:  [1140/3316]  eta: 18:07:03  loss: 7.2213 (7.2419)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3431  data: 0.0036
Test:  [1150/3316]  eta: 17:53:43  loss: 7.2467 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3091  data: 0.0034
Test:  [1140/3316]  eta: 18:08:57  loss: 7.2213 (7.2419)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2446  data: 0.0031
Test:  [1140/3316]  eta: 18:09:00  loss: 7.2213 (7.2419)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3848  data: 0.0033
Test:  [1140/3316]  eta: 18:10:59  loss: 7.2213 (7.2419)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4433  data: 0.0030
Test:  [1150/3316]  eta: 17:56:49  loss: 7.2467 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3846  data: 0.0029
Test:  [1150/3316]  eta: 17:59:41  loss: 7.2467 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3324  data: 0.0025
Test:  [1160/3316]  eta: 17:46:25  loss: 7.2467 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3276  data: 0.0025
Test:  [1150/3316]  eta: 18:01:32  loss: 7.2467 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3490  data: 0.0026
Test:  [1160/3316]  eta: 17:48:18  loss: 7.2467 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2774  data: 0.0025
Test:  [1150/3316]  eta: 18:03:23  loss: 7.2467 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2517  data: 0.0027
Test:  [1150/3316]  eta: 18:03:30  loss: 7.2467 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4072  data: 0.0025
Test:  [1150/3316]  eta: 18:05:32  loss: 7.2467 (7.2423)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5729  data: 0.0029
Test:  [1160/3316]  eta: 17:51:28  loss: 7.2467 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5240  data: 0.0030
Test:  [1160/3316]  eta: 17:54:18  loss: 7.2467 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4989  data: 0.0035
Test:  [1170/3316]  eta: 17:41:13  loss: 7.2162 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5971  data: 0.0037
Test:  [1160/3316]  eta: 17:56:10  loss: 7.2467 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5418  data: 0.0036
Test:  [1170/3316]  eta: 17:43:04  loss: 7.2162 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5280  data: 0.0035
Test:  [1160/3316]  eta: 17:58:01  loss: 7.2467 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5295  data: 0.0040
Test:  [1160/3316]  eta: 17:58:08  loss: 7.2467 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6433  data: 0.0033
Test:  [1160/3316]  eta: 18:00:05  loss: 7.2467 (7.2421)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6730  data: 0.0036
Test:  [1170/3316]  eta: 17:46:07  loss: 7.2162 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6005  data: 0.0042
Test:  [1170/3316]  eta: 17:48:59  loss: 7.2162 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7140  data: 0.0042
Test:  [1180/3316]  eta: 17:36:00  loss: 7.1886 (7.2418)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8107  data: 0.0043
Test:  [1170/3316]  eta: 17:50:48  loss: 7.2162 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7214  data: 0.0041
Test:  [1180/3316]  eta: 17:37:50  loss: 7.1886 (7.2418)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7912  data: 0.0040
Test:  [1170/3316]  eta: 17:52:37  loss: 7.2162 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7538  data: 0.0041
Test:  [1170/3316]  eta: 17:52:45  loss: 7.2162 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8060  data: 0.0034
Test:  [1170/3316]  eta: 17:54:42  loss: 7.2162 (7.2420)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7502  data: 0.0037
Test:  [1180/3316]  eta: 17:40:50  loss: 7.1886 (7.2418)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6757  data: 0.0038
Test:  [1180/3316]  eta: 17:43:42  loss: 7.1886 (7.2418)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8212  data: 0.0032
Test:  [1190/3316]  eta: 17:30:48  loss: 7.1927 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7880  data: 0.0031
Test:  [1180/3316]  eta: 17:45:26  loss: 7.1886 (7.2418)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6933  data: 0.0034
Test:  [1190/3316]  eta: 17:32:33  loss: 7.1927 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7143  data: 0.0031
Test:  [1180/3316]  eta: 17:47:11  loss: 7.1886 (7.2418)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6302  data: 0.0029
Test:  [1180/3316]  eta: 17:47:21  loss: 7.1886 (7.2418)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7105  data: 0.0027
Test:  [1180/3316]  eta: 17:49:18  loss: 7.1886 (7.2418)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7745  data: 0.0028
Test:  [1190/3316]  eta: 17:35:33  loss: 7.1927 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7449  data: 0.0031
Test:  [1190/3316]  eta: 17:38:20  loss: 7.1927 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7434  data: 0.0026
Test:  [1200/3316]  eta: 17:25:29  loss: 7.2223 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5749  data: 0.0026
Test:  [1190/3316]  eta: 17:39:58  loss: 7.1927 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5138  data: 0.0029
Test:  [1200/3316]  eta: 17:27:14  loss: 7.2223 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5321  data: 0.0027
Test:  [1190/3316]  eta: 17:41:42  loss: 7.1927 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4607  data: 0.0027
Test:  [1190/3316]  eta: 17:41:54  loss: 7.1927 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5588  data: 0.0026
Test:  [1190/3316]  eta: 17:43:50  loss: 7.1927 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6009  data: 0.0027
Test:  [1200/3316]  eta: 17:30:13  loss: 7.2223 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6377  data: 0.0030
Test:  [1200/3316]  eta: 17:33:07  loss: 7.2223 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8202  data: 0.0036
Test:  [1210/3316]  eta: 17:20:19  loss: 7.2299 (7.2414)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6235  data: 0.0040
Test:  [1200/3316]  eta: 17:34:42  loss: 7.2223 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6403  data: 0.0040
Test:  [1210/3316]  eta: 17:22:04  loss: 7.2299 (7.2414)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6947  data: 0.0035
Test:  [1200/3316]  eta: 17:36:23  loss: 7.2223 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6077  data: 0.0042
Test:  [1200/3316]  eta: 17:36:36  loss: 7.2223 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6979  data: 0.0038
Test:  [1200/3316]  eta: 17:38:31  loss: 7.2223 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7358  data: 0.0038
Test:  [1210/3316]  eta: 17:25:00  loss: 7.2299 (7.2414)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7184  data: 0.0040
Test:  [1210/3316]  eta: 17:27:45  loss: 7.2299 (7.2414)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7928  data: 0.0045
Test:  [1220/3316]  eta: 17:15:04  loss: 7.2299 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7059  data: 0.0048
Test:  [1210/3316]  eta: 17:29:21  loss: 7.2299 (7.2414)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7878  data: 0.0046
Test:  [1220/3316]  eta: 17:16:48  loss: 7.2299 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7834  data: 0.0039
Test:  [1210/3316]  eta: 17:31:00  loss: 7.2299 (7.2414)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7348  data: 0.0046
Test:  [1210/3316]  eta: 17:31:11  loss: 7.2299 (7.2414)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7278  data: 0.0041
Test:  [1210/3316]  eta: 17:33:06  loss: 7.2299 (7.2414)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7706  data: 0.0039
Test:  [1220/3316]  eta: 17:19:43  loss: 7.2299 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7735  data: 0.0040
Test:  [1220/3316]  eta: 17:22:23  loss: 7.2299 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5092  data: 0.0036
Test:  [1230/3316]  eta: 17:09:46  loss: 7.1759 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4570  data: 0.0035
Test:  [1220/3316]  eta: 17:23:55  loss: 7.2299 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5079  data: 0.0031
Test:  [1230/3316]  eta: 17:11:26  loss: 7.1759 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3997  data: 0.0029
Test:  [1220/3316]  eta: 17:25:31  loss: 7.2299 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4108  data: 0.0027
Test:  [1220/3316]  eta: 17:25:42  loss: 7.2299 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3687  data: 0.0026
Test:  [1220/3316]  eta: 17:27:34  loss: 7.2299 (7.2415)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3612  data: 0.0026
Test:  [1230/3316]  eta: 17:14:17  loss: 7.1759 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3668  data: 0.0026
Test:  [1230/3316]  eta: 17:16:55  loss: 7.1759 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2623  data: 0.0024
Test:  [1240/3316]  eta: 17:04:24  loss: 7.1582 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2205  data: 0.0025
Test:  [1230/3316]  eta: 17:18:32  loss: 7.1759 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3899  data: 0.0024
Test:  [1240/3316]  eta: 17:06:08  loss: 7.1582 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2948  data: 0.0033
Test:  [1230/3316]  eta: 17:20:05  loss: 7.1759 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2809  data: 0.0027
Test:  [1230/3316]  eta: 17:20:18  loss: 7.1759 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3418  data: 0.0030
Test:  [1230/3316]  eta: 17:22:07  loss: 7.1759 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2749  data: 0.0031
Test:  [1240/3316]  eta: 17:08:58  loss: 7.1582 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2689  data: 0.0031
Test:  [1240/3316]  eta: 17:11:35  loss: 7.1582 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3123  data: 0.0040
Test:  [1250/3316]  eta: 16:59:09  loss: 7.2317 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2748  data: 0.0034
Test:  [1240/3316]  eta: 17:13:02  loss: 7.1582 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2077  data: 0.0034
Test:  [1250/3316]  eta: 17:00:45  loss: 7.2317 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2244  data: 0.0041
Test:  [1240/3316]  eta: 17:14:34  loss: 7.1582 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1790  data: 0.0034
Test:  [1240/3316]  eta: 17:14:49  loss: 7.1582 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3182  data: 0.0037
Test:  [1240/3316]  eta: 17:16:35  loss: 7.1582 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2202  data: 0.0036
Test:  [1250/3316]  eta: 17:03:35  loss: 7.2317 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3138  data: 0.0033
Test:  [1250/3316]  eta: 17:06:08  loss: 7.2317 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3197  data: 0.0041
Test:  [1260/3316]  eta: 16:53:50  loss: 7.2961 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3357  data: 0.0034
Test:  [1250/3316]  eta: 17:07:34  loss: 7.2317 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0517  data: 0.0033
Test:  [1260/3316]  eta: 16:55:25  loss: 7.2961 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1034  data: 0.0032
Test:  [1250/3316]  eta: 17:09:04  loss: 7.2317 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0178  data: 0.0030
Test:  [1250/3316]  eta: 17:09:21  loss: 7.2317 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1522  data: 0.0030
Test:  [1250/3316]  eta: 17:11:04  loss: 7.2317 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0569  data: 0.0028
Test:  [1260/3316]  eta: 16:58:14  loss: 7.2961 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2175  data: 0.0025
Test:  [1260/3316]  eta: 17:00:44  loss: 7.2961 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1226  data: 0.0023
Test:  [1270/3316]  eta: 16:48:32  loss: 7.3042 (7.2412)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1928  data: 0.0024
Test:  [1260/3316]  eta: 17:02:06  loss: 7.2961 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0750  data: 0.0023
Test:  [1270/3316]  eta: 16:50:03  loss: 7.3042 (7.2412)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1210  data: 0.0024
Test:  [1260/3316]  eta: 17:03:36  loss: 7.2961 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0583  data: 0.0023
Test:  [1260/3316]  eta: 17:03:53  loss: 7.2961 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1369  data: 0.0024
Test:  [1260/3316]  eta: 17:05:40  loss: 7.2961 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2453  data: 0.0023
Test:  [1270/3316]  eta: 16:52:56  loss: 7.3042 (7.2412)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3585  data: 0.0028
Test:  [1270/3316]  eta: 16:55:27  loss: 7.3042 (7.2412)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3864  data: 0.0031
Test:  [1280/3316]  eta: 16:43:21  loss: 7.1922 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4215  data: 0.0032
Test:  [1270/3316]  eta: 16:56:47  loss: 7.3042 (7.2412)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2713  data: 0.0030
Test:  [1280/3316]  eta: 16:44:49  loss: 7.1922 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2931  data: 0.0034
Test:  [1270/3316]  eta: 16:58:14  loss: 7.3042 (7.2412)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2657  data: 0.0035
Test:  [1270/3316]  eta: 16:58:33  loss: 7.3042 (7.2412)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3213  data: 0.0041
Test:  [1270/3316]  eta: 17:00:10  loss: 7.3042 (7.2412)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2353  data: 0.0038
Test:  [1280/3316]  eta: 16:47:33  loss: 7.1922 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2640  data: 0.0040
Test:  [1280/3316]  eta: 16:50:03  loss: 7.1922 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3904  data: 0.0036
Test:  [1290/3316]  eta: 16:38:06  loss: 7.1813 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4801  data: 0.0033
Test:  [1280/3316]  eta: 16:51:23  loss: 7.1922 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3374  data: 0.0033
Test:  [1290/3316]  eta: 16:39:30  loss: 7.1813 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3135  data: 0.0035
Test:  [1280/3316]  eta: 16:52:48  loss: 7.1922 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2943  data: 0.0035
Test:  [1280/3316]  eta: 16:53:09  loss: 7.1922 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3911  data: 0.0041
Test:  [1280/3316]  eta: 16:54:42  loss: 7.1922 (7.2406)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0505  data: 0.0039
Test:  [1290/3316]  eta: 16:42:12  loss: 7.1813 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1128  data: 0.0035
Test:  [1290/3316]  eta: 16:44:42  loss: 7.1813 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2000  data: 0.0027
Test:  [1300/3316]  eta: 16:32:51  loss: 7.1846 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2970  data: 0.0026
Test:  [1290/3316]  eta: 16:45:59  loss: 7.1813 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1734  data: 0.0027
Test:  [1300/3316]  eta: 16:34:11  loss: 7.1846 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1335  data: 0.0025
Test:  [1290/3316]  eta: 16:47:22  loss: 7.1813 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1180  data: 0.0023
Test:  [1290/3316]  eta: 16:47:45  loss: 7.1813 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2848  data: 0.0023
Test:  [1290/3316]  eta: 16:49:15  loss: 7.1813 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0942  data: 0.0023
Test:  [1300/3316]  eta: 16:36:51  loss: 7.1846 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1506  data: 0.0023
Test:  [1300/3316]  eta: 16:39:25  loss: 7.1846 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3752  data: 0.0022
Test:  [1310/3316]  eta: 16:27:41  loss: 7.1391 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4388  data: 0.0027
Test:  [1300/3316]  eta: 16:40:42  loss: 7.1846 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3837  data: 0.0030
Test:  [1310/3316]  eta: 16:28:59  loss: 7.1391 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3137  data: 0.0030
Test:  [1300/3316]  eta: 16:42:02  loss: 7.1846 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2555  data: 0.0026
Test:  [1300/3316]  eta: 16:42:27  loss: 7.1846 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4234  data: 0.0033
Test:  [1300/3316]  eta: 16:43:53  loss: 7.1846 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2552  data: 0.0035
Test:  [1310/3316]  eta: 16:31:38  loss: 7.1391 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3508  data: 0.0035
Test:  [1310/3316]  eta: 16:34:02  loss: 7.1391 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3079  data: 0.0034
Test:  [1320/3316]  eta: 16:22:23  loss: 7.1853 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3499  data: 0.0037
Test:  [1310/3316]  eta: 16:35:18  loss: 7.1391 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3133  data: 0.0037
Test:  [1320/3316]  eta: 16:23:40  loss: 7.1853 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2963  data: 0.0036
Test:  [1310/3316]  eta: 16:36:37  loss: 7.1391 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2384  data: 0.0032
Test:  [1310/3316]  eta: 16:37:03  loss: 7.1391 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3486  data: 0.0040
Test:  [1310/3316]  eta: 16:38:25  loss: 7.1391 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2004  data: 0.0037
Test:  [1320/3316]  eta: 16:26:17  loss: 7.1853 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3161  data: 0.0036
Test:  [1320/3316]  eta: 16:28:40  loss: 7.1853 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0775  data: 0.0034
Test:  [1330/3316]  eta: 16:17:07  loss: 7.2475 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1151  data: 0.0033
Test:  [1320/3316]  eta: 16:29:54  loss: 7.1853 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0499  data: 0.0031
Test:  [1330/3316]  eta: 16:18:23  loss: 7.2475 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1168  data: 0.0031
Test:  [1320/3316]  eta: 16:31:12  loss: 7.1853 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0465  data: 0.0029
Test:  [1320/3316]  eta: 16:31:38  loss: 7.1853 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1088  data: 0.0029
Test:  [1320/3316]  eta: 16:33:00  loss: 7.1853 (7.2392)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0410  data: 0.0025
Test:  [1330/3316]  eta: 16:20:57  loss: 7.2475 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0752  data: 0.0025
Test:  [1330/3316]  eta: 16:23:19  loss: 7.2475 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0976  data: 0.0024
Test:  [1340/3316]  eta: 16:11:54  loss: 7.2184 (7.2389)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2214  data: 0.0024
Test:  [1330/3316]  eta: 16:24:34  loss: 7.2475 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1503  data: 0.0026
Test:  [1340/3316]  eta: 16:13:09  loss: 7.2184 (7.2389)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1932  data: 0.0025
Test:  [1330/3316]  eta: 16:25:51  loss: 7.2475 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1306  data: 0.0025
Test:  [1330/3316]  eta: 16:26:23  loss: 7.2475 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3856  data: 0.0025
Test:  [1330/3316]  eta: 16:27:44  loss: 7.2475 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3905  data: 0.0026
Test:  [1340/3316]  eta: 16:15:48  loss: 7.2184 (7.2389)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4110  data: 0.0028
Test:  [1340/3316]  eta: 16:18:09  loss: 7.2184 (7.2389)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4800  data: 0.0034
Test:  [1350/3316]  eta: 16:06:52  loss: 7.2524 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6783  data: 0.0040
Test:  [1340/3316]  eta: 16:19:23  loss: 7.2184 (7.2389)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5170  data: 0.0042
Test:  [1350/3316]  eta: 16:08:03  loss: 7.2524 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5413  data: 0.0039
Test:  [1340/3316]  eta: 16:20:39  loss: 7.2184 (7.2389)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5348  data: 0.0040
Test:  [1340/3316]  eta: 16:21:05  loss: 7.2184 (7.2389)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5481  data: 0.0045
Test:  [1340/3316]  eta: 16:22:24  loss: 7.2184 (7.2389)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5315  data: 0.0037
Test:  [1350/3316]  eta: 16:10:33  loss: 7.2524 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5460  data: 0.0036
Test:  [1350/3316]  eta: 16:12:50  loss: 7.2524 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5048  data: 0.0037
Test:  [1360/3316]  eta: 16:01:41  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7031  data: 0.0041
Test:  [1350/3316]  eta: 16:14:02  loss: 7.2524 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4820  data: 0.0042
Test:  [1360/3316]  eta: 16:02:47  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4838  data: 0.0039
Test:  [1350/3316]  eta: 16:15:17  loss: 7.2524 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4838  data: 0.0038
Test:  [1350/3316]  eta: 16:15:43  loss: 7.2524 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2615  data: 0.0044
Test:  [1350/3316]  eta: 16:16:59  loss: 7.2524 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2006  data: 0.0033
Test:  [1360/3316]  eta: 16:05:14  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2074  data: 0.0032
Test:  [1360/3316]  eta: 16:07:29  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1158  data: 0.0027
Test:  [1370/3316]  eta: 15:56:27  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2857  data: 0.0024
Test:  [1360/3316]  eta: 16:08:41  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1167  data: 0.0025
Test:  [1370/3316]  eta: 15:57:30  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0861  data: 0.0023
Test:  [1360/3316]  eta: 16:09:54  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0717  data: 0.0022
Test:  [1360/3316]  eta: 16:10:21  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1330  data: 0.0022
Test:  [1360/3316]  eta: 16:11:36  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0382  data: 0.0022
Test:  [1370/3316]  eta: 15:59:57  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0852  data: 0.0023
Test:  [1370/3316]  eta: 16:02:15  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2341  data: 0.0027
Test:  [1380/3316]  eta: 15:51:19  loss: 7.2719 (7.2394)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3601  data: 0.0026
Test:  [1370/3316]  eta: 16:03:26  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2744  data: 0.0031
Test:  [1380/3316]  eta: 15:52:20  loss: 7.2719 (7.2394)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2381  data: 0.0031
Test:  [1370/3316]  eta: 16:04:40  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2778  data: 0.0034
Test:  [1370/3316]  eta: 16:05:08  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3852  data: 0.0029
Test:  [1370/3316]  eta: 16:06:20  loss: 7.2610 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3123  data: 0.0032
Test:  [1380/3316]  eta: 15:54:47  loss: 7.2719 (7.2394)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3509  data: 0.0039
Test:  [1380/3316]  eta: 15:56:57  loss: 7.2719 (7.2394)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3003  data: 0.0037
Test:  [1390/3316]  eta: 15:46:07  loss: 7.2562 (7.2391)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4037  data: 0.0038
Test:  [1380/3316]  eta: 15:58:09  loss: 7.2719 (7.2394)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3711  data: 0.0041
Test:  [1390/3316]  eta: 15:47:08  loss: 7.2562 (7.2391)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3772  data: 0.0038
Test:  [1380/3316]  eta: 15:59:21  loss: 7.2719 (7.2394)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3810  data: 0.0040
Test:  [1380/3316]  eta: 15:59:51  loss: 7.2719 (7.2394)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5050  data: 0.0035
Test:  [1380/3316]  eta: 16:01:01  loss: 7.2719 (7.2394)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4335  data: 0.0033
Test:  [1390/3316]  eta: 15:49:32  loss: 7.2562 (7.2391)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4288  data: 0.0042
Test:  [1390/3316]  eta: 15:51:42  loss: 7.2562 (7.2391)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2641  data: 0.0035
Test:  [1400/3316]  eta: 15:40:57  loss: 7.2474 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3043  data: 0.0035
Test:  [1390/3316]  eta: 15:52:53  loss: 7.2562 (7.2391)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2692  data: 0.0035
Test:  [1400/3316]  eta: 15:41:56  loss: 7.2474 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2899  data: 0.0031
Test:  [1390/3316]  eta: 15:54:01  loss: 7.2562 (7.2391)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1626  data: 0.0030
Test:  [1390/3316]  eta: 15:54:33  loss: 7.2562 (7.2391)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3259  data: 0.0030
Test:  [1390/3316]  eta: 15:55:39  loss: 7.2562 (7.2391)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1501  data: 0.0025
Test:  [1400/3316]  eta: 15:44:15  loss: 7.2474 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1315  data: 0.0026
Test:  [1400/3316]  eta: 15:46:23  loss: 7.2474 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1654  data: 0.0024
Test:  [1410/3316]  eta: 15:35:43  loss: 7.2835 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1811  data: 0.0024
Test:  [1400/3316]  eta: 15:47:32  loss: 7.2474 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0962  data: 0.0025
Test:  [1410/3316]  eta: 15:36:46  loss: 7.2835 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3106  data: 0.0023
Test:  [1400/3316]  eta: 15:48:44  loss: 7.2474 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1914  data: 0.0024
Test:  [1400/3316]  eta: 15:49:16  loss: 7.2474 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3122  data: 0.0025
Test:  [1400/3316]  eta: 15:50:20  loss: 7.2474 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1359  data: 0.0031
Test:  [1410/3316]  eta: 15:39:02  loss: 7.2835 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1529  data: 0.0030
Test:  [1410/3316]  eta: 15:41:09  loss: 7.2835 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1361  data: 0.0037
Test:  [1420/3316]  eta: 15:30:33  loss: 7.3069 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1889  data: 0.0038
Test:  [1410/3316]  eta: 15:42:16  loss: 7.2835 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0851  data: 0.0034
Test:  [1420/3316]  eta: 15:31:30  loss: 7.3069 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1381  data: 0.0037
Test:  [1410/3316]  eta: 15:43:24  loss: 7.2835 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1407  data: 0.0031
Test:  [1410/3316]  eta: 15:43:55  loss: 7.2835 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1535  data: 0.0033
Test:  [1410/3316]  eta: 15:44:58  loss: 7.2835 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1014  data: 0.0038
Test:  [1420/3316]  eta: 15:33:46  loss: 7.3069 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1726  data: 0.0037
Test:  [1420/3316]  eta: 15:35:50  loss: 7.3069 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1175  data: 0.0040
Test:  [1430/3316]  eta: 15:25:19  loss: 7.2903 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1497  data: 0.0039
Test:  [1420/3316]  eta: 15:36:55  loss: 7.3069 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0629  data: 0.0034
Test:  [1430/3316]  eta: 15:26:14  loss: 7.2903 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9091  data: 0.0038
Test:  [1420/3316]  eta: 15:38:03  loss: 7.3069 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9555  data: 0.0032
Test:  [1420/3316]  eta: 15:38:35  loss: 7.3069 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9582  data: 0.0031
Test:  [1420/3316]  eta: 15:39:36  loss: 7.3069 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9341  data: 0.0030
Test:  [1430/3316]  eta: 15:28:31  loss: 7.2903 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0259  data: 0.0030
Test:  [1430/3316]  eta: 15:30:35  loss: 7.2903 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0873  data: 0.0028
Test:  [1440/3316]  eta: 15:20:09  loss: 7.2271 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0973  data: 0.0027
Test:  [1430/3316]  eta: 15:31:39  loss: 7.2903 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0077  data: 0.0025
Test:  [1440/3316]  eta: 15:21:03  loss: 7.2271 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0277  data: 0.0027
Test:  [1430/3316]  eta: 15:32:46  loss: 7.2903 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0346  data: 0.0024
Test:  [1430/3316]  eta: 15:33:19  loss: 7.2903 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1349  data: 0.0023
Test:  [1430/3316]  eta: 15:34:19  loss: 7.2903 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0588  data: 0.0026
Test:  [1440/3316]  eta: 15:23:17  loss: 7.2271 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0888  data: 0.0028
Test:  [1440/3316]  eta: 15:25:18  loss: 7.2271 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1039  data: 0.0030
Test:  [1450/3316]  eta: 15:14:57  loss: 7.2554 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1185  data: 0.0031
Test:  [1440/3316]  eta: 15:26:19  loss: 7.2271 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9938  data: 0.0028
Test:  [1450/3316]  eta: 15:15:48  loss: 7.2554 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0494  data: 0.0030
Test:  [1440/3316]  eta: 15:27:26  loss: 7.2271 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0067  data: 0.0027
Test:  [1440/3316]  eta: 15:28:00  loss: 7.2271 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1281  data: 0.0026
Test:  [1440/3316]  eta: 15:28:59  loss: 7.2271 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0808  data: 0.0028
Test:  [1450/3316]  eta: 15:18:02  loss: 7.2554 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0581  data: 0.0031
Test:  [1450/3316]  eta: 15:20:02  loss: 7.2554 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9833  data: 0.0027
Test:  [1460/3316]  eta: 15:09:45  loss: 7.2677 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0254  data: 0.0027
Test:  [1450/3316]  eta: 15:21:00  loss: 7.2554 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8598  data: 0.0027
Test:  [1460/3316]  eta: 15:10:34  loss: 7.2677 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9114  data: 0.0027
Test:  [1450/3316]  eta: 15:22:06  loss: 7.2554 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8507  data: 0.0026
Test:  [1450/3316]  eta: 15:22:40  loss: 7.2554 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9299  data: 0.0026
Test:  [1450/3316]  eta: 15:23:44  loss: 7.2554 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1613  data: 0.0029
Test:  [1460/3316]  eta: 15:12:51  loss: 7.2677 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1302  data: 0.0029
Test:  [1460/3316]  eta: 15:14:51  loss: 7.2677 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2124  data: 0.0034
Test:  [1470/3316]  eta: 15:04:39  loss: 7.2575 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2348  data: 0.0036
Test:  [1460/3316]  eta: 15:15:47  loss: 7.2677 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0707  data: 0.0031
Test:  [1470/3316]  eta: 15:05:25  loss: 7.2575 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0749  data: 0.0040
Test:  [1460/3316]  eta: 15:16:51  loss: 7.2677 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0349  data: 0.0032
Test:  [1460/3316]  eta: 15:17:26  loss: 7.2677 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1168  data: 0.0034
Test:  [1460/3316]  eta: 15:18:25  loss: 7.2677 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1475  data: 0.0036
Test:  [1470/3316]  eta: 15:07:36  loss: 7.2575 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1254  data: 0.0036
Test:  [1470/3316]  eta: 15:09:36  loss: 7.2575 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2298  data: 0.0037
Test:  [1480/3316]  eta: 14:59:28  loss: 7.2358 (7.2403)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2352  data: 0.0039
Test:  [1470/3316]  eta: 15:10:29  loss: 7.2575 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0767  data: 0.0034
Test:  [1480/3316]  eta: 15:00:12  loss: 7.2358 (7.2403)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0942  data: 0.0039
Test:  [1470/3316]  eta: 15:11:32  loss: 7.2575 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0318  data: 0.0032
Test:  [1470/3316]  eta: 15:12:09  loss: 7.2575 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1465  data: 0.0033
Test:  [1470/3316]  eta: 15:13:05  loss: 7.2575 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9081  data: 0.0031
Test:  [1480/3316]  eta: 15:02:21  loss: 7.2358 (7.2403)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9145  data: 0.0032
Test:  [1480/3316]  eta: 15:04:21  loss: 7.2358 (7.2403)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0069  data: 0.0026
Test:  [1490/3316]  eta: 14:54:16  loss: 7.2134 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0135  data: 0.0026
Test:  [1480/3316]  eta: 15:05:12  loss: 7.2358 (7.2403)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8739  data: 0.0026
Test:  [1490/3316]  eta: 14:54:59  loss: 7.2134 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9082  data: 0.0023
Test:  [1480/3316]  eta: 15:06:15  loss: 7.2358 (7.2403)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8940  data: 0.0024
Test:  [1480/3316]  eta: 15:06:51  loss: 7.2358 (7.2403)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9672  data: 0.0022
Test:  [1480/3316]  eta: 15:07:46  loss: 7.2358 (7.2403)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8909  data: 0.0022
Test:  [1490/3316]  eta: 14:57:07  loss: 7.2134 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8952  data: 0.0023
Test:  [1490/3316]  eta: 14:59:12  loss: 7.2134 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2470  data: 0.0022
Test:  [1500/3316]  eta: 14:49:11  loss: 7.2134 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2221  data: 0.0032
Test:  [1490/3316]  eta: 15:00:01  loss: 7.2134 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1155  data: 0.0026
Test:  [1500/3316]  eta: 14:49:53  loss: 7.2134 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1638  data: 0.0029
Test:  [1490/3316]  eta: 15:01:04  loss: 7.2134 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1745  data: 0.0029
Test:  [1490/3316]  eta: 15:01:40  loss: 7.2134 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2041  data: 0.0029
Test:  [1490/3316]  eta: 15:02:34  loss: 7.2134 (7.2404)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1378  data: 0.0029
Test:  [1500/3316]  eta: 14:51:59  loss: 7.2134 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1310  data: 0.0039
Test:  [1500/3316]  eta: 14:53:59  loss: 7.2134 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2927  data: 0.0037
Test:  [1510/3316]  eta: 14:44:01  loss: 7.2605 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2251  data: 0.0042
Test:  [1500/3316]  eta: 14:54:45  loss: 7.2134 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1336  data: 0.0035
Test:  [1510/3316]  eta: 14:44:42  loss: 7.2605 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1723  data: 0.0036
Test:  [1500/3316]  eta: 14:55:47  loss: 7.2134 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1434  data: 0.0035
Test:  [1500/3316]  eta: 14:56:23  loss: 7.2134 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1731  data: 0.0035
Test:  [1500/3316]  eta: 14:57:16  loss: 7.2134 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1233  data: 0.0031
Test:  [1510/3316]  eta: 14:46:46  loss: 7.2605 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1460  data: 0.0041
Test:  [1510/3316]  eta: 14:48:46  loss: 7.2605 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0818  data: 0.0037
Test:  [1510/3316]  eta: 14:49:30  loss: 7.2605 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9109  data: 0.0032
Test:  [1520/3316]  eta: 14:38:51  loss: 7.2605 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9869  data: 0.0034
Test:  [1520/3316]  eta: 14:39:30  loss: 7.2605 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9133  data: 0.0030
Test:  [1510/3316]  eta: 14:50:30  loss: 7.2605 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8725  data: 0.0028
Test:  [1510/3316]  eta: 14:51:07  loss: 7.2605 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9061  data: 0.0028
Test:  [1510/3316]  eta: 14:51:58  loss: 7.2605 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8587  data: 0.0024
Test:  [1520/3316]  eta: 14:41:33  loss: 7.2605 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9263  data: 0.0025
Test:  [1520/3316]  eta: 14:43:34  loss: 7.2605 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0896  data: 0.0023
Test:  [1520/3316]  eta: 14:44:15  loss: 7.2605 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9043  data: 0.0024
Test:  [1530/3316]  eta: 14:33:42  loss: 7.1863 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9931  data: 0.0024
Test:  [1530/3316]  eta: 14:34:19  loss: 7.1863 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8988  data: 0.0023
Test:  [1520/3316]  eta: 14:45:14  loss: 7.2605 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8701  data: 0.0023
Test:  [1520/3316]  eta: 14:45:55  loss: 7.2605 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1013  data: 0.0022
Test:  [1520/3316]  eta: 14:46:45  loss: 7.2605 (7.2405)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0450  data: 0.0027
Test:  [1530/3316]  eta: 14:36:25  loss: 7.1863 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1159  data: 0.0027
Test:  [1530/3316]  eta: 14:38:27  loss: 7.1863 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2999  data: 0.0038
Test:  [1530/3316]  eta: 14:39:05  loss: 7.1863 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1126  data: 0.0034
Test:  [1540/3316]  eta: 14:28:37  loss: 7.1863 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2190  data: 0.0032
Test:  [1540/3316]  eta: 14:29:13  loss: 7.1863 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0884  data: 0.0037
Test:  [1530/3316]  eta: 14:40:03  loss: 7.1863 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0511  data: 0.0042
Test:  [1530/3316]  eta: 14:40:40  loss: 7.1863 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1118  data: 0.0035
Test:  [1530/3316]  eta: 14:41:28  loss: 7.1863 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0395  data: 0.0038
Test:  [1540/3316]  eta: 14:31:13  loss: 7.1863 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1195  data: 0.0034
Test:  [1540/3316]  eta: 14:33:15  loss: 7.1863 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2726  data: 0.0040
Test:  [1540/3316]  eta: 14:33:52  loss: 7.1863 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1261  data: 0.0035
Test:  [1550/3316]  eta: 14:23:29  loss: 7.2113 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2179  data: 0.0034
Test:  [1550/3316]  eta: 14:24:02  loss: 7.2113 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0974  data: 0.0038
Test:  [1540/3316]  eta: 14:34:47  loss: 7.1863 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0430  data: 0.0040
Test:  [1540/3316]  eta: 14:35:25  loss: 7.1863 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9367  data: 0.0035
Test:  [1540/3316]  eta: 14:36:12  loss: 7.1863 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8483  data: 0.0033
Test:  [1550/3316]  eta: 14:26:02  loss: 7.2113 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9298  data: 0.0030
Test:  [1550/3316]  eta: 14:28:03  loss: 7.2113 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0327  data: 0.0026
Test:  [1550/3316]  eta: 14:28:38  loss: 7.2113 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9249  data: 0.0024
Test:  [1560/3316]  eta: 14:18:20  loss: 7.2240 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0070  data: 0.0025
Test:  [1560/3316]  eta: 14:18:53  loss: 7.2240 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9296  data: 0.0023
Test:  [1550/3316]  eta: 14:29:32  loss: 7.2113 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8311  data: 0.0022
Test:  [1550/3316]  eta: 14:30:10  loss: 7.2113 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9354  data: 0.0022
Test:  [1550/3316]  eta: 14:30:56  loss: 7.2113 (7.2398)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.8585  data: 0.0022
Test:  [1560/3316]  eta: 14:20:51  loss: 7.2240 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9312  data: 0.0022
Test:  [1560/3316]  eta: 14:22:56  loss: 7.2240 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2157  data: 0.0024
Test:  [1560/3316]  eta: 14:23:28  loss: 7.2240 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0683  data: 0.0028
Test:  [1570/3316]  eta: 14:13:15  loss: 7.2981 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1244  data: 0.0028
Test:  [1570/3316]  eta: 14:13:47  loss: 7.2981 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0772  data: 0.0032
Test:  [1560/3316]  eta: 14:24:21  loss: 7.2240 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9734  data: 0.0029
Test:  [1560/3316]  eta: 14:25:00  loss: 7.2240 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1106  data: 0.0030
Test:  [1560/3316]  eta: 14:25:44  loss: 7.2240 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0131  data: 0.0032
Test:  [1570/3316]  eta: 14:15:43  loss: 7.2981 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0869  data: 0.0031
Test:  [1570/3316]  eta: 14:17:46  loss: 7.2981 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2398  data: 0.0033
Test:  [1570/3316]  eta: 14:18:16  loss: 7.2981 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0597  data: 0.0036
Test:  [1580/3316]  eta: 14:08:07  loss: 7.2758 (7.2409)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0831  data: 0.0039
Test:  [1580/3316]  eta: 14:08:39  loss: 7.2758 (7.2409)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1006  data: 0.0040
Test:  [1570/3316]  eta: 14:19:07  loss: 7.2981 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0152  data: 0.0034
Test:  [1570/3316]  eta: 14:19:47  loss: 7.2981 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1355  data: 0.0036
Test:  [1570/3316]  eta: 14:20:30  loss: 7.2981 (7.2407)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0636  data: 0.0034
Test:  [1580/3316]  eta: 14:10:34  loss: 7.2758 (7.2409)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0966  data: 0.0035
Test:  [1580/3316]  eta: 14:12:36  loss: 7.2758 (7.2409)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1210  data: 0.0033
Test:  [1580/3316]  eta: 14:13:05  loss: 7.2758 (7.2409)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9664  data: 0.0032
Test:  [1590/3316]  eta: 14:03:00  loss: 7.2637 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9827  data: 0.0034
Test:  [1590/3316]  eta: 14:03:32  loss: 7.2637 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0667  data: 0.0031
Test:  [1580/3316]  eta: 14:13:55  loss: 7.2758 (7.2409)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 27.9492  data: 0.0028
Test:  [1580/3316]  eta: 14:14:36  loss: 7.2758 (7.2409)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0533  data: 0.0031
Test:  [1580/3316]  eta: 14:15:20  loss: 7.2758 (7.2409)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1323  data: 0.0025
Test:  [1590/3316]  eta: 14:05:28  loss: 7.2637 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1352  data: 0.0026
Test:  [1590/3316]  eta: 14:07:31  loss: 7.2637 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3071  data: 0.0030
Test:  [1590/3316]  eta: 14:07:57  loss: 7.2637 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1758  data: 0.0031
Test:  [1600/3316]  eta: 13:57:57  loss: 7.2481 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2146  data: 0.0030
Test:  [1600/3316]  eta: 13:58:29  loss: 7.2481 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2454  data: 0.0031
Test:  [1590/3316]  eta: 14:08:47  loss: 7.2637 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1649  data: 0.0033
Test:  [1590/3316]  eta: 14:09:28  loss: 7.2637 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2295  data: 0.0034
Test:  [1590/3316]  eta: 14:10:07  loss: 7.2637 (7.2410)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1211  data: 0.0030
Test:  [1600/3316]  eta: 14:00:21  loss: 7.2481 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2372  data: 0.0031
Test:  [1600/3316]  eta: 14:02:22  loss: 7.2481 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3088  data: 0.0031
Test:  [1600/3316]  eta: 14:02:47  loss: 7.2481 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1661  data: 0.0033
Test:  [1610/3316]  eta: 13:52:51  loss: 7.1730 (7.2402)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2453  data: 0.0033
Test:  [1610/3316]  eta: 13:53:23  loss: 7.1730 (7.2402)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2219  data: 0.0031
Test:  [1600/3316]  eta: 14:03:37  loss: 7.2481 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2016  data: 0.0032
Test:  [1600/3316]  eta: 14:04:17  loss: 7.2481 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1981  data: 0.0031
Test:  [1600/3316]  eta: 14:04:56  loss: 7.2481 (7.2408)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.0016  data: 0.0030
Test:  [1610/3316]  eta: 13:55:13  loss: 7.1730 (7.2402)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.1351  data: 0.0030
Test:  [1610/3316]  eta: 13:57:16  loss: 7.1730 (7.2402)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2808  data: 0.0026
Test:  [1610/3316]  eta: 13:57:41  loss: 7.1730 (7.2402)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2117  data: 0.0027
Test:  [1620/3316]  eta: 13:47:50  loss: 7.1230 (7.2395)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2873  data: 0.0035
Test:  [1620/3316]  eta: 13:48:20  loss: 7.1230 (7.2395)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2558  data: 0.0026
Test:  [1610/3316]  eta: 13:58:31  loss: 7.1730 (7.2402)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.2694  data: 0.0028
Test:  [1610/3316]  eta: 13:59:11  loss: 7.1730 (7.2402)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3055  data: 0.0031
Test:  [1610/3316]  eta: 13:59:50  loss: 7.1730 (7.2402)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3296  data: 0.0030
Test:  [1620/3316]  eta: 13:50:12  loss: 7.1230 (7.2395)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3552  data: 0.0033
Test:  [1620/3316]  eta: 13:52:13  loss: 7.1230 (7.2395)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4748  data: 0.0037
Test:  [1620/3316]  eta: 13:52:34  loss: 7.1230 (7.2395)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3440  data: 0.0038
Test:  [1630/3316]  eta: 13:42:47  loss: 7.1736 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4040  data: 0.0045
Test:  [1630/3316]  eta: 13:43:16  loss: 7.1736 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3448  data: 0.0036
Test:  [1620/3316]  eta: 13:53:23  loss: 7.1230 (7.2395)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.3580  data: 0.0040
Test:  [1620/3316]  eta: 13:54:05  loss: 7.1230 (7.2395)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4900  data: 0.0041
Test:  [1620/3316]  eta: 13:54:43  loss: 7.1230 (7.2395)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5180  data: 0.0034
Test:  [1630/3316]  eta: 13:45:10  loss: 7.1736 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6061  data: 0.0038
Test:  [1630/3316]  eta: 13:47:10  loss: 7.1736 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6180  data: 0.0039
Test:  [1630/3316]  eta: 13:47:30  loss: 7.1736 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4368  data: 0.0041
Test:  [1640/3316]  eta: 13:37:49  loss: 7.2327 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5481  data: 0.0039
Test:  [1640/3316]  eta: 13:38:17  loss: 7.2327 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4533  data: 0.0037
Test:  [1630/3316]  eta: 13:48:19  loss: 7.1736 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.4119  data: 0.0037
Test:  [1630/3316]  eta: 13:49:02  loss: 7.1736 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5956  data: 0.0034
Test:  [1630/3316]  eta: 13:49:39  loss: 7.1736 (7.2390)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5465  data: 0.0031
Test:  [1640/3316]  eta: 13:40:08  loss: 7.2327 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5848  data: 0.0032
Test:  [1640/3316]  eta: 13:42:08  loss: 7.2327 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6567  data: 0.0028
Test:  [1640/3316]  eta: 13:42:27  loss: 7.2327 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5901  data: 0.0029
Test:  [1650/3316]  eta: 13:32:50  loss: 7.3295 (7.2397)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7035  data: 0.0028
Test:  [1650/3316]  eta: 13:33:17  loss: 7.3295 (7.2397)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6137  data: 0.0028
Test:  [1640/3316]  eta: 13:43:15  loss: 7.2327 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5776  data: 0.0029
Test:  [1640/3316]  eta: 13:43:59  loss: 7.2327 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7504  data: 0.0027
Test:  [1640/3316]  eta: 13:44:37  loss: 7.2327 (7.2396)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7458  data: 0.0034
Test:  [1650/3316]  eta: 13:35:10  loss: 7.3295 (7.2397)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7277  data: 0.0033
Test:  [1650/3316]  eta: 13:37:09  loss: 7.3295 (7.2397)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8121  data: 0.0041
Test:  [1650/3316]  eta: 13:37:26  loss: 7.3295 (7.2397)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6811  data: 0.0041
Test:  [1660/3316]  eta: 13:27:53  loss: 7.2780 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7561  data: 0.0041
Test:  [1660/3316]  eta: 13:28:20  loss: 7.2780 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7621  data: 0.0041
Test:  [1650/3316]  eta: 13:38:14  loss: 7.3295 (7.2397)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7134  data: 0.0045
Test:  [1650/3316]  eta: 13:38:58  loss: 7.3295 (7.2397)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7970  data: 0.0041
Test:  [1650/3316]  eta: 13:39:31  loss: 7.3295 (7.2397)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6712  data: 0.0042
Test:  [1660/3316]  eta: 13:30:07  loss: 7.2780 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6854  data: 0.0040
Test:  [1660/3316]  eta: 13:32:07  loss: 7.2780 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8094  data: 0.0042
Test:  [1660/3316]  eta: 13:32:22  loss: 7.2780 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6406  data: 0.0043
Test:  [1670/3316]  eta: 13:22:53  loss: 7.2877 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7103  data: 0.0041
Test:  [1670/3316]  eta: 13:23:21  loss: 7.2877 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7792  data: 0.0043
Test:  [1660/3316]  eta: 13:33:10  loss: 7.2780 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.6803  data: 0.0042
Test:  [1660/3316]  eta: 13:33:55  loss: 7.2780 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.7673  data: 0.0041
Test:  [1660/3316]  eta: 13:34:28  loss: 7.2780 (7.2401)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5886  data: 0.0037
Test:  [1670/3316]  eta: 13:25:07  loss: 7.2877 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.5767  data: 0.0035
Test:  [1670/3316]  eta: 13:27:12  loss: 7.2877 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9898  data: 0.0031
Test:  [1670/3316]  eta: 13:27:25  loss: 7.2877 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8255  data: 0.0035
Test:  [1680/3316]  eta: 13:17:59  loss: 7.2666 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8658  data: 0.0031
Test:  [1680/3316]  eta: 13:18:28  loss: 7.2666 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9395  data: 0.0035
Test:  [1670/3316]  eta: 13:28:13  loss: 7.2877 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.8785  data: 0.0033
Test:  [1670/3316]  eta: 13:28:58  loss: 7.2877 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9786  data: 0.0034
Test:  [1670/3316]  eta: 13:29:29  loss: 7.2877 (7.2399)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9266  data: 0.0039
Test:  [1680/3316]  eta: 13:20:14  loss: 7.2666 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0198  data: 0.0036
Test:  [1680/3316]  eta: 13:22:13  loss: 7.2666 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.1617  data: 0.0045
Test:  [1680/3316]  eta: 13:22:25  loss: 7.2666 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9880  data: 0.0045
Test:  [1690/3316]  eta: 13:13:02  loss: 7.2536 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9759  data: 0.0044
Test:  [1690/3316]  eta: 13:13:30  loss: 7.2536 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 29.0256  data: 0.0046
Test:  [1680/3316]  eta: 13:23:11  loss: 7.2666 (7.2400)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 28.9872  data: 0.0042
