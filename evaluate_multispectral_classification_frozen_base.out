/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
Not using distributed mode
[11:12:04.876843] job dir: /home/ada/satmae/SatMAE
[11:12:04.876931] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=8,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='sentinel',
device='cuda',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
grouped_bands=[],
input_size=96,
layer_decay=0.75,
local_rank=0,
log_dir='/home/ada/satmae/multispectral/evaluation/frozen_vit_base',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
model_type='group_c',
nb_classes=62,
num_workers=8,
output_dir='/home/ada/satmae/multispectral/evaluation/frozen_vit_base',
patch_size=8,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/multispectral/checkpoints/pretrain-vit-base-e199.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/multispectral/data/fmow-sentinel/val.csv',
train_path='/home/ada/satmae/multispectral/data/fmow-sentinel/train.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[11:12:08.413630] <util.datasets.SentinelIndividualImageDataset object at 0x7fdd78eb1dd0>
[11:12:08.814698] <util.datasets.SentinelIndividualImageDataset object at 0x7fdd79044b50>
[11:12:08.814791] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fdd78eb1f50>
[11:12:08.814938] Grouping bands [[0, 1, 2, 6], [3, 4, 5, 7], [8, 9]]
[11:12:25.622703] Model = GroupChannelsVisionTransformer(
  (patch_embed): ModuleList(
    (0): PatchEmbed(
      (proj): Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8))
    )
    (1): PatchEmbed(
      (proj): Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8))
    )
    (2): PatchEmbed(
      (proj): Conv2d(2, 768, kernel_size=(8, 8), stride=(8, 8))
    )
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=768, out_features=62, bias=True)
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
[11:12:25.622768] number of params (M): 85.67
[11:12:25.622790] base lr: 1.00e-03
[11:12:25.622805] actual lr: 3.13e-05
[11:12:25.622818] accumulate grad iterations: 1
[11:12:25.622831] effective batch size: 8
[11:12:25.624661] criterion = LabelSmoothingCrossEntropy()
[11:12:28.670338] Resume checkpoint /home/ada/satmae/multispectral/checkpoints/pretrain-vit-base-e199.pth
[11:12:29.740707] Test:  [    0/10618]  eta: 3:08:47  loss: 3.8196 (3.8196)  acc1: 12.5000 (12.5000)  acc5: 37.5000 (37.5000)  time: 1.0668  data: 0.6046  max mem: 1611
[11:12:33.030571] Test:  [   10/10618]  eta: 1:10:00  loss: 4.2563 (4.2282)  acc1: 0.0000 (1.1364)  acc5: 0.0000 (6.8182)  time: 0.3960  data: 0.0551  max mem: 1611
[11:12:36.312477] Test:  [   20/10618]  eta: 1:04:14  loss: 4.2627 (4.2611)  acc1: 0.0000 (0.5952)  acc5: 0.0000 (6.5476)  time: 0.3285  data: 0.0002  max mem: 1611
[11:12:39.584413] Test:  [   30/10618]  eta: 1:02:05  loss: 4.1965 (4.2468)  acc1: 0.0000 (1.2097)  acc5: 0.0000 (8.0645)  time: 0.3276  data: 0.0002  max mem: 1611
[11:12:42.853769] Test:  [   40/10618]  eta: 1:00:57  loss: 4.1719 (4.2293)  acc1: 0.0000 (0.9146)  acc5: 12.5000 (8.5366)  time: 0.3270  data: 0.0002  max mem: 1611
[11:12:46.090233] Test:  [   50/10618]  eta: 1:00:08  loss: 4.1782 (4.2199)  acc1: 0.0000 (1.2255)  acc5: 12.5000 (9.0686)  time: 0.3252  data: 0.0002  max mem: 1611
[11:12:49.333398] Test:  [   60/10618]  eta: 0:59:34  loss: 4.1335 (4.2041)  acc1: 0.0000 (1.8443)  acc5: 12.5000 (10.0410)  time: 0.3239  data: 0.0002  max mem: 1611
[11:12:52.587874] Test:  [   70/10618]  eta: 0:59:11  loss: 4.1724 (4.2114)  acc1: 0.0000 (1.5845)  acc5: 12.5000 (9.6831)  time: 0.3248  data: 0.0002  max mem: 1611
[11:12:55.832858] Test:  [   80/10618]  eta: 0:58:52  loss: 4.2168 (4.2108)  acc1: 0.0000 (2.1605)  acc5: 12.5000 (10.3395)  time: 0.3249  data: 0.0002  max mem: 1611
[11:12:59.081644] Test:  [   90/10618]  eta: 0:58:37  loss: 4.2168 (4.2187)  acc1: 0.0000 (1.9231)  acc5: 12.5000 (9.7527)  time: 0.3246  data: 0.0002  max mem: 1611
[11:13:02.312516] Test:  [  100/10618]  eta: 0:58:22  loss: 4.2146 (4.2201)  acc1: 0.0000 (1.8564)  acc5: 0.0000 (9.4059)  time: 0.3239  data: 0.0002  max mem: 1611
[11:13:05.558165] Test:  [  110/10618]  eta: 0:58:10  loss: 4.2151 (4.2243)  acc1: 0.0000 (1.8018)  acc5: 12.5000 (9.7973)  time: 0.3237  data: 0.0002  max mem: 1611
[11:13:08.792663] Test:  [  120/10618]  eta: 0:57:59  loss: 4.1506 (4.2129)  acc1: 0.0000 (1.7562)  acc5: 12.5000 (10.0207)  time: 0.3239  data: 0.0002  max mem: 1611
[11:13:12.039343] Test:  [  130/10618]  eta: 0:57:50  loss: 4.0903 (4.2096)  acc1: 0.0000 (1.6221)  acc5: 12.5000 (10.1145)  time: 0.3240  data: 0.0002  max mem: 1611
[11:13:15.308487] Test:  [  140/10618]  eta: 0:57:44  loss: 4.2488 (4.2144)  acc1: 0.0000 (1.5071)  acc5: 0.0000 (9.5745)  time: 0.3257  data: 0.0002  max mem: 1611
[11:13:18.573642] Test:  [  150/10618]  eta: 0:57:38  loss: 4.2378 (4.2134)  acc1: 0.0000 (1.4073)  acc5: 0.0000 (9.7682)  time: 0.3266  data: 0.0002  max mem: 1611
[11:13:21.838111] Test:  [  160/10618]  eta: 0:57:32  loss: 4.1614 (4.2105)  acc1: 0.0000 (1.3975)  acc5: 12.5000 (10.0155)  time: 0.3264  data: 0.0002  max mem: 1611
[11:13:25.118351] Test:  [  170/10618]  eta: 0:57:27  loss: 4.1704 (4.2083)  acc1: 0.0000 (1.4620)  acc5: 12.5000 (10.0146)  time: 0.3272  data: 0.0002  max mem: 1611
[11:13:28.393844] Test:  [  180/10618]  eta: 0:57:23  loss: 4.0669 (4.2020)  acc1: 0.0000 (1.6575)  acc5: 12.5000 (10.5663)  time: 0.3277  data: 0.0002  max mem: 1611
[11:13:31.680809] Test:  [  190/10618]  eta: 0:57:19  loss: 4.1350 (4.2014)  acc1: 0.0000 (1.7016)  acc5: 12.5000 (10.6021)  time: 0.3280  data: 0.0002  max mem: 1611
[11:13:34.968874] Test:  [  200/10618]  eta: 0:57:15  loss: 4.2017 (4.2005)  acc1: 0.0000 (1.6169)  acc5: 12.5000 (10.6343)  time: 0.3287  data: 0.0002  max mem: 1611
[11:13:38.254647] Test:  [  210/10618]  eta: 0:57:11  loss: 4.1902 (4.2021)  acc1: 0.0000 (1.5995)  acc5: 0.0000 (10.4265)  time: 0.3286  data: 0.0002  max mem: 1611
[11:13:41.541816] Test:  [  220/10618]  eta: 0:57:07  loss: 4.1755 (4.2014)  acc1: 0.0000 (1.5271)  acc5: 0.0000 (10.1810)  time: 0.3286  data: 0.0002  max mem: 1611
[11:13:44.810886] Test:  [  230/10618]  eta: 0:57:02  loss: 4.1685 (4.1992)  acc1: 0.0000 (1.5152)  acc5: 0.0000 (10.3896)  time: 0.3277  data: 0.0002  max mem: 1611
[11:13:48.113180] Test:  [  240/10618]  eta: 0:56:59  loss: 4.2168 (4.1991)  acc1: 0.0000 (1.5041)  acc5: 12.5000 (10.3734)  time: 0.3285  data: 0.0002  max mem: 1611
[11:13:51.402315] Test:  [  250/10618]  eta: 0:56:56  loss: 4.1992 (4.1974)  acc1: 0.0000 (1.5438)  acc5: 12.5000 (10.4582)  time: 0.3295  data: 0.0002  max mem: 1611
[11:13:54.670784] Test:  [  260/10618]  eta: 0:56:51  loss: 4.1489 (4.1972)  acc1: 0.0000 (1.5805)  acc5: 12.5000 (10.4406)  time: 0.3278  data: 0.0002  max mem: 1611
[11:13:57.933569] Test:  [  270/10618]  eta: 0:56:47  loss: 4.1079 (4.1923)  acc1: 0.0000 (1.5683)  acc5: 12.5000 (10.6550)  time: 0.3265  data: 0.0002  max mem: 1611
[11:14:01.210483] Test:  [  280/10618]  eta: 0:56:43  loss: 4.0908 (4.1911)  acc1: 0.0000 (1.5569)  acc5: 12.5000 (10.6317)  time: 0.3269  data: 0.0002  max mem: 1611
[11:14:04.491703] Test:  [  290/10618]  eta: 0:56:39  loss: 4.1338 (4.1908)  acc1: 0.0000 (1.5034)  acc5: 0.0000 (10.5241)  time: 0.3278  data: 0.0002  max mem: 1611
[11:14:07.744670] Test:  [  300/10618]  eta: 0:56:35  loss: 4.1594 (4.1918)  acc1: 0.0000 (1.4535)  acc5: 0.0000 (10.4236)  time: 0.3266  data: 0.0002  max mem: 1611
[11:14:11.007416] Test:  [  310/10618]  eta: 0:56:30  loss: 4.1746 (4.1903)  acc1: 0.0000 (1.4871)  acc5: 0.0000 (10.4502)  time: 0.3257  data: 0.0002  max mem: 1611
[11:14:14.274108] Test:  [  320/10618]  eta: 0:56:26  loss: 4.1841 (4.1916)  acc1: 0.0000 (1.4798)  acc5: 12.5000 (10.3972)  time: 0.3264  data: 0.0002  max mem: 1611
[11:14:17.545394] Test:  [  330/10618]  eta: 0:56:23  loss: 4.1941 (4.1897)  acc1: 0.0000 (1.5106)  acc5: 12.5000 (10.5363)  time: 0.3268  data: 0.0002  max mem: 1611
[11:14:20.797974] Test:  [  340/10618]  eta: 0:56:18  loss: 4.1624 (4.1898)  acc1: 0.0000 (1.5029)  acc5: 12.5000 (10.4839)  time: 0.3261  data: 0.0002  max mem: 1611
[11:14:24.075342] Test:  [  350/10618]  eta: 0:56:15  loss: 4.1436 (4.1878)  acc1: 0.0000 (1.4957)  acc5: 0.0000 (10.3989)  time: 0.3264  data: 0.0002  max mem: 1611
[11:14:27.325015] Test:  [  360/10618]  eta: 0:56:10  loss: 4.1091 (4.1856)  acc1: 0.0000 (1.5235)  acc5: 12.5000 (10.4571)  time: 0.3263  data: 0.0002  max mem: 1611
[11:14:30.570708] Test:  [  370/10618]  eta: 0:56:06  loss: 4.0940 (4.1827)  acc1: 0.0000 (1.5499)  acc5: 12.5000 (10.6132)  time: 0.3247  data: 0.0002  max mem: 1611
[11:14:33.803979] Test:  [  380/10618]  eta: 0:56:01  loss: 4.0940 (4.1818)  acc1: 0.0000 (1.5092)  acc5: 12.5000 (10.5315)  time: 0.3239  data: 0.0002  max mem: 1611
[11:14:37.061352] Test:  [  390/10618]  eta: 0:55:57  loss: 4.1919 (4.1837)  acc1: 0.0000 (1.4706)  acc5: 12.5000 (10.4540)  time: 0.3244  data: 0.0002  max mem: 1611
[11:14:40.303499] Test:  [  400/10618]  eta: 0:55:53  loss: 4.2192 (4.1839)  acc1: 0.0000 (1.4651)  acc5: 12.5000 (10.3803)  time: 0.3249  data: 0.0002  max mem: 1611
[11:14:43.559386] Test:  [  410/10618]  eta: 0:55:49  loss: 4.1899 (4.1836)  acc1: 0.0000 (1.4599)  acc5: 0.0000 (10.3102)  time: 0.3248  data: 0.0002  max mem: 1611
[11:14:46.809680] Test:  [  420/10618]  eta: 0:55:45  loss: 4.1711 (4.1819)  acc1: 0.0000 (1.5143)  acc5: 0.0000 (10.3622)  time: 0.3252  data: 0.0002  max mem: 1611
[11:14:50.039328] Test:  [  430/10618]  eta: 0:55:40  loss: 4.1353 (4.1817)  acc1: 0.0000 (1.4791)  acc5: 0.0000 (10.4698)  time: 0.3239  data: 0.0002  max mem: 1611
[11:14:53.276095] Test:  [  440/10618]  eta: 0:55:36  loss: 4.1587 (4.1822)  acc1: 0.0000 (1.5023)  acc5: 12.5000 (10.4592)  time: 0.3232  data: 0.0002  max mem: 1611
[11:14:56.496675] Test:  [  450/10618]  eta: 0:55:31  loss: 4.1587 (4.1825)  acc1: 0.0000 (1.4690)  acc5: 12.5000 (10.4490)  time: 0.3228  data: 0.0002  max mem: 1611
[11:14:59.747853] Test:  [  460/10618]  eta: 0:55:27  loss: 4.2317 (4.1831)  acc1: 0.0000 (1.4913)  acc5: 0.0000 (10.4121)  time: 0.3235  data: 0.0002  max mem: 1611
[11:15:03.010305] Test:  [  470/10618]  eta: 0:55:24  loss: 4.2078 (4.1817)  acc1: 0.0000 (1.5127)  acc5: 0.0000 (10.3769)  time: 0.3256  data: 0.0002  max mem: 1611
[11:15:06.272913] Test:  [  480/10618]  eta: 0:55:20  loss: 4.1694 (4.1827)  acc1: 0.0000 (1.5593)  acc5: 12.5000 (10.4730)  time: 0.3262  data: 0.0002  max mem: 1611
[11:15:09.543103] Test:  [  490/10618]  eta: 0:55:17  loss: 4.2158 (4.1845)  acc1: 0.0000 (1.5275)  acc5: 12.5000 (10.5143)  time: 0.3265  data: 0.0002  max mem: 1611
[11:15:12.807156] Test:  [  500/10618]  eta: 0:55:13  loss: 4.1709 (4.1824)  acc1: 0.0000 (1.5968)  acc5: 12.5000 (10.5539)  time: 0.3266  data: 0.0002  max mem: 1611
[11:15:16.065580] Test:  [  510/10618]  eta: 0:55:10  loss: 4.1465 (4.1836)  acc1: 0.0000 (1.5900)  acc5: 12.5000 (10.4941)  time: 0.3260  data: 0.0002  max mem: 1611
[11:15:19.332411] Test:  [  520/10618]  eta: 0:55:06  loss: 4.2070 (4.1842)  acc1: 0.0000 (1.5595)  acc5: 0.0000 (10.4607)  time: 0.3262  data: 0.0002  max mem: 1611
[11:15:22.601925] Test:  [  530/10618]  eta: 0:55:03  loss: 4.1758 (4.1841)  acc1: 0.0000 (1.5537)  acc5: 0.0000 (10.4520)  time: 0.3267  data: 0.0002  max mem: 1611
[11:15:25.875599] Test:  [  540/10618]  eta: 0:55:00  loss: 4.1504 (4.1842)  acc1: 0.0000 (1.5943)  acc5: 12.5000 (10.4667)  time: 0.3271  data: 0.0002  max mem: 1611
[11:15:29.148830] Test:  [  550/10618]  eta: 0:54:56  loss: 4.1934 (4.1846)  acc1: 0.0000 (1.5880)  acc5: 12.5000 (10.4809)  time: 0.3273  data: 0.0002  max mem: 1611
[11:15:32.446497] Test:  [  560/10618]  eta: 0:54:53  loss: 4.1780 (4.1826)  acc1: 0.0000 (1.5820)  acc5: 12.5000 (10.5615)  time: 0.3285  data: 0.0002  max mem: 1611
[11:15:35.717844] Test:  [  570/10618]  eta: 0:54:50  loss: 4.1597 (4.1828)  acc1: 0.0000 (1.5543)  acc5: 12.5000 (10.5298)  time: 0.3284  data: 0.0002  max mem: 1611
[11:15:38.979770] Test:  [  580/10618]  eta: 0:54:47  loss: 4.1875 (4.1833)  acc1: 0.0000 (1.5921)  acc5: 12.5000 (10.5637)  time: 0.3266  data: 0.0002  max mem: 1611
[11:15:42.229757] Test:  [  590/10618]  eta: 0:54:43  loss: 4.1946 (4.1825)  acc1: 0.0000 (1.6286)  acc5: 12.5000 (10.6176)  time: 0.3255  data: 0.0002  max mem: 1611
[11:15:45.527511] Test:  [  600/10618]  eta: 0:54:40  loss: 4.0916 (4.1822)  acc1: 0.0000 (1.6431)  acc5: 12.5000 (10.6697)  time: 0.3273  data: 0.0002  max mem: 1611
[11:15:48.793297] Test:  [  610/10618]  eta: 0:54:37  loss: 4.1089 (4.1819)  acc1: 0.0000 (1.6367)  acc5: 12.5000 (10.7815)  time: 0.3281  data: 0.0002  max mem: 1611
[11:15:52.079851] Test:  [  620/10618]  eta: 0:54:33  loss: 4.1753 (4.1814)  acc1: 0.0000 (1.7110)  acc5: 12.5000 (10.8293)  time: 0.3275  data: 0.0002  max mem: 1611
[11:15:55.369860] Test:  [  630/10618]  eta: 0:54:30  loss: 4.1753 (4.1823)  acc1: 0.0000 (1.6838)  acc5: 12.5000 (10.7567)  time: 0.3287  data: 0.0002  max mem: 1611
[11:15:58.648718] Test:  [  640/10618]  eta: 0:54:27  loss: 4.2146 (4.1828)  acc1: 0.0000 (1.6966)  acc5: 0.0000 (10.6864)  time: 0.3284  data: 0.0002  max mem: 1611
[11:16:01.932433] Test:  [  650/10618]  eta: 0:54:24  loss: 4.1636 (4.1822)  acc1: 0.0000 (1.7089)  acc5: 0.0000 (10.7335)  time: 0.3280  data: 0.0002  max mem: 1611
[11:16:05.209960] Test:  [  660/10618]  eta: 0:54:21  loss: 4.1907 (4.1837)  acc1: 0.0000 (1.6831)  acc5: 12.5000 (10.6657)  time: 0.3280  data: 0.0002  max mem: 1611
[11:16:08.497335] Test:  [  670/10618]  eta: 0:54:18  loss: 4.2461 (4.1843)  acc1: 0.0000 (1.6580)  acc5: 12.5000 (10.6930)  time: 0.3282  data: 0.0002  max mem: 1611
[11:16:11.778365] Test:  [  680/10618]  eta: 0:54:14  loss: 4.2446 (4.1852)  acc1: 0.0000 (1.6520)  acc5: 12.5000 (10.6461)  time: 0.3283  data: 0.0002  max mem: 1611
[11:16:15.058889] Test:  [  690/10618]  eta: 0:54:11  loss: 4.1433 (4.1845)  acc1: 0.0000 (1.6462)  acc5: 12.5000 (10.6548)  time: 0.3280  data: 0.0002  max mem: 1611
[11:16:18.349143] Test:  [  700/10618]  eta: 0:54:08  loss: 4.1541 (4.1842)  acc1: 0.0000 (1.6405)  acc5: 0.0000 (10.6812)  time: 0.3284  data: 0.0002  max mem: 1611
[11:16:21.634166] Test:  [  710/10618]  eta: 0:54:05  loss: 4.1614 (4.1835)  acc1: 0.0000 (1.6174)  acc5: 12.5000 (10.7595)  time: 0.3287  data: 0.0002  max mem: 1611
[11:16:24.920616] Test:  [  720/10618]  eta: 0:54:02  loss: 4.1926 (4.1854)  acc1: 0.0000 (1.5950)  acc5: 12.5000 (10.6796)  time: 0.3285  data: 0.0002  max mem: 1611
