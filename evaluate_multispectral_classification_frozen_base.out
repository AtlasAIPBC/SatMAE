/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
Not using distributed mode
[11:12:04.876843] job dir: /home/ada/satmae/SatMAE
[11:12:04.876931] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=8,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='sentinel',
device='cuda',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
grouped_bands=[],
input_size=96,
layer_decay=0.75,
local_rank=0,
log_dir='/home/ada/satmae/multispectral/evaluation/frozen_vit_base',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
model_type='group_c',
nb_classes=62,
num_workers=8,
output_dir='/home/ada/satmae/multispectral/evaluation/frozen_vit_base',
patch_size=8,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/multispectral/checkpoints/pretrain-vit-base-e199.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/multispectral/data/fmow-sentinel/val.csv',
train_path='/home/ada/satmae/multispectral/data/fmow-sentinel/train.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[11:12:08.413630] <util.datasets.SentinelIndividualImageDataset object at 0x7fdd78eb1dd0>
[11:12:08.814698] <util.datasets.SentinelIndividualImageDataset object at 0x7fdd79044b50>
[11:12:08.814791] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fdd78eb1f50>
[11:12:08.814938] Grouping bands [[0, 1, 2, 6], [3, 4, 5, 7], [8, 9]]
[11:12:25.622703] Model = GroupChannelsVisionTransformer(
  (patch_embed): ModuleList(
    (0): PatchEmbed(
      (proj): Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8))
    )
    (1): PatchEmbed(
      (proj): Conv2d(4, 768, kernel_size=(8, 8), stride=(8, 8))
    )
    (2): PatchEmbed(
      (proj): Conv2d(2, 768, kernel_size=(8, 8), stride=(8, 8))
    )
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=768, out_features=62, bias=True)
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
)
[11:12:25.622768] number of params (M): 85.67
[11:12:25.622790] base lr: 1.00e-03
[11:12:25.622805] actual lr: 3.13e-05
[11:12:25.622818] accumulate grad iterations: 1
[11:12:25.622831] effective batch size: 8
[11:12:25.624661] criterion = LabelSmoothingCrossEntropy()
[11:12:28.670338] Resume checkpoint /home/ada/satmae/multispectral/checkpoints/pretrain-vit-base-e199.pth
[11:12:29.740707] Test:  [    0/10618]  eta: 3:08:47  loss: 3.8196 (3.8196)  acc1: 12.5000 (12.5000)  acc5: 37.5000 (37.5000)  time: 1.0668  data: 0.6046  max mem: 1611
[11:12:33.030571] Test:  [   10/10618]  eta: 1:10:00  loss: 4.2563 (4.2282)  acc1: 0.0000 (1.1364)  acc5: 0.0000 (6.8182)  time: 0.3960  data: 0.0551  max mem: 1611
[11:12:36.312477] Test:  [   20/10618]  eta: 1:04:14  loss: 4.2627 (4.2611)  acc1: 0.0000 (0.5952)  acc5: 0.0000 (6.5476)  time: 0.3285  data: 0.0002  max mem: 1611
[11:12:39.584413] Test:  [   30/10618]  eta: 1:02:05  loss: 4.1965 (4.2468)  acc1: 0.0000 (1.2097)  acc5: 0.0000 (8.0645)  time: 0.3276  data: 0.0002  max mem: 1611
[11:12:42.853769] Test:  [   40/10618]  eta: 1:00:57  loss: 4.1719 (4.2293)  acc1: 0.0000 (0.9146)  acc5: 12.5000 (8.5366)  time: 0.3270  data: 0.0002  max mem: 1611
[11:12:46.090233] Test:  [   50/10618]  eta: 1:00:08  loss: 4.1782 (4.2199)  acc1: 0.0000 (1.2255)  acc5: 12.5000 (9.0686)  time: 0.3252  data: 0.0002  max mem: 1611
[11:12:49.333398] Test:  [   60/10618]  eta: 0:59:34  loss: 4.1335 (4.2041)  acc1: 0.0000 (1.8443)  acc5: 12.5000 (10.0410)  time: 0.3239  data: 0.0002  max mem: 1611
[11:12:52.587874] Test:  [   70/10618]  eta: 0:59:11  loss: 4.1724 (4.2114)  acc1: 0.0000 (1.5845)  acc5: 12.5000 (9.6831)  time: 0.3248  data: 0.0002  max mem: 1611
[11:12:55.832858] Test:  [   80/10618]  eta: 0:58:52  loss: 4.2168 (4.2108)  acc1: 0.0000 (2.1605)  acc5: 12.5000 (10.3395)  time: 0.3249  data: 0.0002  max mem: 1611
[11:12:59.081644] Test:  [   90/10618]  eta: 0:58:37  loss: 4.2168 (4.2187)  acc1: 0.0000 (1.9231)  acc5: 12.5000 (9.7527)  time: 0.3246  data: 0.0002  max mem: 1611
[11:13:02.312516] Test:  [  100/10618]  eta: 0:58:22  loss: 4.2146 (4.2201)  acc1: 0.0000 (1.8564)  acc5: 0.0000 (9.4059)  time: 0.3239  data: 0.0002  max mem: 1611
[11:13:05.558165] Test:  [  110/10618]  eta: 0:58:10  loss: 4.2151 (4.2243)  acc1: 0.0000 (1.8018)  acc5: 12.5000 (9.7973)  time: 0.3237  data: 0.0002  max mem: 1611
[11:13:08.792663] Test:  [  120/10618]  eta: 0:57:59  loss: 4.1506 (4.2129)  acc1: 0.0000 (1.7562)  acc5: 12.5000 (10.0207)  time: 0.3239  data: 0.0002  max mem: 1611
[11:13:12.039343] Test:  [  130/10618]  eta: 0:57:50  loss: 4.0903 (4.2096)  acc1: 0.0000 (1.6221)  acc5: 12.5000 (10.1145)  time: 0.3240  data: 0.0002  max mem: 1611
[11:13:15.308487] Test:  [  140/10618]  eta: 0:57:44  loss: 4.2488 (4.2144)  acc1: 0.0000 (1.5071)  acc5: 0.0000 (9.5745)  time: 0.3257  data: 0.0002  max mem: 1611
[11:13:18.573642] Test:  [  150/10618]  eta: 0:57:38  loss: 4.2378 (4.2134)  acc1: 0.0000 (1.4073)  acc5: 0.0000 (9.7682)  time: 0.3266  data: 0.0002  max mem: 1611
[11:13:21.838111] Test:  [  160/10618]  eta: 0:57:32  loss: 4.1614 (4.2105)  acc1: 0.0000 (1.3975)  acc5: 12.5000 (10.0155)  time: 0.3264  data: 0.0002  max mem: 1611
[11:13:25.118351] Test:  [  170/10618]  eta: 0:57:27  loss: 4.1704 (4.2083)  acc1: 0.0000 (1.4620)  acc5: 12.5000 (10.0146)  time: 0.3272  data: 0.0002  max mem: 1611
[11:13:28.393844] Test:  [  180/10618]  eta: 0:57:23  loss: 4.0669 (4.2020)  acc1: 0.0000 (1.6575)  acc5: 12.5000 (10.5663)  time: 0.3277  data: 0.0002  max mem: 1611
[11:13:31.680809] Test:  [  190/10618]  eta: 0:57:19  loss: 4.1350 (4.2014)  acc1: 0.0000 (1.7016)  acc5: 12.5000 (10.6021)  time: 0.3280  data: 0.0002  max mem: 1611
[11:13:34.968874] Test:  [  200/10618]  eta: 0:57:15  loss: 4.2017 (4.2005)  acc1: 0.0000 (1.6169)  acc5: 12.5000 (10.6343)  time: 0.3287  data: 0.0002  max mem: 1611
[11:13:38.254647] Test:  [  210/10618]  eta: 0:57:11  loss: 4.1902 (4.2021)  acc1: 0.0000 (1.5995)  acc5: 0.0000 (10.4265)  time: 0.3286  data: 0.0002  max mem: 1611
[11:13:41.541816] Test:  [  220/10618]  eta: 0:57:07  loss: 4.1755 (4.2014)  acc1: 0.0000 (1.5271)  acc5: 0.0000 (10.1810)  time: 0.3286  data: 0.0002  max mem: 1611
[11:13:44.810886] Test:  [  230/10618]  eta: 0:57:02  loss: 4.1685 (4.1992)  acc1: 0.0000 (1.5152)  acc5: 0.0000 (10.3896)  time: 0.3277  data: 0.0002  max mem: 1611
[11:13:48.113180] Test:  [  240/10618]  eta: 0:56:59  loss: 4.2168 (4.1991)  acc1: 0.0000 (1.5041)  acc5: 12.5000 (10.3734)  time: 0.3285  data: 0.0002  max mem: 1611
[11:13:51.402315] Test:  [  250/10618]  eta: 0:56:56  loss: 4.1992 (4.1974)  acc1: 0.0000 (1.5438)  acc5: 12.5000 (10.4582)  time: 0.3295  data: 0.0002  max mem: 1611
[11:13:54.670784] Test:  [  260/10618]  eta: 0:56:51  loss: 4.1489 (4.1972)  acc1: 0.0000 (1.5805)  acc5: 12.5000 (10.4406)  time: 0.3278  data: 0.0002  max mem: 1611
