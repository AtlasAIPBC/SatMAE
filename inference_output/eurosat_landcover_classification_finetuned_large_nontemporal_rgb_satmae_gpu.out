/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
Not using distributed mode
[07:09:24.549784] job dir: /home/ada/satmae/SatMAE
[07:09:24.549871] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=8,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='rgb',
device='cuda',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
grouped_bands=[],
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='/home/ada/satmae/other_data/eurosat/evaluation/rgb_nontemporal',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type=None,
nb_classes=1000,
num_workers=8,
output_dir='/home/ada/satmae/other_data/eurosat/evaluation/rgb_nontemporal',
patch_size=16,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/temporal/checkpoints/fmow_finetune.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/other_data/eurosat/val_rgb.csv',
train_path='/home/ada/satmae/other_data/eurosat/train_rgb.csv',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[07:09:24.565274] <util.datasets.CustomDatasetFromImages object at 0x7faee71c6c10>
[07:09:24.588612] <util.datasets.CustomDatasetFromImages object at 0x7faee71c1a50>
[07:09:24.588674] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7faee71c1b50>
[07:09:34.735005] Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=1000, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
[07:09:34.735094] number of params (M): 304.33
[07:09:34.735118] base lr: 1.00e-03
[07:09:34.735133] actual lr: 3.13e-05
[07:09:34.735146] accumulate grad iterations: 1
[07:09:34.735158] effective batch size: 8
[07:09:34.739629] criterion = LabelSmoothingCrossEntropy()
[07:09:36.954578] Resume checkpoint /home/ada/satmae/temporal/checkpoints/fmow_finetune.pth
[07:09:37.666595] Test:  [   0/2194]  eta: 0:25:54  loss: 5.0840 (5.0840)  acc1: 0.0000 (0.0000)  acc5: 25.0000 (25.0000)  time: 0.7084  data: 0.3589  max mem: 4645
[07:09:38.791714] Test:  [  10/2194]  eta: 0:06:03  loss: 5.1963 (5.0534)  acc1: 0.0000 (9.0909)  acc5: 25.0000 (18.1818)  time: 0.1666  data: 0.0328  max mem: 4645
[07:09:39.923894] Test:  [  20/2194]  eta: 0:05:06  loss: 5.2341 (5.1599)  acc1: 0.0000 (7.1429)  acc5: 12.5000 (16.6667)  time: 0.1128  data: 0.0001  max mem: 4645
[07:09:41.064726] Test:  [  30/2194]  eta: 0:04:46  loss: 5.2078 (5.1578)  acc1: 0.0000 (6.8548)  acc5: 12.5000 (15.7258)  time: 0.1136  data: 0.0001  max mem: 4645
[07:09:42.196054] Test:  [  40/2194]  eta: 0:04:35  loss: 5.0863 (5.1483)  acc1: 12.5000 (7.0122)  acc5: 12.5000 (16.7683)  time: 0.1135  data: 0.0001  max mem: 4645
[07:09:43.323019] Test:  [  50/2194]  eta: 0:04:27  loss: 5.0863 (5.1720)  acc1: 0.0000 (6.6176)  acc5: 12.5000 (15.6863)  time: 0.1128  data: 0.0001  max mem: 4645
[07:09:44.461293] Test:  [  60/2194]  eta: 0:04:22  loss: 5.1343 (5.1826)  acc1: 0.0000 (7.1721)  acc5: 12.5000 (15.9836)  time: 0.1132  data: 0.0001  max mem: 4645
[07:09:45.611914] Test:  [  70/2194]  eta: 0:04:18  loss: 5.1343 (5.1418)  acc1: 12.5000 (7.3944)  acc5: 12.5000 (16.3732)  time: 0.1144  data: 0.0001  max mem: 4645
[07:09:46.763647] Test:  [  80/2194]  eta: 0:04:15  loss: 4.7396 (5.1113)  acc1: 0.0000 (8.1790)  acc5: 12.5000 (16.9753)  time: 0.1150  data: 0.0002  max mem: 4645
[07:09:47.919529] Test:  [  90/2194]  eta: 0:04:13  loss: 4.7396 (5.0744)  acc1: 0.0000 (8.3791)  acc5: 25.0000 (17.7198)  time: 0.1153  data: 0.0002  max mem: 4645
[07:09:49.074633] Test:  [ 100/2194]  eta: 0:04:11  loss: 5.2244 (5.0953)  acc1: 0.0000 (8.1683)  acc5: 12.5000 (17.5743)  time: 0.1155  data: 0.0001  max mem: 4645
[07:09:50.236766] Test:  [ 110/2194]  eta: 0:04:09  loss: 5.0933 (5.0679)  acc1: 0.0000 (8.3333)  acc5: 12.5000 (17.9054)  time: 0.1158  data: 0.0001  max mem: 4645
[07:09:51.397453] Test:  [ 120/2194]  eta: 0:04:07  loss: 4.7784 (5.0504)  acc1: 0.0000 (8.7810)  acc5: 25.0000 (18.4917)  time: 0.1161  data: 0.0001  max mem: 4645
[07:09:52.565591] Test:  [ 130/2194]  eta: 0:04:05  loss: 4.9240 (5.0552)  acc1: 12.5000 (8.6832)  acc5: 12.5000 (18.3206)  time: 0.1164  data: 0.0001  max mem: 4645
[07:09:53.745468] Test:  [ 140/2194]  eta: 0:04:04  loss: 4.9272 (5.0452)  acc1: 12.5000 (8.5993)  acc5: 12.5000 (18.2624)  time: 0.1173  data: 0.0001  max mem: 4645
[07:09:54.921191] Test:  [ 150/2194]  eta: 0:04:02  loss: 5.0723 (5.0725)  acc1: 0.0000 (8.1126)  acc5: 12.5000 (17.7152)  time: 0.1177  data: 0.0001  max mem: 4645
[07:09:56.105533] Test:  [ 160/2194]  eta: 0:04:01  loss: 5.2678 (5.0797)  acc1: 0.0000 (8.0745)  acc5: 12.5000 (17.7019)  time: 0.1179  data: 0.0001  max mem: 4645
[07:09:57.284397] Test:  [ 170/2194]  eta: 0:04:00  loss: 5.1719 (5.0934)  acc1: 12.5000 (8.1871)  acc5: 12.5000 (17.9094)  time: 0.1181  data: 0.0001  max mem: 4645
[07:09:58.464277] Test:  [ 180/2194]  eta: 0:03:59  loss: 5.3662 (5.1013)  acc1: 12.5000 (8.1492)  acc5: 12.5000 (17.7486)  time: 0.1179  data: 0.0001  max mem: 4645
[07:09:59.644892] Test:  [ 190/2194]  eta: 0:03:57  loss: 5.3030 (5.0996)  acc1: 0.0000 (8.0497)  acc5: 12.5000 (17.9319)  time: 0.1179  data: 0.0001  max mem: 4645
[07:10:00.835211] Test:  [ 200/2194]  eta: 0:03:56  loss: 5.1277 (5.0871)  acc1: 0.0000 (8.0224)  acc5: 25.0000 (18.2214)  time: 0.1185  data: 0.0001  max mem: 4645
[07:10:02.033185] Test:  [ 210/2194]  eta: 0:03:55  loss: 4.9969 (5.0913)  acc1: 0.0000 (7.8199)  acc5: 25.0000 (18.0095)  time: 0.1193  data: 0.0002  max mem: 4645
[07:10:03.233022] Test:  [ 220/2194]  eta: 0:03:54  loss: 4.9873 (5.0865)  acc1: 0.0000 (7.8620)  acc5: 12.5000 (18.0430)  time: 0.1198  data: 0.0002  max mem: 4645
[07:10:04.424497] Test:  [ 230/2194]  eta: 0:03:53  loss: 4.8295 (5.0782)  acc1: 12.5000 (8.0628)  acc5: 25.0000 (18.3442)  time: 0.1195  data: 0.0001  max mem: 4645
[07:10:05.630817] Test:  [ 240/2194]  eta: 0:03:52  loss: 5.0441 (5.0827)  acc1: 12.5000 (8.1950)  acc5: 25.0000 (18.4647)  time: 0.1198  data: 0.0001  max mem: 4645
[07:10:06.827064] Test:  [ 250/2194]  eta: 0:03:51  loss: 5.2019 (5.0903)  acc1: 12.5000 (8.1175)  acc5: 12.5000 (18.2769)  time: 0.1201  data: 0.0001  max mem: 4645
[07:10:08.038783] Test:  [ 260/2194]  eta: 0:03:50  loss: 5.1400 (5.0917)  acc1: 0.0000 (8.0939)  acc5: 12.5000 (18.2950)  time: 0.1203  data: 0.0001  max mem: 4645
[07:10:09.253908] Test:  [ 270/2194]  eta: 0:03:49  loss: 5.0178 (5.0912)  acc1: 12.5000 (8.1181)  acc5: 25.0000 (18.3118)  time: 0.1213  data: 0.0001  max mem: 4645
[07:10:10.465267] Test:  [ 280/2194]  eta: 0:03:48  loss: 5.0969 (5.0940)  acc1: 0.0000 (7.9626)  acc5: 12.5000 (18.1495)  time: 0.1213  data: 0.0001  max mem: 4645
[07:10:11.675992] Test:  [ 290/2194]  eta: 0:03:47  loss: 5.3938 (5.1029)  acc1: 0.0000 (7.9467)  acc5: 12.5000 (18.0842)  time: 0.1210  data: 0.0001  max mem: 4645
[07:10:12.891427] Test:  [ 300/2194]  eta: 0:03:45  loss: 5.3229 (5.1024)  acc1: 0.0000 (7.8904)  acc5: 12.5000 (18.1894)  time: 0.1212  data: 0.0001  max mem: 4645
[07:10:14.120531] Test:  [ 310/2194]  eta: 0:03:44  loss: 5.2153 (5.1042)  acc1: 0.0000 (7.8778)  acc5: 12.5000 (18.2074)  time: 0.1221  data: 0.0001  max mem: 4645
[07:10:15.346186] Test:  [ 320/2194]  eta: 0:03:43  loss: 5.2440 (5.1123)  acc1: 0.0000 (7.8271)  acc5: 12.5000 (18.1075)  time: 0.1227  data: 0.0001  max mem: 4645
[07:10:16.582654] Test:  [ 330/2194]  eta: 0:03:42  loss: 5.2632 (5.1120)  acc1: 0.0000 (7.7795)  acc5: 12.5000 (18.1269)  time: 0.1230  data: 0.0002  max mem: 4645
[07:10:17.830594] Test:  [ 340/2194]  eta: 0:03:42  loss: 5.1206 (5.1116)  acc1: 0.0000 (7.8812)  acc5: 12.5000 (18.1818)  time: 0.1241  data: 0.0001  max mem: 4645
[07:10:19.082216] Test:  [ 350/2194]  eta: 0:03:41  loss: 5.0850 (5.1162)  acc1: 12.5000 (7.9060)  acc5: 12.5000 (17.9843)  time: 0.1249  data: 0.0001  max mem: 4645
[07:10:20.348493] Test:  [ 360/2194]  eta: 0:03:40  loss: 5.2167 (5.1286)  acc1: 0.0000 (7.8947)  acc5: 12.5000 (17.7978)  time: 0.1258  data: 0.0001  max mem: 4645
[07:10:21.611336] Test:  [ 370/2194]  eta: 0:03:39  loss: 5.2236 (5.1284)  acc1: 0.0000 (7.8504)  acc5: 0.0000 (17.6887)  time: 0.1264  data: 0.0001  max mem: 4645
[07:10:22.878249] Test:  [ 380/2194]  eta: 0:03:38  loss: 5.1094 (5.1334)  acc1: 0.0000 (7.7756)  acc5: 12.5000 (17.5853)  time: 0.1264  data: 0.0001  max mem: 4645
[07:10:24.150690] Test:  [ 390/2194]  eta: 0:03:37  loss: 5.3433 (5.1378)  acc1: 0.0000 (7.7366)  acc5: 12.5000 (17.6151)  time: 0.1269  data: 0.0001  max mem: 4645
[07:10:25.434983] Test:  [ 400/2194]  eta: 0:03:36  loss: 5.3433 (5.1386)  acc1: 0.0000 (7.7307)  acc5: 12.5000 (17.7057)  time: 0.1278  data: 0.0001  max mem: 4645
[07:10:26.728260] Test:  [ 410/2194]  eta: 0:03:35  loss: 5.1239 (5.1406)  acc1: 12.5000 (7.8163)  acc5: 25.0000 (17.7007)  time: 0.1288  data: 0.0001  max mem: 4645
[07:10:28.013152] Test:  [ 420/2194]  eta: 0:03:34  loss: 5.0854 (5.1353)  acc1: 12.5000 (7.8088)  acc5: 25.0000 (17.8147)  time: 0.1288  data: 0.0001  max mem: 4645
[07:10:29.294987] Test:  [ 430/2194]  eta: 0:03:34  loss: 4.9005 (5.1316)  acc1: 12.5000 (7.8306)  acc5: 25.0000 (17.8944)  time: 0.1283  data: 0.0001  max mem: 4645
[07:10:30.581012] Test:  [ 440/2194]  eta: 0:03:33  loss: 4.8049 (5.1242)  acc1: 12.5000 (7.9932)  acc5: 25.0000 (18.1406)  time: 0.1283  data: 0.0001  max mem: 4645
[07:10:31.846527] Test:  [ 450/2194]  eta: 0:03:32  loss: 5.0144 (5.1273)  acc1: 12.5000 (7.9545)  acc5: 25.0000 (18.0987)  time: 0.1275  data: 0.0001  max mem: 4645
[07:10:33.106449] Test:  [ 460/2194]  eta: 0:03:31  loss: 5.2816 (5.1306)  acc1: 0.0000 (7.9176)  acc5: 12.5000 (17.9772)  time: 0.1262  data: 0.0001  max mem: 4645
[07:10:34.361298] Test:  [ 470/2194]  eta: 0:03:29  loss: 5.3706 (5.1378)  acc1: 0.0000 (7.8556)  acc5: 12.5000 (17.9140)  time: 0.1257  data: 0.0001  max mem: 4645
[07:10:35.607760] Test:  [ 480/2194]  eta: 0:03:28  loss: 5.1846 (5.1281)  acc1: 12.5000 (8.0301)  acc5: 12.5000 (18.0613)  time: 0.1250  data: 0.0001  max mem: 4645
[07:10:36.839038] Test:  [ 490/2194]  eta: 0:03:27  loss: 4.9885 (5.1329)  acc1: 12.5000 (8.0193)  acc5: 25.0000 (18.0244)  time: 0.1238  data: 0.0001  max mem: 4645
[07:10:38.062567] Test:  [ 500/2194]  eta: 0:03:26  loss: 5.2240 (5.1336)  acc1: 12.5000 (8.0339)  acc5: 12.5000 (18.0389)  time: 0.1227  data: 0.0001  max mem: 4645
[07:10:39.275916] Test:  [ 510/2194]  eta: 0:03:25  loss: 5.0712 (5.1313)  acc1: 12.5000 (8.0724)  acc5: 12.5000 (17.9795)  time: 0.1218  data: 0.0001  max mem: 4645
[07:10:40.485206] Test:  [ 520/2194]  eta: 0:03:23  loss: 5.2318 (5.1360)  acc1: 0.0000 (7.9894)  acc5: 12.5000 (17.9463)  time: 0.1211  data: 0.0001  max mem: 4645
[07:10:41.691788] Test:  [ 530/2194]  eta: 0:03:22  loss: 5.3596 (5.1392)  acc1: 0.0000 (8.0273)  acc5: 12.5000 (17.9379)  time: 0.1207  data: 0.0001  max mem: 4645
[07:10:42.900759] Test:  [ 540/2194]  eta: 0:03:21  loss: 5.3870 (5.1433)  acc1: 0.0000 (7.9482)  acc5: 12.5000 (17.7911)  time: 0.1207  data: 0.0002  max mem: 4645
[07:10:44.111084] Test:  [ 550/2194]  eta: 0:03:20  loss: 5.2112 (5.1436)  acc1: 0.0000 (7.8947)  acc5: 12.5000 (17.8085)  time: 0.1209  data: 0.0002  max mem: 4645
[07:10:45.317067] Test:  [ 560/2194]  eta: 0:03:18  loss: 4.9695 (5.1342)  acc1: 0.0000 (8.0214)  acc5: 12.5000 (17.9367)  time: 0.1207  data: 0.0001  max mem: 4645
[07:10:46.509481] Test:  [ 570/2194]  eta: 0:03:17  loss: 5.3459 (5.1414)  acc1: 0.0000 (7.9466)  acc5: 12.5000 (17.8196)  time: 0.1198  data: 0.0001  max mem: 4645
[07:10:47.696503] Test:  [ 580/2194]  eta: 0:03:16  loss: 5.3684 (5.1439)  acc1: 0.0000 (7.8959)  acc5: 12.5000 (17.7926)  time: 0.1189  data: 0.0001  max mem: 4645
[07:10:48.880525] Test:  [ 590/2194]  eta: 0:03:15  loss: 5.0498 (5.1426)  acc1: 0.0000 (7.9103)  acc5: 12.5000 (17.8511)  time: 0.1185  data: 0.0001  max mem: 4645
[07:10:50.068069] Test:  [ 600/2194]  eta: 0:03:13  loss: 5.2155 (5.1485)  acc1: 0.0000 (7.8411)  acc5: 12.5000 (17.7413)  time: 0.1185  data: 0.0001  max mem: 4645
[07:10:51.252420] Test:  [ 610/2194]  eta: 0:03:12  loss: 5.1154 (5.1453)  acc1: 0.0000 (7.8151)  acc5: 12.5000 (17.7782)  time: 0.1185  data: 0.0001  max mem: 4645
[07:10:52.431377] Test:  [ 620/2194]  eta: 0:03:11  loss: 5.1106 (5.1472)  acc1: 0.0000 (7.7697)  acc5: 12.5000 (17.7939)  time: 0.1181  data: 0.0001  max mem: 4645
[07:10:53.608700] Test:  [ 630/2194]  eta: 0:03:09  loss: 5.0765 (5.1419)  acc1: 0.0000 (7.8447)  acc5: 25.0000 (17.9081)  time: 0.1177  data: 0.0001  max mem: 4645
[07:10:54.778988] Test:  [ 640/2194]  eta: 0:03:08  loss: 4.9877 (5.1386)  acc1: 12.5000 (7.8588)  acc5: 25.0000 (17.9797)  time: 0.1173  data: 0.0001  max mem: 4645
[07:10:55.950318] Test:  [ 650/2194]  eta: 0:03:07  loss: 5.2676 (5.1416)  acc1: 0.0000 (7.8725)  acc5: 12.5000 (17.9531)  time: 0.1170  data: 0.0001  max mem: 4645
[07:10:57.115746] Test:  [ 660/2194]  eta: 0:03:05  loss: 5.1123 (5.1362)  acc1: 0.0000 (7.8858)  acc5: 25.0000 (18.0787)  time: 0.1168  data: 0.0001  max mem: 4645
[07:10:58.280305] Test:  [ 670/2194]  eta: 0:03:04  loss: 4.9573 (5.1371)  acc1: 0.0000 (7.9359)  acc5: 25.0000 (18.1073)  time: 0.1164  data: 0.0001  max mem: 4645
[07:10:59.451048] Test:  [ 680/2194]  eta: 0:03:03  loss: 5.1135 (5.1389)  acc1: 12.5000 (7.9479)  acc5: 12.5000 (18.0617)  time: 0.1167  data: 0.0001  max mem: 4645
[07:11:00.617027] Test:  [ 690/2194]  eta: 0:03:01  loss: 5.0842 (5.1345)  acc1: 12.5000 (7.9957)  acc5: 12.5000 (18.1440)  time: 0.1168  data: 0.0001  max mem: 4645
[07:11:01.783157] Test:  [ 700/2194]  eta: 0:03:00  loss: 5.2316 (5.1395)  acc1: 0.0000 (7.9529)  acc5: 12.5000 (18.0635)  time: 0.1165  data: 0.0001  max mem: 4645
[07:11:02.953420] Test:  [ 710/2194]  eta: 0:02:59  loss: 5.2351 (5.1404)  acc1: 0.0000 (7.9641)  acc5: 12.5000 (18.0204)  time: 0.1167  data: 0.0001  max mem: 4645
[07:11:04.114420] Test:  [ 720/2194]  eta: 0:02:58  loss: 5.2161 (5.1402)  acc1: 0.0000 (8.0270)  acc5: 12.5000 (18.0479)  time: 0.1165  data: 0.0001  max mem: 4645
[07:11:05.276205] Test:  [ 730/2194]  eta: 0:02:56  loss: 5.3362 (5.1432)  acc1: 0.0000 (7.9514)  acc5: 12.5000 (17.9720)  time: 0.1161  data: 0.0001  max mem: 4645
[07:11:06.439333] Test:  [ 740/2194]  eta: 0:02:55  loss: 5.0959 (5.1430)  acc1: 0.0000 (7.9285)  acc5: 12.5000 (17.9656)  time: 0.1162  data: 0.0001  max mem: 4645
[07:11:07.601372] Test:  [ 750/2194]  eta: 0:02:54  loss: 4.9916 (5.1413)  acc1: 12.5000 (7.9394)  acc5: 12.5000 (17.9927)  time: 0.1162  data: 0.0001  max mem: 4645
[07:11:08.777651] Test:  [ 760/2194]  eta: 0:02:52  loss: 4.8879 (5.1363)  acc1: 12.5000 (8.0158)  acc5: 12.5000 (18.0519)  time: 0.1168  data: 0.0001  max mem: 4645
[07:11:09.941726] Test:  [ 770/2194]  eta: 0:02:51  loss: 4.8840 (5.1362)  acc1: 12.5000 (8.0253)  acc5: 25.0000 (18.0610)  time: 0.1169  data: 0.0001  max mem: 4645
[07:11:11.112701] Test:  [ 780/2194]  eta: 0:02:50  loss: 4.8748 (5.1343)  acc1: 12.5000 (8.0506)  acc5: 25.0000 (18.1018)  time: 0.1167  data: 0.0001  max mem: 4645
[07:11:12.282443] Test:  [ 790/2194]  eta: 0:02:49  loss: 4.7815 (5.1327)  acc1: 12.5000 (8.0910)  acc5: 25.0000 (18.1416)  time: 0.1170  data: 0.0001  max mem: 4645
[07:11:13.452396] Test:  [ 800/2194]  eta: 0:02:47  loss: 4.9714 (5.1323)  acc1: 0.0000 (8.0836)  acc5: 25.0000 (18.2116)  time: 0.1169  data: 0.0002  max mem: 4645
[07:11:14.627562] Test:  [ 810/2194]  eta: 0:02:46  loss: 5.1775 (5.1336)  acc1: 0.0000 (8.0456)  acc5: 12.5000 (18.1566)  time: 0.1172  data: 0.0001  max mem: 4645
[07:11:15.809001] Test:  [ 820/2194]  eta: 0:02:45  loss: 5.2109 (5.1343)  acc1: 0.0000 (8.0694)  acc5: 12.5000 (18.1638)  time: 0.1178  data: 0.0001  max mem: 4645
[07:11:16.986310] Test:  [ 830/2194]  eta: 0:02:44  loss: 5.3401 (5.1362)  acc1: 12.5000 (8.0776)  acc5: 12.5000 (18.1408)  time: 0.1179  data: 0.0001  max mem: 4645
[07:11:18.165513] Test:  [ 840/2194]  eta: 0:02:42  loss: 5.3257 (5.1368)  acc1: 0.0000 (8.0410)  acc5: 12.5000 (18.1034)  time: 0.1177  data: 0.0001  max mem: 4645
[07:11:19.344494] Test:  [ 850/2194]  eta: 0:02:41  loss: 5.2383 (5.1383)  acc1: 0.0000 (8.0347)  acc5: 12.5000 (18.0670)  time: 0.1178  data: 0.0001  max mem: 4645
[07:11:20.530178] Test:  [ 860/2194]  eta: 0:02:40  loss: 5.1935 (5.1360)  acc1: 12.5000 (8.0430)  acc5: 12.5000 (18.0894)  time: 0.1182  data: 0.0001  max mem: 4645
[07:11:21.709277] Test:  [ 870/2194]  eta: 0:02:39  loss: 5.0286 (5.1357)  acc1: 12.5000 (8.0224)  acc5: 12.5000 (18.0970)  time: 0.1182  data: 0.0001  max mem: 4645
[07:11:22.902874] Test:  [ 880/2194]  eta: 0:02:37  loss: 5.0360 (5.1362)  acc1: 0.0000 (8.0306)  acc5: 12.5000 (18.0760)  time: 0.1186  data: 0.0001  max mem: 4645
[07:11:24.094415] Test:  [ 890/2194]  eta: 0:02:36  loss: 5.0390 (5.1334)  acc1: 0.0000 (8.0387)  acc5: 12.5000 (18.1257)  time: 0.1192  data: 0.0001  max mem: 4645
[07:11:25.284205] Test:  [ 900/2194]  eta: 0:02:35  loss: 4.8801 (5.1322)  acc1: 0.0000 (8.0189)  acc5: 25.0000 (18.1465)  time: 0.1190  data: 0.0001  max mem: 4645
[07:11:26.486971] Test:  [ 910/2194]  eta: 0:02:34  loss: 4.8574 (5.1313)  acc1: 12.5000 (8.0681)  acc5: 25.0000 (18.1806)  time: 0.1195  data: 0.0001  max mem: 4645
[07:11:27.695818] Test:  [ 920/2194]  eta: 0:02:33  loss: 5.2194 (5.1328)  acc1: 0.0000 (8.0347)  acc5: 12.5000 (18.1460)  time: 0.1205  data: 0.0001  max mem: 4645
[07:11:28.900657] Test:  [ 930/2194]  eta: 0:02:31  loss: 4.7781 (5.1283)  acc1: 0.0000 (8.0961)  acc5: 12.5000 (18.2331)  time: 0.1206  data: 0.0001  max mem: 4645
[07:11:30.109813] Test:  [ 940/2194]  eta: 0:02:30  loss: 4.7935 (5.1299)  acc1: 0.0000 (8.0367)  acc5: 25.0000 (18.1987)  time: 0.1206  data: 0.0001  max mem: 4645
[07:11:31.321609] Test:  [ 950/2194]  eta: 0:02:29  loss: 4.9344 (5.1267)  acc1: 0.0000 (8.0442)  acc5: 12.5000 (18.2571)  time: 0.1210  data: 0.0001  max mem: 4645
[07:11:32.530087] Test:  [ 960/2194]  eta: 0:02:28  loss: 4.5587 (5.1225)  acc1: 12.5000 (8.0645)  acc5: 25.0000 (18.3663)  time: 0.1209  data: 0.0001  max mem: 4645
[07:11:33.740555] Test:  [ 970/2194]  eta: 0:02:27  loss: 4.6812 (5.1235)  acc1: 0.0000 (8.0458)  acc5: 25.0000 (18.3702)  time: 0.1209  data: 0.0001  max mem: 4645
[07:11:34.953400] Test:  [ 980/2194]  eta: 0:02:25  loss: 5.1720 (5.1245)  acc1: 0.0000 (8.0530)  acc5: 12.5000 (18.3231)  time: 0.1211  data: 0.0001  max mem: 4645
[07:11:36.164864] Test:  [ 990/2194]  eta: 0:02:24  loss: 5.2331 (5.1232)  acc1: 12.5000 (8.0853)  acc5: 12.5000 (18.3148)  time: 0.1211  data: 0.0001  max mem: 4645
[07:11:37.379851] Test:  [1000/2194]  eta: 0:02:23  loss: 4.8637 (5.1174)  acc1: 12.5000 (8.1918)  acc5: 25.0000 (18.4316)  time: 0.1212  data: 0.0001  max mem: 4645
[07:11:38.592918] Test:  [1010/2194]  eta: 0:02:22  loss: 5.0882 (5.1190)  acc1: 12.5000 (8.1850)  acc5: 12.5000 (18.4100)  time: 0.1213  data: 0.0001  max mem: 4645
[07:11:39.805244] Test:  [1020/2194]  eta: 0:02:21  loss: 5.1077 (5.1160)  acc1: 12.5000 (8.2150)  acc5: 12.5000 (18.4623)  time: 0.1212  data: 0.0001  max mem: 4645
[07:11:41.019981] Test:  [1030/2194]  eta: 0:02:19  loss: 4.8419 (5.1166)  acc1: 0.0000 (8.1838)  acc5: 12.5000 (18.4408)  time: 0.1213  data: 0.0001  max mem: 4645
[07:11:42.233728] Test:  [1040/2194]  eta: 0:02:18  loss: 4.9471 (5.1148)  acc1: 0.0000 (8.1772)  acc5: 12.5000 (18.4798)  time: 0.1213  data: 0.0001  max mem: 4645
[07:11:43.449434] Test:  [1050/2194]  eta: 0:02:17  loss: 4.9297 (5.1159)  acc1: 0.0000 (8.1351)  acc5: 12.5000 (18.4348)  time: 0.1214  data: 0.0001  max mem: 4645
[07:11:44.674162] Test:  [1060/2194]  eta: 0:02:16  loss: 5.1912 (5.1164)  acc1: 0.0000 (8.1056)  acc5: 12.5000 (18.3789)  time: 0.1219  data: 0.0001  max mem: 4645
[07:11:45.885380] Test:  [1070/2194]  eta: 0:02:15  loss: 5.2119 (5.1170)  acc1: 0.0000 (8.0649)  acc5: 12.5000 (18.3357)  time: 0.1217  data: 0.0001  max mem: 4645
[07:11:47.108520] Test:  [1080/2194]  eta: 0:02:14  loss: 5.2119 (5.1174)  acc1: 0.0000 (8.0481)  acc5: 12.5000 (18.3395)  time: 0.1216  data: 0.0001  max mem: 4645
[07:11:48.327991] Test:  [1090/2194]  eta: 0:02:12  loss: 5.0195 (5.1145)  acc1: 0.0000 (8.0775)  acc5: 25.0000 (18.3891)  time: 0.1220  data: 0.0001  max mem: 4645
[07:11:49.552553] Test:  [1100/2194]  eta: 0:02:11  loss: 5.0195 (5.1145)  acc1: 12.5000 (8.1290)  acc5: 25.0000 (18.3810)  time: 0.1221  data: 0.0001  max mem: 4645
[07:11:50.761267] Test:  [1110/2194]  eta: 0:02:10  loss: 5.1829 (5.1154)  acc1: 12.5000 (8.1233)  acc5: 12.5000 (18.3731)  time: 0.1216  data: 0.0001  max mem: 4645
[07:11:51.970285] Test:  [1120/2194]  eta: 0:02:09  loss: 5.1842 (5.1154)  acc1: 12.5000 (8.1735)  acc5: 12.5000 (18.3988)  time: 0.1208  data: 0.0001  max mem: 4645
[07:11:53.183276] Test:  [1130/2194]  eta: 0:02:08  loss: 4.9268 (5.1113)  acc1: 12.5000 (8.2560)  acc5: 25.0000 (18.5234)  time: 0.1210  data: 0.0001  max mem: 4645
[07:11:54.395959] Test:  [1140/2194]  eta: 0:02:06  loss: 5.4456 (5.1169)  acc1: 12.5000 (8.2384)  acc5: 12.5000 (18.4597)  time: 0.1212  data: 0.0001  max mem: 4645
[07:11:55.603735] Test:  [1150/2194]  eta: 0:02:05  loss: 5.5132 (5.1166)  acc1: 0.0000 (8.2320)  acc5: 12.5000 (18.4839)  time: 0.1209  data: 0.0001  max mem: 4645
[07:11:56.815499] Test:  [1160/2194]  eta: 0:02:04  loss: 4.8721 (5.1141)  acc1: 12.5000 (8.2580)  acc5: 25.0000 (18.5508)  time: 0.1209  data: 0.0001  max mem: 4645
[07:11:58.026800] Test:  [1170/2194]  eta: 0:02:03  loss: 4.8928 (5.1149)  acc1: 0.0000 (8.2088)  acc5: 12.5000 (18.4671)  time: 0.1211  data: 0.0001  max mem: 4645
[07:11:59.234363] Test:  [1180/2194]  eta: 0:02:02  loss: 5.1836 (5.1152)  acc1: 0.0000 (8.1499)  acc5: 12.5000 (18.4801)  time: 0.1209  data: 0.0001  max mem: 4645
[07:12:00.448805] Test:  [1190/2194]  eta: 0:02:00  loss: 5.2705 (5.1166)  acc1: 0.0000 (8.1654)  acc5: 12.5000 (18.5034)  time: 0.1210  data: 0.0001  max mem: 4645
[07:12:01.652895] Test:  [1200/2194]  eta: 0:01:59  loss: 5.0737 (5.1157)  acc1: 0.0000 (8.1703)  acc5: 25.0000 (18.5366)  time: 0.1208  data: 0.0001  max mem: 4645
[07:12:02.861940] Test:  [1210/2194]  eta: 0:01:58  loss: 5.0737 (5.1162)  acc1: 0.0000 (8.1647)  acc5: 12.5000 (18.5281)  time: 0.1206  data: 0.0001  max mem: 4645
[07:12:04.071536] Test:  [1220/2194]  eta: 0:01:57  loss: 4.9169 (5.1143)  acc1: 12.5000 (8.1593)  acc5: 12.5000 (18.5504)  time: 0.1209  data: 0.0001  max mem: 4645
[07:12:05.286543] Test:  [1230/2194]  eta: 0:01:56  loss: 4.9065 (5.1132)  acc1: 0.0000 (8.1742)  acc5: 12.5000 (18.5418)  time: 0.1212  data: 0.0001  max mem: 4645
[07:12:06.492810] Test:  [1240/2194]  eta: 0:01:54  loss: 5.2949 (5.1154)  acc1: 0.0000 (8.1688)  acc5: 12.5000 (18.5133)  time: 0.1210  data: 0.0001  max mem: 4645
[07:12:07.702608] Test:  [1250/2194]  eta: 0:01:53  loss: 5.2769 (5.1141)  acc1: 12.5000 (8.1934)  acc5: 12.5000 (18.5452)  time: 0.1207  data: 0.0001  max mem: 4645
[07:12:08.910781] Test:  [1260/2194]  eta: 0:01:52  loss: 4.8298 (5.1143)  acc1: 0.0000 (8.1681)  acc5: 25.0000 (18.5369)  time: 0.1208  data: 0.0002  max mem: 4645
[07:12:10.107072] Test:  [1270/2194]  eta: 0:01:51  loss: 5.2169 (5.1160)  acc1: 0.0000 (8.1629)  acc5: 12.5000 (18.5189)  time: 0.1201  data: 0.0001  max mem: 4645
[07:12:11.312082] Test:  [1280/2194]  eta: 0:01:50  loss: 5.3568 (5.1177)  acc1: 0.0000 (8.1284)  acc5: 12.5000 (18.4914)  time: 0.1200  data: 0.0001  max mem: 4645
[07:12:12.510050] Test:  [1290/2194]  eta: 0:01:48  loss: 4.7341 (5.1139)  acc1: 12.5000 (8.1816)  acc5: 25.0000 (18.6096)  time: 0.1201  data: 0.0001  max mem: 4645
[07:12:13.718307] Test:  [1300/2194]  eta: 0:01:47  loss: 4.7341 (5.1125)  acc1: 12.5000 (8.1956)  acc5: 25.0000 (18.6395)  time: 0.1202  data: 0.0001  max mem: 4645
[07:12:14.910105] Test:  [1310/2194]  eta: 0:01:46  loss: 5.0719 (5.1133)  acc1: 12.5000 (8.1998)  acc5: 12.5000 (18.6117)  time: 0.1199  data: 0.0001  max mem: 4645
[07:12:16.110282] Test:  [1320/2194]  eta: 0:01:45  loss: 5.3464 (5.1143)  acc1: 0.0000 (8.1851)  acc5: 12.5000 (18.5560)  time: 0.1195  data: 0.0001  max mem: 4645
[07:12:17.307798] Test:  [1330/2194]  eta: 0:01:44  loss: 5.2917 (5.1129)  acc1: 0.0000 (8.1987)  acc5: 12.5000 (18.5763)  time: 0.1198  data: 0.0001  max mem: 4645
[07:12:18.507934] Test:  [1340/2194]  eta: 0:01:42  loss: 4.9673 (5.1120)  acc1: 12.5000 (8.2028)  acc5: 12.5000 (18.5962)  time: 0.1198  data: 0.0001  max mem: 4645
[07:12:19.703127] Test:  [1350/2194]  eta: 0:01:41  loss: 5.0154 (5.1121)  acc1: 0.0000 (8.1976)  acc5: 12.5000 (18.5788)  time: 0.1197  data: 0.0001  max mem: 4645
[07:12:20.899337] Test:  [1360/2194]  eta: 0:01:40  loss: 5.0154 (5.1114)  acc1: 12.5000 (8.2109)  acc5: 12.5000 (18.5985)  time: 0.1195  data: 0.0001  max mem: 4645
[07:12:22.089928] Test:  [1370/2194]  eta: 0:01:39  loss: 4.9236 (5.1086)  acc1: 12.5000 (8.2330)  acc5: 25.0000 (18.6360)  time: 0.1193  data: 0.0001  max mem: 4645
[07:12:23.282308] Test:  [1380/2194]  eta: 0:01:37  loss: 5.1638 (5.1091)  acc1: 12.5000 (8.2277)  acc5: 12.5000 (18.6278)  time: 0.1191  data: 0.0001  max mem: 4645
[07:12:24.471558] Test:  [1390/2194]  eta: 0:01:36  loss: 5.1151 (5.1078)  acc1: 12.5000 (8.2495)  acc5: 12.5000 (18.6377)  time: 0.1190  data: 0.0001  max mem: 4645
[07:12:25.656637] Test:  [1400/2194]  eta: 0:01:35  loss: 5.1151 (5.1084)  acc1: 0.0000 (8.2352)  acc5: 25.0000 (18.6474)  time: 0.1186  data: 0.0001  max mem: 4645
[07:12:26.850562] Test:  [1410/2194]  eta: 0:01:34  loss: 5.2335 (5.1099)  acc1: 0.0000 (8.2123)  acc5: 12.5000 (18.5950)  time: 0.1189  data: 0.0001  max mem: 4645
[07:12:28.032818] Test:  [1420/2194]  eta: 0:01:33  loss: 5.2616 (5.1115)  acc1: 0.0000 (8.1809)  acc5: 12.5000 (18.5697)  time: 0.1187  data: 0.0001  max mem: 4645
[07:12:29.225686] Test:  [1430/2194]  eta: 0:01:31  loss: 5.1729 (5.1106)  acc1: 0.0000 (8.1936)  acc5: 12.5000 (18.6059)  time: 0.1187  data: 0.0001  max mem: 4645
[07:12:30.425293] Test:  [1440/2194]  eta: 0:01:30  loss: 4.9883 (5.1101)  acc1: 12.5000 (8.1974)  acc5: 25.0000 (18.6155)  time: 0.1196  data: 0.0001  max mem: 4645
[07:12:31.617065] Test:  [1450/2194]  eta: 0:01:29  loss: 4.8643 (5.1082)  acc1: 12.5000 (8.2099)  acc5: 25.0000 (18.6595)  time: 0.1195  data: 0.0001  max mem: 4645
[07:12:32.809449] Test:  [1460/2194]  eta: 0:01:28  loss: 5.0170 (5.1080)  acc1: 12.5000 (8.2050)  acc5: 25.0000 (18.6602)  time: 0.1191  data: 0.0001  max mem: 4645
[07:12:34.008488] Test:  [1470/2194]  eta: 0:01:27  loss: 5.1899 (5.1080)  acc1: 0.0000 (8.2002)  acc5: 25.0000 (18.6778)  time: 0.1195  data: 0.0001  max mem: 4645
[07:12:35.199884] Test:  [1480/2194]  eta: 0:01:25  loss: 5.1865 (5.1094)  acc1: 0.0000 (8.1955)  acc5: 12.5000 (18.6529)  time: 0.1194  data: 0.0001  max mem: 4645
[07:12:36.395564] Test:  [1490/2194]  eta: 0:01:24  loss: 5.1443 (5.1073)  acc1: 0.0000 (8.2243)  acc5: 12.5000 (18.6704)  time: 0.1193  data: 0.0001  max mem: 4645
[07:12:37.594074] Test:  [1500/2194]  eta: 0:01:23  loss: 5.3497 (5.1090)  acc1: 0.0000 (8.1945)  acc5: 12.5000 (18.6043)  time: 0.1196  data: 0.0002  max mem: 4645
[07:12:38.798236] Test:  [1510/2194]  eta: 0:01:22  loss: 5.3052 (5.1094)  acc1: 0.0000 (8.2065)  acc5: 12.5000 (18.5887)  time: 0.1201  data: 0.0001  max mem: 4645
[07:12:39.995334] Test:  [1520/2194]  eta: 0:01:21  loss: 5.2245 (5.1105)  acc1: 12.5000 (8.2018)  acc5: 12.5000 (18.5815)  time: 0.1200  data: 0.0002  max mem: 4645
[07:12:41.201578] Test:  [1530/2194]  eta: 0:01:19  loss: 5.1364 (5.1099)  acc1: 0.0000 (8.2054)  acc5: 12.5000 (18.5990)  time: 0.1201  data: 0.0001  max mem: 4645
[07:12:42.391252] Test:  [1540/2194]  eta: 0:01:18  loss: 5.0476 (5.1091)  acc1: 0.0000 (8.2008)  acc5: 12.5000 (18.6080)  time: 0.1197  data: 0.0001  max mem: 4645
[07:12:43.584902] Test:  [1550/2194]  eta: 0:01:17  loss: 4.8545 (5.1077)  acc1: 0.0000 (8.1883)  acc5: 12.5000 (18.6331)  time: 0.1191  data: 0.0001  max mem: 4645
[07:12:44.779366] Test:  [1560/2194]  eta: 0:01:16  loss: 4.8181 (5.1082)  acc1: 0.0000 (8.1999)  acc5: 25.0000 (18.6419)  time: 0.1193  data: 0.0001  max mem: 4645
[07:12:45.974897] Test:  [1570/2194]  eta: 0:01:15  loss: 5.1991 (5.1081)  acc1: 12.5000 (8.2272)  acc5: 25.0000 (18.6585)  time: 0.1194  data: 0.0001  max mem: 4645
[07:12:47.174957] Test:  [1580/2194]  eta: 0:01:13  loss: 5.2563 (5.1100)  acc1: 12.5000 (8.2306)  acc5: 12.5000 (18.6433)  time: 0.1197  data: 0.0001  max mem: 4645
[07:12:48.372474] Test:  [1590/2194]  eta: 0:01:12  loss: 5.0209 (5.1092)  acc1: 12.5000 (8.2260)  acc5: 25.0000 (18.6596)  time: 0.1198  data: 0.0001  max mem: 4645
[07:12:49.574090] Test:  [1600/2194]  eta: 0:01:11  loss: 5.0209 (5.1095)  acc1: 0.0000 (8.2214)  acc5: 25.0000 (18.6446)  time: 0.1199  data: 0.0001  max mem: 4645
[07:12:50.777192] Test:  [1610/2194]  eta: 0:01:10  loss: 5.1519 (5.1100)  acc1: 0.0000 (8.2092)  acc5: 12.5000 (18.6530)  time: 0.1202  data: 0.0001  max mem: 4645
[07:12:51.975501] Test:  [1620/2194]  eta: 0:01:09  loss: 5.4053 (5.1125)  acc1: 0.0000 (8.2048)  acc5: 12.5000 (18.6305)  time: 0.1200  data: 0.0001  max mem: 4645
[07:12:53.180018] Test:  [1630/2194]  eta: 0:01:07  loss: 5.1509 (5.1117)  acc1: 12.5000 (8.2311)  acc5: 12.5000 (18.6312)  time: 0.1200  data: 0.0001  max mem: 4645
[07:12:54.389457] Test:  [1640/2194]  eta: 0:01:06  loss: 5.1509 (5.1139)  acc1: 12.5000 (8.2267)  acc5: 12.5000 (18.6015)  time: 0.1206  data: 0.0001  max mem: 4645
[07:12:55.596697] Test:  [1650/2194]  eta: 0:01:05  loss: 5.3231 (5.1152)  acc1: 0.0000 (8.2223)  acc5: 12.5000 (18.5721)  time: 0.1208  data: 0.0001  max mem: 4645
[07:12:56.797974] Test:  [1660/2194]  eta: 0:01:04  loss: 5.2981 (5.1149)  acc1: 12.5000 (8.2405)  acc5: 12.5000 (18.5957)  time: 0.1204  data: 0.0001  max mem: 4645
[07:12:58.004005] Test:  [1670/2194]  eta: 0:01:03  loss: 5.5449 (5.1171)  acc1: 0.0000 (8.2211)  acc5: 25.0000 (18.5667)  time: 0.1203  data: 0.0001  max mem: 4645
[07:12:59.216338] Test:  [1680/2194]  eta: 0:01:01  loss: 5.4941 (5.1171)  acc1: 0.0000 (8.1945)  acc5: 12.5000 (18.5381)  time: 0.1208  data: 0.0001  max mem: 4645
[07:13:00.420740] Test:  [1690/2194]  eta: 0:01:00  loss: 4.8357 (5.1172)  acc1: 0.0000 (8.2052)  acc5: 12.5000 (18.5245)  time: 0.1208  data: 0.0001  max mem: 4645
[07:13:01.633018] Test:  [1700/2194]  eta: 0:00:59  loss: 4.9269 (5.1170)  acc1: 12.5000 (8.2305)  acc5: 25.0000 (18.5406)  time: 0.1208  data: 0.0001  max mem: 4645
[07:13:02.847056] Test:  [1710/2194]  eta: 0:00:58  loss: 5.1226 (5.1185)  acc1: 0.0000 (8.2043)  acc5: 12.5000 (18.4980)  time: 0.1212  data: 0.0001  max mem: 4645
[07:13:04.057871] Test:  [1720/2194]  eta: 0:00:57  loss: 5.1226 (5.1187)  acc1: 0.0000 (8.1929)  acc5: 12.5000 (18.4922)  time: 0.1212  data: 0.0001  max mem: 4645
[07:13:05.268376] Test:  [1730/2194]  eta: 0:00:55  loss: 4.9867 (5.1174)  acc1: 12.5000 (8.2250)  acc5: 25.0000 (18.5225)  time: 0.1210  data: 0.0001  max mem: 4645
[07:13:06.474522] Test:  [1740/2194]  eta: 0:00:54  loss: 5.0759 (5.1198)  acc1: 12.5000 (8.2065)  acc5: 12.5000 (18.4736)  time: 0.1207  data: 0.0001  max mem: 4645
[07:13:07.686410] Test:  [1750/2194]  eta: 0:00:53  loss: 5.2411 (5.1198)  acc1: 0.0000 (8.1953)  acc5: 12.5000 (18.4894)  time: 0.1208  data: 0.0002  max mem: 4645
[07:13:08.892981] Test:  [1760/2194]  eta: 0:00:52  loss: 5.0694 (5.1194)  acc1: 0.0000 (8.1914)  acc5: 12.5000 (18.5051)  time: 0.1208  data: 0.0002  max mem: 4645
[07:13:10.099837] Test:  [1770/2194]  eta: 0:00:50  loss: 4.9817 (5.1188)  acc1: 0.0000 (8.2016)  acc5: 12.5000 (18.5065)  time: 0.1206  data: 0.0001  max mem: 4645
[07:13:11.304549] Test:  [1780/2194]  eta: 0:00:49  loss: 5.2341 (5.1198)  acc1: 0.0000 (8.1906)  acc5: 12.5000 (18.5008)  time: 0.1205  data: 0.0001  max mem: 4645
[07:13:12.510059] Test:  [1790/2194]  eta: 0:00:48  loss: 5.2271 (5.1195)  acc1: 0.0000 (8.1937)  acc5: 12.5000 (18.5162)  time: 0.1204  data: 0.0001  max mem: 4645
[07:13:13.721052] Test:  [1800/2194]  eta: 0:00:47  loss: 4.9452 (5.1172)  acc1: 12.5000 (8.2246)  acc5: 25.0000 (18.5591)  time: 0.1208  data: 0.0001  max mem: 4645
[07:13:14.931803] Test:  [1810/2194]  eta: 0:00:46  loss: 4.6097 (5.1133)  acc1: 12.5000 (8.2827)  acc5: 25.0000 (18.6568)  time: 0.1210  data: 0.0001  max mem: 4645
[07:13:16.143883] Test:  [1820/2194]  eta: 0:00:44  loss: 4.7739 (5.1146)  acc1: 0.0000 (8.2510)  acc5: 25.0000 (18.6299)  time: 0.1211  data: 0.0001  max mem: 4645
[07:13:17.352953] Test:  [1830/2194]  eta: 0:00:43  loss: 5.4624 (5.1157)  acc1: 0.0000 (8.2332)  acc5: 12.5000 (18.5896)  time: 0.1210  data: 0.0001  max mem: 4645
[07:13:18.563811] Test:  [1840/2194]  eta: 0:00:42  loss: 5.4624 (5.1173)  acc1: 0.0000 (8.2292)  acc5: 12.5000 (18.5633)  time: 0.1209  data: 0.0001  max mem: 4645
[07:13:19.774116] Test:  [1850/2194]  eta: 0:00:41  loss: 5.0991 (5.1162)  acc1: 12.5000 (8.2523)  acc5: 12.5000 (18.5981)  time: 0.1210  data: 0.0001  max mem: 4645
[07:13:20.985190] Test:  [1860/2194]  eta: 0:00:40  loss: 5.0991 (5.1177)  acc1: 0.0000 (8.2348)  acc5: 12.5000 (18.5720)  time: 0.1210  data: 0.0001  max mem: 4645
[07:13:22.196043] Test:  [1870/2194]  eta: 0:00:38  loss: 5.2751 (5.1170)  acc1: 0.0000 (8.2643)  acc5: 12.5000 (18.5730)  time: 0.1210  data: 0.0001  max mem: 4645
[07:13:23.405921] Test:  [1880/2194]  eta: 0:00:37  loss: 5.0527 (5.1159)  acc1: 12.5000 (8.2802)  acc5: 12.5000 (18.5805)  time: 0.1210  data: 0.0001  max mem: 4645
[07:13:24.618144] Test:  [1890/2194]  eta: 0:00:36  loss: 5.0527 (5.1163)  acc1: 0.0000 (8.2694)  acc5: 25.0000 (18.5748)  time: 0.1210  data: 0.0001  max mem: 4645
[07:13:25.829950] Test:  [1900/2194]  eta: 0:00:35  loss: 5.1660 (5.1171)  acc1: 0.0000 (8.2457)  acc5: 12.5000 (18.5560)  time: 0.1211  data: 0.0001  max mem: 4645
[07:13:27.040711] Test:  [1910/2194]  eta: 0:00:34  loss: 5.0222 (5.1158)  acc1: 0.0000 (8.2745)  acc5: 12.5000 (18.5570)  time: 0.1211  data: 0.0001  max mem: 4645
[07:13:28.241262] Test:  [1920/2194]  eta: 0:00:32  loss: 4.8465 (5.1148)  acc1: 12.5000 (8.2769)  acc5: 12.5000 (18.5515)  time: 0.1205  data: 0.0001  max mem: 4645
[07:13:29.444508] Test:  [1930/2194]  eta: 0:00:31  loss: 4.9976 (5.1153)  acc1: 12.5000 (8.2729)  acc5: 12.5000 (18.5331)  time: 0.1201  data: 0.0001  max mem: 4645
[07:13:30.650201] Test:  [1940/2194]  eta: 0:00:30  loss: 4.7457 (5.1133)  acc1: 12.5000 (8.2947)  acc5: 12.5000 (18.5729)  time: 0.1204  data: 0.0001  max mem: 4645
[07:13:31.860078] Test:  [1950/2194]  eta: 0:00:29  loss: 5.0662 (5.1133)  acc1: 0.0000 (8.2842)  acc5: 12.5000 (18.5866)  time: 0.1207  data: 0.0001  max mem: 4645
[07:13:33.074211] Test:  [1960/2194]  eta: 0:00:28  loss: 5.2791 (5.1117)  acc1: 0.0000 (8.3057)  acc5: 12.5000 (18.6002)  time: 0.1211  data: 0.0001  max mem: 4645
[07:13:34.269667] Test:  [1970/2194]  eta: 0:00:26  loss: 5.2647 (5.1122)  acc1: 0.0000 (8.2889)  acc5: 25.0000 (18.6010)  time: 0.1204  data: 0.0001  max mem: 4645
[07:13:35.479174] Test:  [1980/2194]  eta: 0:00:25  loss: 5.0757 (5.1107)  acc1: 0.0000 (8.2976)  acc5: 25.0000 (18.6459)  time: 0.1202  data: 0.0001  max mem: 4645
[07:13:36.681939] Test:  [1990/2194]  eta: 0:00:24  loss: 4.9695 (5.1101)  acc1: 12.5000 (8.2998)  acc5: 25.0000 (18.6590)  time: 0.1205  data: 0.0001  max mem: 4645
[07:13:37.886243] Test:  [2000/2194]  eta: 0:00:23  loss: 5.1714 (5.1105)  acc1: 0.0000 (8.2834)  acc5: 12.5000 (18.6469)  time: 0.1203  data: 0.0001  max mem: 4645
[07:13:39.089075] Test:  [2010/2194]  eta: 0:00:22  loss: 5.2178 (5.1107)  acc1: 12.5000 (8.2981)  acc5: 12.5000 (18.6474)  time: 0.1203  data: 0.0001  max mem: 4645
[07:13:40.298962] Test:  [2020/2194]  eta: 0:00:20  loss: 5.0856 (5.1099)  acc1: 12.5000 (8.3003)  acc5: 25.0000 (18.6541)  time: 0.1206  data: 0.0001  max mem: 4645
[07:13:41.497815] Test:  [2030/2194]  eta: 0:00:19  loss: 5.1850 (5.1121)  acc1: 0.0000 (8.2841)  acc5: 12.5000 (18.6177)  time: 0.1204  data: 0.0001  max mem: 4645
[07:13:42.706749] Test:  [2040/2194]  eta: 0:00:18  loss: 5.2603 (5.1115)  acc1: 0.0000 (8.2864)  acc5: 12.5000 (18.6306)  time: 0.1203  data: 0.0001  max mem: 4645
[07:13:43.914460] Test:  [2050/2194]  eta: 0:00:17  loss: 4.9886 (5.1120)  acc1: 0.0000 (8.2886)  acc5: 25.0000 (18.6251)  time: 0.1208  data: 0.0001  max mem: 4645
[07:13:45.120410] Test:  [2060/2194]  eta: 0:00:16  loss: 4.9886 (5.1114)  acc1: 0.0000 (8.2727)  acc5: 25.0000 (18.6378)  time: 0.1206  data: 0.0001  max mem: 4645
[07:13:46.328269] Test:  [2070/2194]  eta: 0:00:14  loss: 4.9666 (5.1113)  acc1: 0.0000 (8.2690)  acc5: 12.5000 (18.6323)  time: 0.1206  data: 0.0001  max mem: 4645
[07:13:47.527821] Test:  [2080/2194]  eta: 0:00:13  loss: 5.2363 (5.1132)  acc1: 0.0000 (8.2472)  acc5: 12.5000 (18.5968)  time: 0.1203  data: 0.0001  max mem: 4645
[07:13:48.730661] Test:  [2090/2194]  eta: 0:00:12  loss: 5.1450 (5.1117)  acc1: 0.0000 (8.2556)  acc5: 12.5000 (18.6275)  time: 0.1200  data: 0.0001  max mem: 4645
[07:13:49.934175] Test:  [2100/2194]  eta: 0:00:11  loss: 4.9531 (5.1119)  acc1: 0.0000 (8.2580)  acc5: 25.0000 (18.6161)  time: 0.1202  data: 0.0001  max mem: 4645
[07:13:51.138577] Test:  [2110/2194]  eta: 0:00:10  loss: 5.0936 (5.1130)  acc1: 0.0000 (8.2366)  acc5: 12.5000 (18.5872)  time: 0.1203  data: 0.0001  max mem: 4645
[07:13:52.346219] Test:  [2120/2194]  eta: 0:00:08  loss: 5.2781 (5.1138)  acc1: 0.0000 (8.2037)  acc5: 12.5000 (18.5702)  time: 0.1205  data: 0.0001  max mem: 4645
[07:13:53.552959] Test:  [2130/2194]  eta: 0:00:07  loss: 5.4133 (5.1148)  acc1: 0.0000 (8.1828)  acc5: 12.5000 (18.5476)  time: 0.1206  data: 0.0001  max mem: 4645
[07:13:54.755926] Test:  [2140/2194]  eta: 0:00:06  loss: 5.5698 (5.1166)  acc1: 0.0000 (8.1679)  acc5: 12.5000 (18.4960)  time: 0.1204  data: 0.0001  max mem: 4645
[07:13:55.961394] Test:  [2150/2194]  eta: 0:00:05  loss: 5.2910 (5.1168)  acc1: 0.0000 (8.1706)  acc5: 12.5000 (18.4914)  time: 0.1203  data: 0.0001  max mem: 4645
[07:13:57.166481] Test:  [2160/2194]  eta: 0:00:04  loss: 5.1547 (5.1187)  acc1: 0.0000 (8.1559)  acc5: 12.5000 (18.4637)  time: 0.1205  data: 0.0001  max mem: 4645
[07:13:58.377299] Test:  [2170/2194]  eta: 0:00:02  loss: 5.3182 (5.1194)  acc1: 0.0000 (8.1529)  acc5: 12.5000 (18.4477)  time: 0.1207  data: 0.0001  max mem: 4645
[07:13:59.574584] Test:  [2180/2194]  eta: 0:00:01  loss: 5.1928 (5.1196)  acc1: 0.0000 (8.1385)  acc5: 12.5000 (18.4434)  time: 0.1203  data: 0.0004  max mem: 4645
[07:14:00.783216] Test:  [2190/2194]  eta: 0:00:00  loss: 5.2043 (5.1211)  acc1: 0.0000 (8.1413)  acc5: 12.5000 (18.4220)  time: 0.1202  data: 0.0004  max mem: 4645
[07:14:01.134201] Test:  [2193/2194]  eta: 0:00:00  loss: 4.9340 (5.1202)  acc1: 0.0000 (8.1481)  acc5: 12.5000 (18.4387)  time: 0.1199  data: 0.0004  max mem: 4645
[07:14:01.185230] Test: Total time: 0:04:24 (0.1204 s / it)
[07:14:01.185313] * Acc@1 8.148 Acc@5 18.439 loss 5.120
[07:14:01.185462] Evaluation on 17550 test images- acc1: 8.15%, acc5: 18.44%
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
