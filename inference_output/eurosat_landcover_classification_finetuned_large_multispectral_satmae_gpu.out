/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
Not using distributed mode
[11:51:13.360226] job dir: /home/ada/satmae/SatMAE
[11:51:13.360312] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
batch_size=8,
blr=0.001,
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
dataset_type='euro_sat',
device='cuda',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=False,
drop_path=0.1,
dropped_bands=None,
epochs=50,
eval=True,
finetune='',
global_pool=True,
grouped_bands=[],
input_size=96,
layer_decay=0.75,
local_rank=0,
log_dir='/home/ada/satmae/other_data/eurosat/evaluation/sentinel',
lr=None,
masked_bands=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_large_patch16',
model_type='group_c',
nb_classes=62,
num_workers=8,
output_dir='/home/ada/satmae/other_data/eurosat/evaluation/sentinel',
patch_size=8,
pin_mem=True,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='/home/ada/satmae/multispectral/checkpoints/finetune-vit-large-e7.pth',
save_every=1,
seed=0,
smoothing=0.1,
start_epoch=0,
test_path='/home/ada/satmae/other_data/eurosat/val_ms.txt',
train_path='/home/ada/satmae/other_data/eurosat/train_ms.txt',
wandb=None,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[11:51:13.370105] <util.datasets.EuroSat object at 0x7eff95f0b690>
[11:51:13.386987] <util.datasets.EuroSat object at 0x7eff95d72f90>
[11:51:13.387053] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7eff95d72fd0>
[11:51:13.387211] Grouping bands [[0, 1, 2, 6], [3, 4, 5, 7], [8, 9]]
[11:51:23.541036] Model = GroupChannelsVisionTransformer(
  (patch_embed): ModuleList(
    (0): PatchEmbed(
      (proj): Conv2d(4, 1024, kernel_size=(8, 8), stride=(8, 8))
    )
    (1): PatchEmbed(
      (proj): Conv2d(4, 1024, kernel_size=(8, 8), stride=(8, 8))
    )
    (2): PatchEmbed(
      (proj): Conv2d(2, 1024, kernel_size=(8, 8), stride=(8, 8))
    )
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (head): Linear(in_features=1024, out_features=62, bias=True)
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
)
[11:51:23.541120] number of params (M): 303.15
[11:51:23.541141] base lr: 1.00e-03
[11:51:23.541155] actual lr: 3.13e-05
[11:51:23.541168] accumulate grad iterations: 1
[11:51:23.541180] effective batch size: 8
[11:51:23.545517] criterion = LabelSmoothingCrossEntropy()
[11:51:25.737804] Resume checkpoint /home/ada/satmae/multispectral/checkpoints/finetune-vit-large-e7.pth
[11:51:26.550491] Test:  [   0/2194]  eta: 0:29:35  loss: 3.5850 (3.5850)  acc1: 0.0000 (0.0000)  acc5: 50.0000 (50.0000)  time: 0.8091  data: 0.3640  max mem: 4626
[11:51:29.225892] Test:  [  10/2194]  eta: 0:11:31  loss: 4.3867 (4.4118)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.1818)  time: 0.3167  data: 0.0332  max mem: 4626
[11:51:31.892190] Test:  [  20/2194]  eta: 0:10:36  loss: 4.5500 (4.5399)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (16.0714)  time: 0.2670  data: 0.0001  max mem: 4626
[11:51:34.576452] Test:  [  30/2194]  eta: 0:10:16  loss: 4.5669 (4.5149)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (17.3387)  time: 0.2674  data: 0.0001  max mem: 4626
[11:51:37.280719] Test:  [  40/2194]  eta: 0:10:06  loss: 4.4492 (4.4685)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.5976)  time: 0.2693  data: 0.0001  max mem: 4626
[11:51:40.011107] Test:  [  50/2194]  eta: 0:09:59  loss: 4.3965 (4.4757)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.1373)  time: 0.2717  data: 0.0001  max mem: 4626
[11:51:42.757399] Test:  [  60/2194]  eta: 0:09:55  loss: 4.4426 (4.4729)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.0328)  time: 0.2737  data: 0.0002  max mem: 4626
[11:51:45.523371] Test:  [  70/2194]  eta: 0:09:51  loss: 4.4019 (4.4542)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (19.1901)  time: 0.2755  data: 0.0002  max mem: 4626
[11:51:48.327630] Test:  [  80/2194]  eta: 0:09:49  loss: 4.4019 (4.4580)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (19.2901)  time: 0.2784  data: 0.0001  max mem: 4626
[11:51:51.159860] Test:  [  90/2194]  eta: 0:09:47  loss: 4.5388 (4.4617)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (19.2308)  time: 0.2817  data: 0.0001  max mem: 4626
[11:51:54.001178] Test:  [ 100/2194]  eta: 0:09:45  loss: 4.5012 (4.4656)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.9356)  time: 0.2836  data: 0.0002  max mem: 4626
[11:51:56.884741] Test:  [ 110/2194]  eta: 0:09:44  loss: 4.3325 (4.4508)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (19.4820)  time: 0.2862  data: 0.0002  max mem: 4626
[11:51:59.788216] Test:  [ 120/2194]  eta: 0:09:43  loss: 4.4221 (4.4561)  acc1: 0.0000 (0.0000)  acc5: 25.0000 (19.3182)  time: 0.2893  data: 0.0002  max mem: 4626
[11:52:02.672806] Test:  [ 130/2194]  eta: 0:09:41  loss: 4.4617 (4.4488)  acc1: 0.0000 (0.0000)  acc5: 25.0000 (19.4656)  time: 0.2893  data: 0.0002  max mem: 4626
[11:52:05.552917] Test:  [ 140/2194]  eta: 0:09:39  loss: 4.3457 (4.4478)  acc1: 0.0000 (0.0000)  acc5: 25.0000 (19.5035)  time: 0.2881  data: 0.0002  max mem: 4626
[11:52:08.466263] Test:  [ 150/2194]  eta: 0:09:38  loss: 4.6292 (4.4538)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (19.4536)  time: 0.2896  data: 0.0002  max mem: 4626
[11:52:11.396923] Test:  [ 160/2194]  eta: 0:09:36  loss: 4.4985 (4.4515)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (19.1770)  time: 0.2921  data: 0.0002  max mem: 4626
[11:52:14.346452] Test:  [ 170/2194]  eta: 0:09:35  loss: 4.4253 (4.4564)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.7135)  time: 0.2939  data: 0.0002  max mem: 4626
[11:52:17.326099] Test:  [ 180/2194]  eta: 0:09:33  loss: 4.6111 (4.4608)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.5773)  time: 0.2964  data: 0.0002  max mem: 4626
[11:52:20.346365] Test:  [ 190/2194]  eta: 0:09:32  loss: 4.5576 (4.4580)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.7173)  time: 0.2999  data: 0.0001  max mem: 4626
[11:52:23.385300] Test:  [ 200/2194]  eta: 0:09:31  loss: 4.4214 (4.4546)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.7189)  time: 0.3029  data: 0.0002  max mem: 4626
[11:52:26.454089] Test:  [ 210/2194]  eta: 0:09:30  loss: 4.4321 (4.4674)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.3649)  time: 0.3053  data: 0.0002  max mem: 4626
[11:52:29.543289] Test:  [ 220/2194]  eta: 0:09:29  loss: 4.4163 (4.4610)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.6652)  time: 0.3078  data: 0.0002  max mem: 4626
[11:52:32.678187] Test:  [ 230/2194]  eta: 0:09:28  loss: 4.3103 (4.4597)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.6147)  time: 0.3111  data: 0.0002  max mem: 4626
[11:52:35.831993] Test:  [ 240/2194]  eta: 0:09:28  loss: 4.5906 (4.4693)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.3610)  time: 0.3144  data: 0.0002  max mem: 4626
[11:52:39.011232] Test:  [ 250/2194]  eta: 0:09:27  loss: 4.5906 (4.4647)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.5259)  time: 0.3166  data: 0.0002  max mem: 4626
[11:52:42.229435] Test:  [ 260/2194]  eta: 0:09:26  loss: 4.4343 (4.4693)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.3908)  time: 0.3198  data: 0.0002  max mem: 4626
[11:52:45.463630] Test:  [ 270/2194]  eta: 0:09:25  loss: 4.5403 (4.4733)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.2196)  time: 0.3225  data: 0.0002  max mem: 4626
[11:52:48.713819] Test:  [ 280/2194]  eta: 0:09:24  loss: 4.3840 (4.4652)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.5053)  time: 0.3241  data: 0.0002  max mem: 4626
[11:52:51.981354] Test:  [ 290/2194]  eta: 0:09:24  loss: 4.3398 (4.4645)  acc1: 0.0000 (0.0000)  acc5: 25.0000 (18.5997)  time: 0.3258  data: 0.0002  max mem: 4626
[11:52:55.262483] Test:  [ 300/2194]  eta: 0:09:23  loss: 4.2820 (4.4610)  acc1: 0.0000 (0.0000)  acc5: 25.0000 (18.6877)  time: 0.3273  data: 0.0002  max mem: 4626
[11:52:58.514146] Test:  [ 310/2194]  eta: 0:09:21  loss: 4.4148 (4.4647)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.6495)  time: 0.3265  data: 0.0002  max mem: 4626
[11:53:01.732833] Test:  [ 320/2194]  eta: 0:09:20  loss: 4.5403 (4.4661)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.4579)  time: 0.3234  data: 0.0002  max mem: 4626
[11:53:04.942741] Test:  [ 330/2194]  eta: 0:09:18  loss: 4.5183 (4.4682)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.5045)  time: 0.3213  data: 0.0002  max mem: 4626
[11:53:08.129531] Test:  [ 340/2194]  eta: 0:09:16  loss: 4.4968 (4.4719)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.2918)  time: 0.3198  data: 0.0002  max mem: 4626
[11:53:11.284615] Test:  [ 350/2194]  eta: 0:09:14  loss: 4.3093 (4.4654)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.4829)  time: 0.3170  data: 0.0002  max mem: 4626
[11:53:14.422321] Test:  [ 360/2194]  eta: 0:09:11  loss: 4.3093 (4.4664)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.3864)  time: 0.3146  data: 0.0002  max mem: 4626
[11:53:17.547957] Test:  [ 370/2194]  eta: 0:09:09  loss: 4.2373 (4.4573)  acc1: 0.0000 (0.0000)  acc5: 25.0000 (18.7332)  time: 0.3131  data: 0.0002  max mem: 4626
[11:53:20.656992] Test:  [ 380/2194]  eta: 0:09:06  loss: 4.2373 (4.4568)  acc1: 0.0000 (0.0000)  acc5: 25.0000 (18.7336)  time: 0.3116  data: 0.0001  max mem: 4626
[11:53:23.752818] Test:  [ 390/2194]  eta: 0:09:04  loss: 4.3914 (4.4600)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.6381)  time: 0.3101  data: 0.0002  max mem: 4626
[11:53:26.833014] Test:  [ 400/2194]  eta: 0:09:01  loss: 4.5193 (4.4614)  acc1: 0.0000 (0.0000)  acc5: 12.5000 (18.5786)  time: 0.3087  data: 0.0002  max mem: 4626
[11:53:29.909306] Test:  [ 410/2194]  eta: 0:08:58  loss: 4.5708 (4.4640)  acc1: 0.0000 (0.0000)  acc5: 25.0000 (18.6131)  time: 0.3077  data: 0.0002  max mem: 4626
[11:53:32.984595] Test:  [ 420/2194]  eta: 0:08:56  loss: 4.5708 (4.4682)  acc1: 0.0000 (0.0297)  acc5: 12.5000 (18.4976)  time: 0.3075  data: 0.0002  max mem: 4626
[11:53:36.055308] Test:  [ 430/2194]  eta: 0:08:53  loss: 4.5000 (4.4671)  acc1: 0.0000 (0.0580)  acc5: 12.5000 (18.4165)  time: 0.3072  data: 0.0002  max mem: 4626
[11:53:39.130099] Test:  [ 440/2194]  eta: 0:08:50  loss: 4.4480 (4.4674)  acc1: 0.0000 (0.0567)  acc5: 12.5000 (18.2823)  time: 0.3072  data: 0.0002  max mem: 4626
[11:53:42.204495] Test:  [ 450/2194]  eta: 0:08:47  loss: 4.6074 (4.4720)  acc1: 0.0000 (0.0554)  acc5: 12.5000 (18.0987)  time: 0.3074  data: 0.0001  max mem: 4626
[11:53:45.284096] Test:  [ 460/2194]  eta: 0:08:44  loss: 4.6074 (4.4762)  acc1: 0.0000 (0.0542)  acc5: 12.5000 (18.1128)  time: 0.3076  data: 0.0001  max mem: 4626
[11:53:48.367127] Test:  [ 470/2194]  eta: 0:08:41  loss: 4.5491 (4.4770)  acc1: 0.0000 (0.0531)  acc5: 12.5000 (18.0732)  time: 0.3080  data: 0.0002  max mem: 4626
[11:53:51.473515] Test:  [ 480/2194]  eta: 0:08:39  loss: 4.4819 (4.4770)  acc1: 0.0000 (0.0520)  acc5: 12.5000 (18.0613)  time: 0.3094  data: 0.0002  max mem: 4626
[11:53:54.580741] Test:  [ 490/2194]  eta: 0:08:36  loss: 4.4819 (4.4769)  acc1: 0.0000 (0.0509)  acc5: 12.5000 (18.1263)  time: 0.3106  data: 0.0002  max mem: 4626
[11:53:57.698941] Test:  [ 500/2194]  eta: 0:08:33  loss: 4.5579 (4.4775)  acc1: 0.0000 (0.0499)  acc5: 12.5000 (18.1886)  time: 0.3112  data: 0.0001  max mem: 4626
[11:54:00.834973] Test:  [ 510/2194]  eta: 0:08:30  loss: 4.6965 (4.4814)  acc1: 0.0000 (0.0489)  acc5: 12.5000 (18.0528)  time: 0.3126  data: 0.0001  max mem: 4626
[11:54:03.980660] Test:  [ 520/2194]  eta: 0:08:28  loss: 4.6370 (4.4835)  acc1: 0.0000 (0.0480)  acc5: 12.5000 (17.9942)  time: 0.3140  data: 0.0002  max mem: 4626
[11:54:07.137850] Test:  [ 530/2194]  eta: 0:08:25  loss: 4.5154 (4.4828)  acc1: 0.0000 (0.0471)  acc5: 12.5000 (18.0085)  time: 0.3151  data: 0.0002  max mem: 4626
[11:54:10.302711] Test:  [ 540/2194]  eta: 0:08:22  loss: 4.4275 (4.4827)  acc1: 0.0000 (0.0462)  acc5: 12.5000 (18.0453)  time: 0.3160  data: 0.0002  max mem: 4626
[11:54:13.479220] Test:  [ 550/2194]  eta: 0:08:20  loss: 4.4236 (4.4833)  acc1: 0.0000 (0.0454)  acc5: 12.5000 (18.0354)  time: 0.3170  data: 0.0002  max mem: 4626
[11:54:16.660031] Test:  [ 560/2194]  eta: 0:08:17  loss: 4.2915 (4.4814)  acc1: 0.0000 (0.0446)  acc5: 12.5000 (18.0481)  time: 0.3178  data: 0.0002  max mem: 4626
[11:54:19.842410] Test:  [ 570/2194]  eta: 0:08:15  loss: 4.4983 (4.4835)  acc1: 0.0000 (0.0438)  acc5: 12.5000 (17.9947)  time: 0.3181  data: 0.0002  max mem: 4626
[11:54:23.024457] Test:  [ 580/2194]  eta: 0:08:12  loss: 4.6531 (4.4874)  acc1: 0.0000 (0.0430)  acc5: 12.5000 (17.8787)  time: 0.3181  data: 0.0002  max mem: 4626
[11:54:26.201139] Test:  [ 590/2194]  eta: 0:08:09  loss: 4.7009 (4.4876)  acc1: 0.0000 (0.0423)  acc5: 12.5000 (17.8511)  time: 0.3179  data: 0.0002  max mem: 4626
[11:54:29.370034] Test:  [ 600/2194]  eta: 0:08:06  loss: 4.4358 (4.4861)  acc1: 0.0000 (0.0416)  acc5: 12.5000 (17.8869)  time: 0.3172  data: 0.0002  max mem: 4626
[11:54:32.544129] Test:  [ 610/2194]  eta: 0:08:04  loss: 4.5750 (4.4896)  acc1: 0.0000 (0.0409)  acc5: 12.5000 (17.7578)  time: 0.3171  data: 0.0002  max mem: 4626
[11:54:35.697556] Test:  [ 620/2194]  eta: 0:08:01  loss: 4.6404 (4.4917)  acc1: 0.0000 (0.0403)  acc5: 12.5000 (17.7134)  time: 0.3163  data: 0.0002  max mem: 4626
[11:54:38.852431] Test:  [ 630/2194]  eta: 0:07:58  loss: 4.4841 (4.4919)  acc1: 0.0000 (0.0396)  acc5: 12.5000 (17.6902)  time: 0.3153  data: 0.0002  max mem: 4626
[11:54:42.002234] Test:  [ 640/2194]  eta: 0:07:55  loss: 4.4878 (4.4933)  acc1: 0.0000 (0.0390)  acc5: 12.5000 (17.6677)  time: 0.3152  data: 0.0002  max mem: 4626
[11:54:45.147345] Test:  [ 650/2194]  eta: 0:07:52  loss: 4.6072 (4.4961)  acc1: 0.0000 (0.0384)  acc5: 12.5000 (17.5499)  time: 0.3147  data: 0.0001  max mem: 4626
[11:54:48.285537] Test:  [ 660/2194]  eta: 0:07:49  loss: 4.5640 (4.4968)  acc1: 0.0000 (0.0378)  acc5: 12.5000 (17.5303)  time: 0.3141  data: 0.0002  max mem: 4626
[11:54:51.412716] Test:  [ 670/2194]  eta: 0:07:46  loss: 4.5640 (4.4984)  acc1: 0.0000 (0.0373)  acc5: 12.5000 (17.4925)  time: 0.3132  data: 0.0002  max mem: 4626
[11:54:54.537236] Test:  [ 680/2194]  eta: 0:07:44  loss: 4.5803 (4.4985)  acc1: 0.0000 (0.0367)  acc5: 12.5000 (17.4559)  time: 0.3125  data: 0.0002  max mem: 4626
[11:54:57.662285] Test:  [ 690/2194]  eta: 0:07:41  loss: 4.6460 (4.5015)  acc1: 0.0000 (0.0362)  acc5: 12.5000 (17.3842)  time: 0.3124  data: 0.0002  max mem: 4626
[11:55:00.784256] Test:  [ 700/2194]  eta: 0:07:38  loss: 4.6326 (4.4999)  acc1: 0.0000 (0.0357)  acc5: 12.5000 (17.4215)  time: 0.3123  data: 0.0001  max mem: 4626
[11:55:03.905628] Test:  [ 710/2194]  eta: 0:07:35  loss: 4.4460 (4.5008)  acc1: 0.0000 (0.0352)  acc5: 12.5000 (17.4051)  time: 0.3121  data: 0.0002  max mem: 4626
[11:55:07.023618] Test:  [ 720/2194]  eta: 0:07:32  loss: 4.6008 (4.5023)  acc1: 0.0000 (0.0347)  acc5: 12.5000 (17.4064)  time: 0.3119  data: 0.0002  max mem: 4626
[11:55:10.147672] Test:  [ 730/2194]  eta: 0:07:29  loss: 4.3743 (4.4984)  acc1: 0.0000 (0.0342)  acc5: 25.0000 (17.5274)  time: 0.3120  data: 0.0002  max mem: 4626
[11:55:13.276073] Test:  [ 740/2194]  eta: 0:07:26  loss: 4.1509 (4.4959)  acc1: 0.0000 (0.0337)  acc5: 25.0000 (17.5945)  time: 0.3125  data: 0.0002  max mem: 4626
[11:55:16.403669] Test:  [ 750/2194]  eta: 0:07:23  loss: 4.4587 (4.4969)  acc1: 0.0000 (0.0333)  acc5: 12.5000 (17.5599)  time: 0.3127  data: 0.0002  max mem: 4626
[11:55:19.533161] Test:  [ 760/2194]  eta: 0:07:20  loss: 4.5879 (4.4974)  acc1: 0.0000 (0.0329)  acc5: 12.5000 (17.5099)  time: 0.3128  data: 0.0002  max mem: 4626
[11:55:22.665545] Test:  [ 770/2194]  eta: 0:07:17  loss: 4.4976 (4.4963)  acc1: 0.0000 (0.0324)  acc5: 12.5000 (17.5422)  time: 0.3130  data: 0.0002  max mem: 4626
[11:55:25.799669] Test:  [ 780/2194]  eta: 0:07:14  loss: 4.5027 (4.4974)  acc1: 0.0000 (0.0320)  acc5: 12.5000 (17.4936)  time: 0.3132  data: 0.0001  max mem: 4626
[11:55:28.935877] Test:  [ 790/2194]  eta: 0:07:11  loss: 4.5439 (4.4979)  acc1: 0.0000 (0.0316)  acc5: 12.5000 (17.4621)  time: 0.3134  data: 0.0002  max mem: 4626
[11:55:32.068670] Test:  [ 800/2194]  eta: 0:07:08  loss: 4.5430 (4.4983)  acc1: 0.0000 (0.0312)  acc5: 12.5000 (17.4157)  time: 0.3134  data: 0.0002  max mem: 4626
[11:55:35.206412] Test:  [ 810/2194]  eta: 0:07:05  loss: 4.5430 (4.4995)  acc1: 0.0000 (0.0308)  acc5: 12.5000 (17.3859)  time: 0.3134  data: 0.0001  max mem: 4626
[11:55:38.337228] Test:  [ 820/2194]  eta: 0:07:02  loss: 4.4717 (4.4991)  acc1: 0.0000 (0.0305)  acc5: 12.5000 (17.3873)  time: 0.3133  data: 0.0002  max mem: 4626
[11:55:41.477490] Test:  [ 830/2194]  eta: 0:06:59  loss: 4.4827 (4.4983)  acc1: 0.0000 (0.0301)  acc5: 12.5000 (17.4037)  time: 0.3135  data: 0.0002  max mem: 4626
[11:55:44.608834] Test:  [ 840/2194]  eta: 0:06:56  loss: 4.4448 (4.4952)  acc1: 0.0000 (0.0297)  acc5: 12.5000 (17.4792)  time: 0.3135  data: 0.0002  max mem: 4626
[11:55:47.745805] Test:  [ 850/2194]  eta: 0:06:53  loss: 4.4224 (4.4942)  acc1: 0.0000 (0.0294)  acc5: 25.0000 (17.5382)  time: 0.3133  data: 0.0002  max mem: 4626
[11:55:50.887679] Test:  [ 860/2194]  eta: 0:06:50  loss: 4.3691 (4.4934)  acc1: 0.0000 (0.0290)  acc5: 12.5000 (17.5523)  time: 0.3139  data: 0.0002  max mem: 4626
[11:55:54.030568] Test:  [ 870/2194]  eta: 0:06:47  loss: 4.3010 (4.4911)  acc1: 0.0000 (0.0287)  acc5: 25.0000 (17.6665)  time: 0.3141  data: 0.0002  max mem: 4626
[11:55:57.172365] Test:  [ 880/2194]  eta: 0:06:44  loss: 4.3010 (4.4902)  acc1: 0.0000 (0.0284)  acc5: 25.0000 (17.6930)  time: 0.3141  data: 0.0002  max mem: 4626
[11:56:00.319796] Test:  [ 890/2194]  eta: 0:06:41  loss: 4.4272 (4.4883)  acc1: 0.0000 (0.0281)  acc5: 25.0000 (17.7609)  time: 0.3144  data: 0.0002  max mem: 4626
[11:56:03.474749] Test:  [ 900/2194]  eta: 0:06:38  loss: 4.2834 (4.4864)  acc1: 0.0000 (0.0277)  acc5: 12.5000 (17.8135)  time: 0.3150  data: 0.0002  max mem: 4626
[11:56:06.619980] Test:  [ 910/2194]  eta: 0:06:35  loss: 4.3604 (4.4857)  acc1: 0.0000 (0.0274)  acc5: 12.5000 (17.8101)  time: 0.3149  data: 0.0002  max mem: 4626
[11:56:09.768211] Test:  [ 920/2194]  eta: 0:06:32  loss: 4.3542 (4.4843)  acc1: 0.0000 (0.0271)  acc5: 25.0000 (17.8610)  time: 0.3146  data: 0.0002  max mem: 4626
[11:56:12.914569] Test:  [ 930/2194]  eta: 0:06:29  loss: 4.3542 (4.4845)  acc1: 0.0000 (0.0269)  acc5: 25.0000 (17.8571)  time: 0.3146  data: 0.0002  max mem: 4626
[11:56:16.069804] Test:  [ 940/2194]  eta: 0:06:26  loss: 4.3740 (4.4831)  acc1: 0.0000 (0.0266)  acc5: 25.0000 (17.8932)  time: 0.3150  data: 0.0002  max mem: 4626
[11:56:19.215524] Test:  [ 950/2194]  eta: 0:06:23  loss: 4.3770 (4.4834)  acc1: 0.0000 (0.0263)  acc5: 25.0000 (17.9022)  time: 0.3149  data: 0.0002  max mem: 4626
[11:56:22.375533] Test:  [ 960/2194]  eta: 0:06:20  loss: 4.3877 (4.4821)  acc1: 0.0000 (0.0260)  acc5: 12.5000 (17.8980)  time: 0.3152  data: 0.0002  max mem: 4626
[11:56:25.526774] Test:  [ 970/2194]  eta: 0:06:17  loss: 4.3921 (4.4818)  acc1: 0.0000 (0.0257)  acc5: 12.5000 (17.8682)  time: 0.3155  data: 0.0001  max mem: 4626
[11:56:28.665545] Test:  [ 980/2194]  eta: 0:06:14  loss: 4.4216 (4.4803)  acc1: 0.0000 (0.0255)  acc5: 12.5000 (17.9536)  time: 0.3144  data: 0.0002  max mem: 4626
[11:56:31.825067] Test:  [ 990/2194]  eta: 0:06:11  loss: 4.4268 (4.4801)  acc1: 0.0000 (0.0252)  acc5: 12.5000 (17.9238)  time: 0.3148  data: 0.0002  max mem: 4626
[11:56:34.983684] Test:  [1000/2194]  eta: 0:06:08  loss: 4.4744 (4.4793)  acc1: 0.0000 (0.0250)  acc5: 12.5000 (17.9071)  time: 0.3158  data: 0.0002  max mem: 4626
[11:56:38.137741] Test:  [1010/2194]  eta: 0:06:05  loss: 4.3511 (4.4785)  acc1: 0.0000 (0.0247)  acc5: 12.5000 (17.9773)  time: 0.3155  data: 0.0002  max mem: 4626
[11:56:41.291079] Test:  [1020/2194]  eta: 0:06:02  loss: 4.3188 (4.4772)  acc1: 0.0000 (0.0245)  acc5: 25.0000 (18.0093)  time: 0.3153  data: 0.0002  max mem: 4626
[11:56:44.445607] Test:  [1030/2194]  eta: 0:05:59  loss: 4.3188 (4.4761)  acc1: 0.0000 (0.0242)  acc5: 25.0000 (18.0650)  time: 0.3153  data: 0.0002  max mem: 4626
[11:56:47.603778] Test:  [1040/2194]  eta: 0:05:56  loss: 4.5310 (4.4768)  acc1: 0.0000 (0.0240)  acc5: 12.5000 (18.0235)  time: 0.3156  data: 0.0001  max mem: 4626
[11:56:50.753821] Test:  [1050/2194]  eta: 0:05:53  loss: 4.5842 (4.4766)  acc1: 0.0000 (0.0238)  acc5: 12.5000 (18.0661)  time: 0.3153  data: 0.0001  max mem: 4626
[11:56:53.910798] Test:  [1060/2194]  eta: 0:05:50  loss: 4.4285 (4.4761)  acc1: 0.0000 (0.0236)  acc5: 25.0000 (18.0844)  time: 0.3153  data: 0.0002  max mem: 4626
[11:56:57.070553] Test:  [1070/2194]  eta: 0:05:47  loss: 4.4724 (4.4771)  acc1: 0.0000 (0.0233)  acc5: 25.0000 (18.0789)  time: 0.3158  data: 0.0002  max mem: 4626
[11:57:00.227598] Test:  [1080/2194]  eta: 0:05:44  loss: 4.4907 (4.4769)  acc1: 0.0000 (0.0231)  acc5: 25.0000 (18.1198)  time: 0.3158  data: 0.0001  max mem: 4626
[11:57:03.388677] Test:  [1090/2194]  eta: 0:05:41  loss: 4.4575 (4.4764)  acc1: 0.0000 (0.0229)  acc5: 25.0000 (18.1370)  time: 0.3158  data: 0.0002  max mem: 4626
[11:57:06.546154] Test:  [1100/2194]  eta: 0:05:38  loss: 4.5212 (4.4772)  acc1: 0.0000 (0.0227)  acc5: 12.5000 (18.1199)  time: 0.3158  data: 0.0002  max mem: 4626
[11:57:09.700131] Test:  [1110/2194]  eta: 0:05:35  loss: 4.5212 (4.4757)  acc1: 0.0000 (0.0225)  acc5: 12.5000 (18.1931)  time: 0.3155  data: 0.0002  max mem: 4626
[11:57:12.858617] Test:  [1120/2194]  eta: 0:05:32  loss: 4.4351 (4.4752)  acc1: 0.0000 (0.0223)  acc5: 12.5000 (18.1534)  time: 0.3155  data: 0.0002  max mem: 4626
[11:57:16.020541] Test:  [1130/2194]  eta: 0:05:29  loss: 4.4783 (4.4764)  acc1: 0.0000 (0.0221)  acc5: 12.5000 (18.1145)  time: 0.3159  data: 0.0002  max mem: 4626
[11:57:19.182207] Test:  [1140/2194]  eta: 0:05:26  loss: 4.4875 (4.4766)  acc1: 0.0000 (0.0219)  acc5: 12.5000 (18.0653)  time: 0.3161  data: 0.0002  max mem: 4626
[11:57:22.342568] Test:  [1150/2194]  eta: 0:05:23  loss: 4.5073 (4.4766)  acc1: 0.0000 (0.0217)  acc5: 12.5000 (18.0387)  time: 0.3160  data: 0.0002  max mem: 4626
[11:57:25.510186] Test:  [1160/2194]  eta: 0:05:20  loss: 4.7180 (4.4792)  acc1: 0.0000 (0.0215)  acc5: 12.5000 (17.9802)  time: 0.3163  data: 0.0002  max mem: 4626
[11:57:28.677258] Test:  [1170/2194]  eta: 0:05:17  loss: 4.5923 (4.4787)  acc1: 0.0000 (0.0213)  acc5: 12.5000 (18.0188)  time: 0.3166  data: 0.0002  max mem: 4626
[11:57:31.841833] Test:  [1180/2194]  eta: 0:05:14  loss: 4.4229 (4.4778)  acc1: 0.0000 (0.0212)  acc5: 25.0000 (18.0356)  time: 0.3165  data: 0.0002  max mem: 4626
[11:57:35.002438] Test:  [1190/2194]  eta: 0:05:11  loss: 4.5125 (4.4788)  acc1: 0.0000 (0.0210)  acc5: 12.5000 (17.9786)  time: 0.3162  data: 0.0002  max mem: 4626
[11:57:38.163246] Test:  [1200/2194]  eta: 0:05:08  loss: 4.5808 (4.4785)  acc1: 0.0000 (0.0208)  acc5: 12.5000 (17.9746)  time: 0.3160  data: 0.0002  max mem: 4626
[11:57:41.319808] Test:  [1210/2194]  eta: 0:05:05  loss: 4.4480 (4.4787)  acc1: 0.0000 (0.0206)  acc5: 12.5000 (17.9500)  time: 0.3158  data: 0.0002  max mem: 4626
[11:57:44.479773] Test:  [1220/2194]  eta: 0:05:02  loss: 4.4480 (4.4779)  acc1: 0.0000 (0.0205)  acc5: 12.5000 (17.9771)  time: 0.3157  data: 0.0002  max mem: 4626
[11:57:47.639405] Test:  [1230/2194]  eta: 0:04:58  loss: 4.3591 (4.4783)  acc1: 0.0000 (0.0203)  acc5: 12.5000 (17.9732)  time: 0.3159  data: 0.0001  max mem: 4626
[11:57:50.801322] Test:  [1240/2194]  eta: 0:04:55  loss: 4.4397 (4.4779)  acc1: 0.0000 (0.0201)  acc5: 25.0000 (17.9996)  time: 0.3160  data: 0.0001  max mem: 4626
[11:57:53.961994] Test:  [1250/2194]  eta: 0:04:52  loss: 4.4407 (4.4774)  acc1: 0.0000 (0.0200)  acc5: 25.0000 (18.0056)  time: 0.3160  data: 0.0002  max mem: 4626
[11:57:57.125214] Test:  [1260/2194]  eta: 0:04:49  loss: 4.3301 (4.4773)  acc1: 0.0000 (0.0198)  acc5: 12.5000 (18.0412)  time: 0.3161  data: 0.0002  max mem: 4626
[11:58:00.285442] Test:  [1270/2194]  eta: 0:04:46  loss: 4.4041 (4.4772)  acc1: 0.0000 (0.0197)  acc5: 25.0000 (18.0566)  time: 0.3161  data: 0.0002  max mem: 4626
[11:58:03.438546] Test:  [1280/2194]  eta: 0:04:43  loss: 4.4041 (4.4764)  acc1: 0.0000 (0.0195)  acc5: 25.0000 (18.1011)  time: 0.3156  data: 0.0002  max mem: 4626
[11:58:06.601978] Test:  [1290/2194]  eta: 0:04:40  loss: 4.3362 (4.4773)  acc1: 0.0000 (0.0194)  acc5: 12.5000 (18.0674)  time: 0.3157  data: 0.0002  max mem: 4626
[11:58:09.767319] Test:  [1300/2194]  eta: 0:04:37  loss: 4.3362 (4.4771)  acc1: 0.0000 (0.0192)  acc5: 12.5000 (18.0726)  time: 0.3163  data: 0.0002  max mem: 4626
[11:58:12.931550] Test:  [1310/2194]  eta: 0:04:34  loss: 4.3083 (4.4769)  acc1: 0.0000 (0.0191)  acc5: 25.0000 (18.0969)  time: 0.3164  data: 0.0002  max mem: 4626
[11:58:16.092538] Test:  [1320/2194]  eta: 0:04:31  loss: 4.1848 (4.4744)  acc1: 0.0000 (0.0189)  acc5: 25.0000 (18.2059)  time: 0.3162  data: 0.0002  max mem: 4626
[11:58:19.255321] Test:  [1330/2194]  eta: 0:04:28  loss: 4.1309 (4.4731)  acc1: 0.0000 (0.0188)  acc5: 25.0000 (18.2476)  time: 0.3161  data: 0.0002  max mem: 4626
[11:58:22.417485] Test:  [1340/2194]  eta: 0:04:25  loss: 4.3206 (4.4732)  acc1: 0.0000 (0.0186)  acc5: 12.5000 (18.2327)  time: 0.3162  data: 0.0002  max mem: 4626
[11:58:25.579505] Test:  [1350/2194]  eta: 0:04:22  loss: 4.4224 (4.4728)  acc1: 0.0000 (0.0185)  acc5: 12.5000 (18.2642)  time: 0.3161  data: 0.0002  max mem: 4626
[11:58:28.741553] Test:  [1360/2194]  eta: 0:04:19  loss: 4.5415 (4.4738)  acc1: 0.0000 (0.0184)  acc5: 12.5000 (18.2311)  time: 0.3161  data: 0.0002  max mem: 4626
[11:58:31.916401] Test:  [1370/2194]  eta: 0:04:16  loss: 4.5818 (4.4744)  acc1: 0.0000 (0.0182)  acc5: 12.5000 (18.2075)  time: 0.3168  data: 0.0002  max mem: 4626
[11:58:35.073220] Test:  [1380/2194]  eta: 0:04:12  loss: 4.5151 (4.4731)  acc1: 0.0000 (0.0181)  acc5: 12.5000 (18.2295)  time: 0.3165  data: 0.0001  max mem: 4626
[11:58:38.238574] Test:  [1390/2194]  eta: 0:04:09  loss: 4.4111 (4.4730)  acc1: 0.0000 (0.0180)  acc5: 12.5000 (18.2153)  time: 0.3160  data: 0.0002  max mem: 4626
[11:58:41.409310] Test:  [1400/2194]  eta: 0:04:06  loss: 4.5820 (4.4748)  acc1: 0.0000 (0.0178)  acc5: 12.5000 (18.1834)  time: 0.3167  data: 0.0002  max mem: 4626
[11:58:44.565406] Test:  [1410/2194]  eta: 0:04:03  loss: 4.6516 (4.4749)  acc1: 0.0000 (0.0177)  acc5: 12.5000 (18.1786)  time: 0.3163  data: 0.0002  max mem: 4626
[11:58:47.719857] Test:  [1420/2194]  eta: 0:04:00  loss: 4.3621 (4.4730)  acc1: 0.0000 (0.0176)  acc5: 25.0000 (18.2266)  time: 0.3154  data: 0.0002  max mem: 4626
[11:58:50.878065] Test:  [1430/2194]  eta: 0:03:57  loss: 4.3621 (4.4732)  acc1: 0.0000 (0.0175)  acc5: 12.5000 (18.2041)  time: 0.3155  data: 0.0002  max mem: 4626
[11:58:54.038479] Test:  [1440/2194]  eta: 0:03:54  loss: 4.5229 (4.4737)  acc1: 0.0000 (0.0173)  acc5: 12.5000 (18.1645)  time: 0.3158  data: 0.0002  max mem: 4626
[11:58:57.207588] Test:  [1450/2194]  eta: 0:03:51  loss: 4.5388 (4.4742)  acc1: 0.0000 (0.0172)  acc5: 12.5000 (18.1427)  time: 0.3164  data: 0.0002  max mem: 4626
[11:59:00.374651] Test:  [1460/2194]  eta: 0:03:48  loss: 4.6157 (4.4757)  acc1: 0.0000 (0.0171)  acc5: 12.5000 (18.1211)  time: 0.3167  data: 0.0002  max mem: 4626
[11:59:03.541650] Test:  [1470/2194]  eta: 0:03:45  loss: 4.5732 (4.4758)  acc1: 0.0000 (0.0170)  acc5: 12.5000 (18.1084)  time: 0.3166  data: 0.0002  max mem: 4626
[11:59:06.705036] Test:  [1480/2194]  eta: 0:03:42  loss: 4.4421 (4.4756)  acc1: 0.0000 (0.0169)  acc5: 12.5000 (18.1381)  time: 0.3164  data: 0.0002  max mem: 4626
[11:59:09.861491] Test:  [1490/2194]  eta: 0:03:39  loss: 4.4275 (4.4751)  acc1: 0.0000 (0.0168)  acc5: 25.0000 (18.1422)  time: 0.3159  data: 0.0002  max mem: 4626
[11:59:13.021590] Test:  [1500/2194]  eta: 0:03:35  loss: 4.2720 (4.4728)  acc1: 0.0000 (0.0167)  acc5: 25.0000 (18.2129)  time: 0.3157  data: 0.0002  max mem: 4626
[11:59:16.188433] Test:  [1510/2194]  eta: 0:03:32  loss: 4.2900 (4.4730)  acc1: 0.0000 (0.0165)  acc5: 25.0000 (18.2164)  time: 0.3163  data: 0.0002  max mem: 4626
[11:59:19.350913] Test:  [1520/2194]  eta: 0:03:29  loss: 4.5449 (4.4729)  acc1: 0.0000 (0.0164)  acc5: 12.5000 (18.2446)  time: 0.3164  data: 0.0002  max mem: 4626
[11:59:22.516320] Test:  [1530/2194]  eta: 0:03:26  loss: 4.4514 (4.4728)  acc1: 0.0000 (0.0163)  acc5: 12.5000 (18.2397)  time: 0.3163  data: 0.0002  max mem: 4626
[11:59:25.680082] Test:  [1540/2194]  eta: 0:03:23  loss: 4.3154 (4.4716)  acc1: 0.0000 (0.0162)  acc5: 12.5000 (18.2755)  time: 0.3164  data: 0.0002  max mem: 4626
[11:59:28.848407] Test:  [1550/2194]  eta: 0:03:20  loss: 4.2454 (4.4716)  acc1: 0.0000 (0.0161)  acc5: 25.0000 (18.2785)  time: 0.3165  data: 0.0002  max mem: 4626
[11:59:32.014798] Test:  [1560/2194]  eta: 0:03:17  loss: 4.5361 (4.4731)  acc1: 0.0000 (0.0160)  acc5: 12.5000 (18.2495)  time: 0.3167  data: 0.0002  max mem: 4626
[11:59:35.174727] Test:  [1570/2194]  eta: 0:03:14  loss: 4.6533 (4.4733)  acc1: 0.0000 (0.0159)  acc5: 12.5000 (18.2447)  time: 0.3162  data: 0.0002  max mem: 4626
[11:59:38.338783] Test:  [1580/2194]  eta: 0:03:11  loss: 4.4697 (4.4739)  acc1: 0.0000 (0.0158)  acc5: 12.5000 (18.2005)  time: 0.3161  data: 0.0001  max mem: 4626
[11:59:41.507235] Test:  [1590/2194]  eta: 0:03:08  loss: 4.4783 (4.4736)  acc1: 0.0000 (0.0157)  acc5: 12.5000 (18.1882)  time: 0.3165  data: 0.0002  max mem: 4626
[11:59:44.667797] Test:  [1600/2194]  eta: 0:03:05  loss: 4.4783 (4.4743)  acc1: 0.0000 (0.0156)  acc5: 12.5000 (18.1996)  time: 0.3164  data: 0.0002  max mem: 4626
[11:59:47.832432] Test:  [1610/2194]  eta: 0:03:01  loss: 4.5386 (4.4749)  acc1: 0.0000 (0.0155)  acc5: 12.5000 (18.1797)  time: 0.3162  data: 0.0002  max mem: 4626
[11:59:50.995426] Test:  [1620/2194]  eta: 0:02:58  loss: 4.5291 (4.4748)  acc1: 0.0000 (0.0154)  acc5: 12.5000 (18.1678)  time: 0.3163  data: 0.0001  max mem: 4626
[11:59:54.166338] Test:  [1630/2194]  eta: 0:02:55  loss: 4.4912 (4.4761)  acc1: 0.0000 (0.0153)  acc5: 12.5000 (18.1484)  time: 0.3166  data: 0.0002  max mem: 4626
[11:59:57.329395] Test:  [1640/2194]  eta: 0:02:52  loss: 4.5073 (4.4758)  acc1: 0.0000 (0.0152)  acc5: 12.5000 (18.1597)  time: 0.3166  data: 0.0002  max mem: 4626
[12:00:00.491270] Test:  [1650/2194]  eta: 0:02:49  loss: 4.4927 (4.4763)  acc1: 0.0000 (0.0151)  acc5: 12.5000 (18.1784)  time: 0.3162  data: 0.0002  max mem: 4626
[12:00:03.657058] Test:  [1660/2194]  eta: 0:02:46  loss: 4.4363 (4.4771)  acc1: 0.0000 (0.0151)  acc5: 12.5000 (18.1442)  time: 0.3163  data: 0.0002  max mem: 4626
[12:00:06.822863] Test:  [1670/2194]  eta: 0:02:43  loss: 4.5288 (4.4776)  acc1: 0.0000 (0.0150)  acc5: 12.5000 (18.1104)  time: 0.3165  data: 0.0002  max mem: 4626
[12:00:09.980761] Test:  [1680/2194]  eta: 0:02:40  loss: 4.5107 (4.4773)  acc1: 0.0000 (0.0149)  acc5: 12.5000 (18.1217)  time: 0.3161  data: 0.0002  max mem: 4626
[12:00:13.139306] Test:  [1690/2194]  eta: 0:02:37  loss: 4.4106 (4.4769)  acc1: 0.0000 (0.0148)  acc5: 12.5000 (18.1549)  time: 0.3157  data: 0.0002  max mem: 4626
[12:00:16.304653] Test:  [1700/2194]  eta: 0:02:34  loss: 4.5449 (4.4780)  acc1: 0.0000 (0.0147)  acc5: 12.5000 (18.1070)  time: 0.3161  data: 0.0002  max mem: 4626
[12:00:19.459695] Test:  [1710/2194]  eta: 0:02:30  loss: 4.5366 (4.4780)  acc1: 0.0000 (0.0146)  acc5: 12.5000 (18.1034)  time: 0.3159  data: 0.0002  max mem: 4626
[12:00:22.619905] Test:  [1720/2194]  eta: 0:02:27  loss: 4.5164 (4.4787)  acc1: 0.0000 (0.0145)  acc5: 12.5000 (18.0927)  time: 0.3157  data: 0.0002  max mem: 4626
[12:00:25.783277] Test:  [1730/2194]  eta: 0:02:24  loss: 4.3999 (4.4783)  acc1: 0.0000 (0.0144)  acc5: 12.5000 (18.0893)  time: 0.3161  data: 0.0002  max mem: 4626
[12:00:28.939518] Test:  [1740/2194]  eta: 0:02:21  loss: 4.2898 (4.4777)  acc1: 0.0000 (0.0144)  acc5: 25.0000 (18.1074)  time: 0.3159  data: 0.0002  max mem: 4626
[12:00:32.095418] Test:  [1750/2194]  eta: 0:02:18  loss: 4.4290 (4.4782)  acc1: 0.0000 (0.0143)  acc5: 12.5000 (18.0825)  time: 0.3155  data: 0.0002  max mem: 4626
[12:00:35.261307] Test:  [1760/2194]  eta: 0:02:15  loss: 4.4553 (4.4777)  acc1: 0.0000 (0.0142)  acc5: 12.5000 (18.1076)  time: 0.3160  data: 0.0002  max mem: 4626
[12:00:38.424145] Test:  [1770/2194]  eta: 0:02:12  loss: 4.5581 (4.4781)  acc1: 0.0000 (0.0141)  acc5: 12.5000 (18.1042)  time: 0.3164  data: 0.0002  max mem: 4626
[12:00:41.589630] Test:  [1780/2194]  eta: 0:02:09  loss: 4.6025 (4.4785)  acc1: 0.0000 (0.0140)  acc5: 12.5000 (18.0867)  time: 0.3163  data: 0.0001  max mem: 4626
[12:00:44.753467] Test:  [1790/2194]  eta: 0:02:06  loss: 4.4082 (4.4781)  acc1: 0.0000 (0.0140)  acc5: 12.5000 (18.1044)  time: 0.3164  data: 0.0002  max mem: 4626
[12:00:47.906159] Test:  [1800/2194]  eta: 0:02:02  loss: 4.3010 (4.4769)  acc1: 0.0000 (0.0139)  acc5: 25.0000 (18.1427)  time: 0.3157  data: 0.0002  max mem: 4626
[12:00:51.067516] Test:  [1810/2194]  eta: 0:01:59  loss: 4.2971 (4.4772)  acc1: 0.0000 (0.0138)  acc5: 25.0000 (18.1322)  time: 0.3156  data: 0.0001  max mem: 4626
[12:00:54.225983] Test:  [1820/2194]  eta: 0:01:56  loss: 4.3850 (4.4765)  acc1: 0.0000 (0.0137)  acc5: 12.5000 (18.1700)  time: 0.3159  data: 0.0002  max mem: 4626
[12:00:57.384376] Test:  [1830/2194]  eta: 0:01:53  loss: 4.1860 (4.4751)  acc1: 0.0000 (0.0137)  acc5: 25.0000 (18.2209)  time: 0.3158  data: 0.0002  max mem: 4626
[12:01:00.538904] Test:  [1840/2194]  eta: 0:01:50  loss: 4.1807 (4.4751)  acc1: 0.0000 (0.0136)  acc5: 25.0000 (18.2374)  time: 0.3156  data: 0.0001  max mem: 4626
[12:01:03.702676] Test:  [1850/2194]  eta: 0:01:47  loss: 4.3274 (4.4747)  acc1: 0.0000 (0.0135)  acc5: 25.0000 (18.2334)  time: 0.3158  data: 0.0002  max mem: 4626
[12:01:06.857105] Test:  [1860/2194]  eta: 0:01:44  loss: 4.3845 (4.4746)  acc1: 0.0000 (0.0134)  acc5: 12.5000 (18.2429)  time: 0.3158  data: 0.0002  max mem: 4626
[12:01:10.013995] Test:  [1870/2194]  eta: 0:01:41  loss: 4.4526 (4.4747)  acc1: 0.0000 (0.0134)  acc5: 12.5000 (18.2322)  time: 0.3155  data: 0.0002  max mem: 4626
[12:01:13.178012] Test:  [1880/2194]  eta: 0:01:38  loss: 4.4573 (4.4745)  acc1: 0.0000 (0.0133)  acc5: 12.5000 (18.2350)  time: 0.3160  data: 0.0002  max mem: 4626
[12:01:16.350191] Test:  [1890/2194]  eta: 0:01:34  loss: 4.4331 (4.4744)  acc1: 0.0000 (0.0132)  acc5: 12.5000 (18.2311)  time: 0.3167  data: 0.0002  max mem: 4626
[12:01:19.509498] Test:  [1900/2194]  eta: 0:01:31  loss: 4.3608 (4.4733)  acc1: 0.0000 (0.0132)  acc5: 25.0000 (18.2667)  time: 0.3165  data: 0.0002  max mem: 4626
[12:01:22.672189] Test:  [1910/2194]  eta: 0:01:28  loss: 4.5168 (4.4743)  acc1: 0.0000 (0.0131)  acc5: 12.5000 (18.2365)  time: 0.3160  data: 0.0002  max mem: 4626
[12:01:25.839222] Test:  [1920/2194]  eta: 0:01:25  loss: 4.6472 (4.4749)  acc1: 0.0000 (0.0130)  acc5: 12.5000 (18.2132)  time: 0.3164  data: 0.0001  max mem: 4626
[12:01:28.999010] Test:  [1930/2194]  eta: 0:01:22  loss: 4.4536 (4.4750)  acc1: 0.0000 (0.0129)  acc5: 12.5000 (18.2095)  time: 0.3163  data: 0.0002  max mem: 4626
[12:01:32.163774] Test:  [1940/2194]  eta: 0:01:19  loss: 4.4451 (4.4747)  acc1: 0.0000 (0.0129)  acc5: 12.5000 (18.2058)  time: 0.3161  data: 0.0002  max mem: 4626
[12:01:35.333593] Test:  [1950/2194]  eta: 0:01:16  loss: 4.4526 (4.4753)  acc1: 0.0000 (0.0128)  acc5: 12.5000 (18.1766)  time: 0.3166  data: 0.0002  max mem: 4626
[12:01:38.493752] Test:  [1960/2194]  eta: 0:01:13  loss: 4.6377 (4.4764)  acc1: 0.0000 (0.0127)  acc5: 12.5000 (18.1476)  time: 0.3164  data: 0.0002  max mem: 4626
[12:01:41.659364] Test:  [1970/2194]  eta: 0:01:09  loss: 4.7109 (4.4771)  acc1: 0.0000 (0.0127)  acc5: 12.5000 (18.1317)  time: 0.3162  data: 0.0001  max mem: 4626
[12:01:44.824487] Test:  [1980/2194]  eta: 0:01:06  loss: 4.5762 (4.4775)  acc1: 0.0000 (0.0126)  acc5: 12.5000 (18.1095)  time: 0.3165  data: 0.0002  max mem: 4626
[12:01:47.988965] Test:  [1990/2194]  eta: 0:01:03  loss: 4.5776 (4.4776)  acc1: 0.0000 (0.0126)  acc5: 12.5000 (18.0939)  time: 0.3164  data: 0.0002  max mem: 4626
[12:01:51.149521] Test:  [2000/2194]  eta: 0:01:00  loss: 4.5776 (4.4771)  acc1: 0.0000 (0.0125)  acc5: 12.5000 (18.1222)  time: 0.3162  data: 0.0001  max mem: 4626
[12:01:54.308787] Test:  [2010/2194]  eta: 0:00:57  loss: 4.5535 (4.4777)  acc1: 0.0000 (0.0124)  acc5: 12.5000 (18.1067)  time: 0.3159  data: 0.0002  max mem: 4626
[12:01:57.468149] Test:  [2020/2194]  eta: 0:00:54  loss: 4.4827 (4.4777)  acc1: 0.0000 (0.0124)  acc5: 25.0000 (18.1098)  time: 0.3158  data: 0.0002  max mem: 4626
[12:02:00.626883] Test:  [2030/2194]  eta: 0:00:51  loss: 4.3882 (4.4773)  acc1: 0.0000 (0.0123)  acc5: 25.0000 (18.1192)  time: 0.3158  data: 0.0002  max mem: 4626
[12:02:03.779358] Test:  [2040/2194]  eta: 0:00:48  loss: 4.2866 (4.4766)  acc1: 0.0000 (0.0122)  acc5: 12.5000 (18.1406)  time: 0.3155  data: 0.0002  max mem: 4626
[12:02:06.937246] Test:  [2050/2194]  eta: 0:00:45  loss: 4.2866 (4.4765)  acc1: 0.0000 (0.0122)  acc5: 25.0000 (18.1497)  time: 0.3154  data: 0.0002  max mem: 4626
[12:02:10.095819] Test:  [2060/2194]  eta: 0:00:41  loss: 4.4519 (4.4764)  acc1: 0.0000 (0.0121)  acc5: 12.5000 (18.1587)  time: 0.3157  data: 0.0002  max mem: 4626
[12:02:13.248684] Test:  [2070/2194]  eta: 0:00:38  loss: 4.3816 (4.4760)  acc1: 0.0000 (0.0121)  acc5: 12.5000 (18.1796)  time: 0.3155  data: 0.0002  max mem: 4626
[12:02:16.407170] Test:  [2080/2194]  eta: 0:00:35  loss: 4.4387 (4.4764)  acc1: 0.0000 (0.0120)  acc5: 12.5000 (18.1764)  time: 0.3155  data: 0.0002  max mem: 4626
[12:02:19.563239] Test:  [2090/2194]  eta: 0:00:32  loss: 4.5522 (4.4761)  acc1: 0.0000 (0.0120)  acc5: 12.5000 (18.1671)  time: 0.3156  data: 0.0002  max mem: 4626
[12:02:22.726039] Test:  [2100/2194]  eta: 0:00:29  loss: 4.5151 (4.4760)  acc1: 0.0000 (0.0119)  acc5: 12.5000 (18.1818)  time: 0.3159  data: 0.0002  max mem: 4626
[12:02:25.875518] Test:  [2110/2194]  eta: 0:00:26  loss: 4.4917 (4.4759)  acc1: 0.0000 (0.0118)  acc5: 12.5000 (18.1964)  time: 0.3155  data: 0.0001  max mem: 4626
[12:02:29.043734] Test:  [2120/2194]  eta: 0:00:23  loss: 4.4658 (4.4768)  acc1: 0.0000 (0.0118)  acc5: 12.5000 (18.1872)  time: 0.3158  data: 0.0002  max mem: 4626
[12:02:32.193682] Test:  [2130/2194]  eta: 0:00:20  loss: 4.5098 (4.4762)  acc1: 0.0000 (0.0117)  acc5: 12.5000 (18.2133)  time: 0.3158  data: 0.0002  max mem: 4626
[12:02:35.349006] Test:  [2140/2194]  eta: 0:00:16  loss: 4.5068 (4.4761)  acc1: 0.0000 (0.0117)  acc5: 12.5000 (18.2041)  time: 0.3152  data: 0.0002  max mem: 4626
[12:02:38.500421] Test:  [2150/2194]  eta: 0:00:13  loss: 4.5762 (4.4765)  acc1: 0.0000 (0.0116)  acc5: 12.5000 (18.1892)  time: 0.3153  data: 0.0001  max mem: 4626
[12:02:41.660711] Test:  [2160/2194]  eta: 0:00:10  loss: 4.5881 (4.4769)  acc1: 0.0000 (0.0116)  acc5: 12.5000 (18.1745)  time: 0.3155  data: 0.0001  max mem: 4626
[12:02:44.816989] Test:  [2170/2194]  eta: 0:00:07  loss: 4.5881 (4.4770)  acc1: 0.0000 (0.0115)  acc5: 12.5000 (18.1771)  time: 0.3157  data: 0.0002  max mem: 4626
[12:02:47.975358] Test:  [2180/2194]  eta: 0:00:04  loss: 4.5088 (4.4765)  acc1: 0.0000 (0.0115)  acc5: 12.5000 (18.1969)  time: 0.3157  data: 0.0004  max mem: 4626
[12:02:51.137616] Test:  [2190/2194]  eta: 0:00:01  loss: 4.5088 (4.4767)  acc1: 0.0000 (0.0114)  acc5: 12.5000 (18.1823)  time: 0.3160  data: 0.0004  max mem: 4626
[12:02:52.022988] Test:  [2193/2194]  eta: 0:00:00  loss: 4.5088 (4.4764)  acc1: 0.0000 (0.0114)  acc5: 12.5000 (18.1823)  time: 0.3126  data: 0.0004  max mem: 4626
[12:02:52.071561] Test: Total time: 0:11:26 (0.3128 s / it)
[12:02:52.071646] * Acc@1 0.011 Acc@5 18.182 loss 4.476
[12:02:52.071815] Evaluation on 17550 test images- acc1: 0.01%, acc5: 18.18%
/home/ada/anaconda3/envs/sat_env/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
